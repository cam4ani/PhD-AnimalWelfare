{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HenVariable(df, config, path_FocalBird, ts_name, name_='', timestamp_name='Timestamp'):\n",
    "    \n",
    "    ''' Note: work with ts that have nan (typically at begining)\n",
    "    Compute some variable at a \"level\" level, which could be from 17h-3h i.e. over two consecutive days.\n",
    "    \n",
    "    Input:\n",
    "    df_ts: Each row correspond to a specific timestamp, each column to a specific hen timeseries (which column name must start \n",
    "        with hen_ ). Must also have a Timestamp and a level column, which will be used to aggregate info and compute variables on these \n",
    "        aggregated info\n",
    "    config: file with parameter\n",
    "    \n",
    "    Output:\n",
    "    daily dataframe (where daily is according to the level variable) with according variables'''\n",
    "    \n",
    "    #start recording the time it last\n",
    "    START_TIME = time.clock()\n",
    "    \n",
    "    print('----------------- create time serie for simplicity....')\n",
    "    df_ts = time_series_henColumn_tsRow(df, config, col_ts='Zone', ts_with_all_hen_value=False)\n",
    "\n",
    "    #compute nbr_sec computation here (list of difference between each timestamp, and must always be the same)\n",
    "    li_ts = df_ts[timestamp_name].tolist()\n",
    "    li_diff_ts = list(set(list(map(operator.sub, li_ts[1:], li_ts[0:-1]))))\n",
    "    if len(li_diff_ts)!=1:\n",
    "        print('ERROR: your timestamp columns have different one to one difference: ', li_diff_ts)\n",
    "        sys.exit()\n",
    "    nbr_sec = li_diff_ts[0].total_seconds()\n",
    "    print('your time series has %d seconds between two timestamps'%nbr_sec)    \n",
    "    \n",
    "    ############ initialise parameters from config file\n",
    "    path_extracted_data = config.path_extracted_data\n",
    "    li_date2remove = config.li_date2remove\n",
    "    id_run = config.id_run\n",
    "    date_max = config.date_max\n",
    "    dico_night_hour = config.dico_night_hour\n",
    "    dico_zone_order = config.dico_zone_order\n",
    "    dico_date2remove_pens= config.dico_date2remove_pens\n",
    "    #EntropyTimeComputation = config.EntropyTimeComputation\n",
    "    #NbrData = config.NbrData\n",
    "    \n",
    "    ############ add correct 'level' variable (i.e. consecutive time slot for night time series)\n",
    "    df_ts['is_day'] = df_ts[timestamp_name].map(lambda x: is_day(x, dico_night_hour))\n",
    "    #note that minuit is: 0, and its date should be as 1,2 (day-1, day)\n",
    "    if ts_name == 'time_serie_night':\n",
    "        df_ts = df_ts[~df_ts['is_day']].copy()\n",
    "        df_ts['level'] = df_ts[timestamp_name].map(lambda x: str(x)[0:-9]+'_'+str(x+dt.timedelta(days=1))[8:10] if\\\n",
    "                                                name_level(x,dico_night_hour) else str(x-dt.timedelta(days=1))[0:-9]+'_'+str(x)[8:10])\n",
    "    elif ts_name == 'time_serie_day':\n",
    "        df_ts = df_ts[df_ts['is_day']].copy()\n",
    "        df_ts['level'] = df_ts['date'].copy()\n",
    "    else:\n",
    "        print('ERROR: ts_name parameter must either be time_serie_night or time_serie_day')\n",
    "        sys.exit()\n",
    "        \n",
    "    ############ verifications\n",
    "    #verify columns name of df_ts and select the column we need\n",
    "    li_hen = [i for i in list(df_ts) if i.startswith('hen_')]\n",
    "    if not all([i in df_ts.columns for i in [timestamp_name,'level']]):\n",
    "        print('ERROR: your df_ts must have timestamp and level column name')\n",
    "        sys.exit()\n",
    "    df = df_ts.filter([timestamp_name,'level']+li_hen).copy()\n",
    "    #verify that the timestamp has same difference than the suggested nbr_sec parameter\n",
    "    df = df.sort_values(timestamp_name)\n",
    "    if (df[timestamp_name].iloc[1]-df[timestamp_name].iloc[0]).seconds!=nbr_sec:\n",
    "        print('ERROR: your timestamp difference does not equal your nbr_sec parameter')\n",
    "        sys.exit()\n",
    "    \n",
    "    #list of involved level\n",
    "    li_day = set(df['level'].tolist())  \n",
    "\n",
    "    ############ one row per unique hen-timestamp \n",
    "    df = pd.melt(df, id_vars=[timestamp_name,'level'], value_vars=li_hen)\n",
    "    df.rename(columns={'variable':'HenID','value':'Zone'}, inplace=True)\n",
    "    #we define the duration of each row to be the nbr_sec, its better than computing with the next timestamp as if we removed some days\n",
    "    #due to health-assessemnt, then it will induce wrong durations! also more efficient that way. BUT its an assumption, that the row must\n",
    "    #be equally spaced and nbr_sec is the duration in between each timestamp\n",
    "    df['duration_sec'] = nbr_sec\n",
    "    #list of not nan Zones\n",
    "    li_Zone = [x for x in df[~df['Zone'].isnull()]['Zone'].unique()]\n",
    "\n",
    "    ########################################################\n",
    "    print('----------------- total duration per Zone in seconds!!....')\n",
    "    #one row per day, hen, existingzone\n",
    "    df_ = df.groupby(['HenID','level','Zone'])['duration_sec'].agg(lambda x: sum(x)).reset_index()\n",
    "    #one row per day and hen, each columns account for a zone_duration\n",
    "    df_daily = df_.pivot_table(values='duration_sec', index=['HenID', 'level'], columns='Zone')\n",
    "    df_daily.rename(columns={x:'duration_'+x for x in li_Zone}, inplace=True)\n",
    "    #lets verify with total duration\n",
    "    df_daily['verification_daily_total_duration'] = df_daily.apply(lambda x: np.nansum([x[i] for i in ['duration_'+x for x in li_Zone]]),\n",
    "                                                                   axis=1)\n",
    "    df_daily = df_daily.reset_index()\n",
    "    #replace np.nan duration by 0\n",
    "    df_daily.replace(np.nan,0, inplace=True)\n",
    "    df_daily['verification_daily_total_nbr_hour'] = df_daily['verification_daily_total_duration'].map(lambda x: x/60/60)\n",
    "    print('The number of hours per \\\"level\\\" period is of:')\n",
    "    display(df_daily.groupby(['verification_daily_total_nbr_hour'])['level','HenID'].agg(lambda x: list(x)).reset_index())\n",
    "\n",
    "    #create an ordered list of the normalized duration per zone for chi2distance later (hen will first be sorted by entropy, and \n",
    "    #hence we will do this at the end)\n",
    "    li_zone_dur = [c for c in df_daily.columns if c.startswith('duration_')] #keep same order\n",
    "    df_daily['dur_values'] = df_daily.apply(lambda x: str([x[i] for i in li_zone_dur]), axis=1)\n",
    "    df_daily['dur_values'] = df_daily['dur_values'].map(lambda x: eval(x))\n",
    "    df_daily['dur_values_normalized'] = df_daily['dur_values'].map(lambda x: [i/float(np.sum(x)) if float(np.sum(x))!=0 else 0 for i in x])\n",
    "    \n",
    "    ########################################################\n",
    "    print('----------------- first time stamp in each zone per day....')\n",
    "    df_ = df.groupby(['HenID', 'level','Zone'])[timestamp_name].agg(lambda x: min(list(x))).reset_index()\n",
    "    #agg function = 'first' ats its string value, and the default function is the mean. Here by construction df_ has unique such \n",
    "    #values\n",
    "    df__ = df_.pivot_table(values=timestamp_name, index=['HenID', 'level'], columns='Zone', aggfunc='first')\n",
    "    df__.rename(columns={x:'FirstTimestamp_'+x for x in li_Zone}, inplace=True)\n",
    "    df__ = df__.reset_index()\n",
    "    df_daily = pd.merge(df_daily, df__, how='outer', on=['HenID','level'])\n",
    "\n",
    "    ########################################################\n",
    "    print('----------------- number of Zone (excluding nan)....')\n",
    "    df_ = df[~df['Zone'].isnull()].groupby(['HenID','level'])['Zone'].agg(lambda x: len(set((x)))).reset_index()\n",
    "    df_.rename(columns={'Zone':'Total_number_zone'}, inplace=True)\n",
    "    df_daily = pd.merge(df_daily, df_, how='outer', on=['HenID','level'])\n",
    "    \n",
    "    \n",
    "    ########################################################        \n",
    "    #compute some variables based on a list of zones over a day, where each zone count for the same nbr_sec second\n",
    "    #e.g.[einstreu,eintreu,rampe,rampe.....]\n",
    "    #excluding empty zones, because it influences for exemple the entropy computation (if full of nan, then might be more predictable)    \n",
    "    print('----------------- compute some variables based on a list of zones over a day....')\n",
    "                        \n",
    "    df_ = df[~df['Zone'].isnull()].groupby(['HenID','level']).agg(\n",
    "           list_of_durations=pd.NamedAgg(column='Zone', aggfunc=lambda x: list_of_durations(x, nbr_sec)),\n",
    "           zone_list=pd.NamedAgg(column='Zone', aggfunc=lambda x: tuple(x)),\n",
    "           Max_duration_zones=pd.NamedAgg(column='Zone', aggfunc=lambda x: max_duration_zones(x)),\n",
    "           dico_duration_stats=pd.NamedAgg(column='Zone', aggfunc=lambda x: dico_duration_stats(x, nbr_sec)),\n",
    "           dico_zone_sortedduration=pd.NamedAgg(column='Zone', aggfunc=lambda x: dico_zone_sortedduration(x, nbr_sec)),\n",
    "           Total_number_transition=pd.NamedAgg(column='Zone', aggfunc=lambda x: nbr_transition(list((x)))),\n",
    "           nbr_bouts=pd.NamedAgg(column='Zone', aggfunc=lambda x: nbr_bouts_per_zone(list((x))))).reset_index()\n",
    "\n",
    "    df_daily = pd.merge(df_daily, df_, how='outer', on=['HenID','level'])\n",
    "    for z in li_Zone:\n",
    "        df_daily['nbr_bouts_'+z] = df_daily['nbr_bouts'].map(lambda x: x.get(z,0))\n",
    "    df_daily.drop(['nbr_bouts'], inplace=True, axis=1)\n",
    "    \n",
    "    #add info from stats of the duration list (from a dictionary column into x (=len(dico)) columns)\n",
    "    df_daily = pd.concat([df_daily.drop(['dico_duration_stats'], axis=1), df_daily['dico_duration_stats'].apply(pd.Series)], axis=1)\n",
    "        \n",
    "        \n",
    "    ################################################################################################################\n",
    "    ############################# add basics hens info, remove unwanted dates and save #############################\n",
    "    ################################################################################################################\n",
    "            \n",
    "    #add basics hens info\n",
    "    #download info on henID associtation to (TagID,date) \n",
    "    df_FB = pd.read_excel(path_FocalBird, parse_dates=['StartDate','EndDate'])\n",
    "    df_FB['HenID'] = df_FB['HenID'].map(lambda x: 'hen_'+str(x))\n",
    "    df_FB = df_FB[df_FB['ShouldBeExcluded']!='yes']\n",
    "    df_FB['EndDate'].fillna(date_max+dt.timedelta(days=1), inplace=True)\n",
    "    \n",
    "    #Note: the HenID was already match according to the correct dates. \n",
    "    #Assumption:Each henID is linked to a unique PenID!\n",
    "    df_daily = pd.merge(df_daily, df_FB[['HenID','PenID','CLASS','29-09 weight']], on=['HenID'], how='left')\n",
    "\n",
    "    #remove dates with health care\n",
    "    print('-------------- Lets remove unwanted dates that impacted ALL PENS')\n",
    "    if len(li_date2remove)!=0:\n",
    "        df_daily['date_toberemoved'] = df_daily['level'].map(lambda x: x in li_date2remove)\n",
    "        x0 = df_daily.shape[0]\n",
    "        df_daily = df_daily[~df_daily['date_toberemoved']]\n",
    "        print_color((('By removing the unwanted days we passed from %d to %d timestamp (losing '%(x0,\n",
    "                    df_daily.shape[0]),'black'), (x0-df_daily.shape[0],'red'),(' timestamp)','black')))    \n",
    "\n",
    "    #remove dates linked to specific system\n",
    "    print('-------------- Lets remove unwanted dates that impacted FEW PENS')\n",
    "    if len(dico_date2remove_pens)!=0:\n",
    "        df_daily['date_2remove_penper'] = df_daily.apply(lambda x: int(x['PenID']) in dico_date2remove_pens[x['level']], axis=1)\n",
    "        x0 = df_daily.shape[0]\n",
    "        df_daily = df_daily[~df_daily['date_2remove_penper']]\n",
    "        print_color((('By removing the unwanted days we passed from %d to %d timestamp (losing '%(x0,\n",
    "                    df_daily.shape[0]),'black'), (x0-df_daily.shape[0],'red'),(' timestamp)','black')))   \n",
    "        \n",
    "    #remove dates linked to specific hens\n",
    "    print('-------------- Lets remove dates that impacted FEW HENS')\n",
    "    #create a dictionary with henID as keys and a list of tracking-active days\n",
    "    dico_hen_activedate = defaultdict(list)\n",
    "    for i in range(df_FB.shape[0]):\n",
    "        x = df_FB.iloc[i]\n",
    "        li_dates = pd.date_range(start=x['StartDate']+dt.timedelta(days=1), \n",
    "                                 end=x['EndDate']-dt.timedelta(days=1), freq='D')\n",
    "        dico_hen_activedate[x['HenID']].extend([dt.datetime.date(d) for d in li_dates])\n",
    "    df_daily['level'] = df_daily['level'].map(lambda x: dt.datetime.date(x))\n",
    "    df_daily['date_2remove_penhen'] = df_daily.apply(lambda x: x['level'] not in dico_hen_activedate[x['HenID']], axis=1)\n",
    "    x0 = df_daily.shape[0]\n",
    "    df_daily = df_daily[~df_daily['date_2remove_penhen']]\n",
    "    print_color((('By removing the unwanted days we passed from %d to %d timestamp (losing '%(x0,\n",
    "                df_daily.shape[0]),'black'), (x0-df_daily.shape[0],'red'),(' timestamp)','black')))      \n",
    "\n",
    "    #save\n",
    "    df_daily.drop(['verification_daily_total_nbr_hour','zone_list'],inplace=True,axis=1) #verification_daily_total_duration\n",
    "    df_daily.to_csv(os.path.join(path_extracted_data, id_run+'_'+ts_name+'_'+name_+'_variables.csv'), sep=';', index=False)\n",
    "\n",
    "    END_TIME = time.clock()\n",
    "    print (\"Total running time: %.2f mn\" %((END_TIME-START_TIME)/60))\n",
    "\n",
    "    \n",
    "    return(df_daily)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
