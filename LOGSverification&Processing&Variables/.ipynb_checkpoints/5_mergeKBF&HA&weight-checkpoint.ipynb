{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic package\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import operator\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cdist #for euclidean distance of consistency\n",
    "from numpy import inf\n",
    "import networkx as nx\n",
    "from dtaidistance import dtw\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "import uuid #to generate random id\n",
    "import pickle #to save/load list of selected hens\n",
    "import cv2\n",
    "\n",
    "#test equal variance\n",
    "from scipy.stats import levene\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import AgglomerativeClustering \n",
    "\n",
    "#scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "#interpolate curves for clustering among birds with not exact same days tracked\n",
    "#from scipy.interpolate import interp1d\n",
    "\n",
    "#modelling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, cohen_kappa_score, r2_score,\\\n",
    "mean_squared_error, mean_absolute_error, explained_variance_score#catboost, for a better support of categorical data\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from scipy.stats import pearsonr, spearmanr \n",
    "import scipy.stats as stats\n",
    "\n",
    "#PCA\n",
    "from sklearn import decomposition\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import KMeans #only numerical var\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import kmodes\n",
    "from kmodes.kmodes import KModes #with categorical var as well\n",
    "\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dexplot as dxp #for barplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#network\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "from UTILS import chi2_distance, ts_visual, time_series_henColumn_tsRow, correct_key, FB_daily\n",
    "import config_origins as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change the configuration file if not done yet!\n",
      "correctlightschedule_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print('change the configuration file if not done yet!')\n",
    "path_extracted_data = config.path_extracted_data\n",
    "path_dataoutput = config.path_dataoutput\n",
    "path_initial_data = config.path_initial_data\n",
    "id_run = config.id_run\n",
    "#id_run = 'chapter0_final_'\n",
    "#path_dataoutput = r'G:\\VPHI\\Welfare\\2- Research Projects\\OFHE2.OriginsE2\\DataOutput'\n",
    "#path_extracted_data = os.path.join(path_dataoutput,'TrackingSystem') \n",
    "#path_extracted_data = os.path.join(path_extracted_data, id_run)\n",
    "dico_night_hour = config.dico_night_hour\n",
    "dico_matching = config.dico_matching\n",
    "li_binmn = config.li_binmn\n",
    "penalty = config.penalty\n",
    "dico_HAID_date = config.dico_HAID_date\n",
    "dico_window = config.dico_window\n",
    "path_extracted_HA = config.path_extracted_HA\n",
    "path_extracted_HA_visual = config.path_extracted_HA_visual\n",
    "path_extracted_data_SNA = config.path_extracted_data_SNA\n",
    "#create a director if not existing\n",
    "if not os.path.exists(path_extracted_HA_visual):\n",
    "    os.makedirs(path_extracted_HA_visual)\n",
    "print(id_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HenID</th>\n",
       "      <th>PenID</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>R-Pen</th>\n",
       "      <th>InitialStartDate</th>\n",
       "      <th>29-09 weight</th>\n",
       "      <th>10-12 juin weight</th>\n",
       "      <th>weight 23-11-2020</th>\n",
       "      <th>weight 04-01-2021</th>\n",
       "      <th>weight 01-02-21</th>\n",
       "      <th>weight 12-04-21</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>percentage_of_gain_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>9</td>\n",
       "      <td>Other</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1696.5</td>\n",
       "      <td>1787.8</td>\n",
       "      <td>1800.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OFH</td>\n",
       "      <td>49.603175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hen_10</td>\n",
       "      <td>11</td>\n",
       "      <td>LEXP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>70.4</td>\n",
       "      <td>1488.3</td>\n",
       "      <td>1628.4</td>\n",
       "      <td>1602.1</td>\n",
       "      <td>1587.2</td>\n",
       "      <td>OFH</td>\n",
       "      <td>39.093458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hen_100</td>\n",
       "      <td>5</td>\n",
       "      <td>Other</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>1182.0</td>\n",
       "      <td>80.5</td>\n",
       "      <td>1642.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OFH</td>\n",
       "      <td>38.959391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     HenID  PenID  CLASS  R-Pen InitialStartDate  29-09 weight  \\\n",
       "0    hen_1      9  Other    1.0       2020-06-10        1134.0   \n",
       "1   hen_10     11   LEXP    1.0       2020-06-10        1070.0   \n",
       "2  hen_100      5  Other    3.0       2020-06-10        1182.0   \n",
       "\n",
       "   10-12 juin weight  weight 23-11-2020  weight 04-01-2021  weight 01-02-21  \\\n",
       "0               57.0             1696.5             1787.8           1800.9   \n",
       "1               70.4             1488.3             1628.4           1602.1   \n",
       "2               80.5             1642.5                NaN              NaN   \n",
       "\n",
       "   weight 12-04-21 Treatment  percentage_of_gain_weight  \n",
       "0              NaN       OFH                  49.603175  \n",
       "1           1587.2       OFH                  39.093458  \n",
       "2              NaN       OFH                  38.959391  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#focal birds info (one row per bird) for weight\n",
    "df_FB = pd.read_csv(os.path.join(path_extracted_data,id_run+'df_FOCALBIRDS.csv'), sep=';', parse_dates=['InitialStartDate'],\n",
    "                     dayfirst=True) \n",
    "df_FB['percentage_of_gain_weight'] = df_FB.apply(lambda x: (x['weight 23-11-2020']-x['29-09 weight'])/x['29-09 weight']*100, axis=1)\n",
    "dico_cl_name = {'EPI': 'Other', 'MEXP':'MEXP', 'LEXP': 'LEXP', 'LEXPLOST':'Other','MEXPLOST':'Other','NewAfterEpi':'Other'}\n",
    "df_FB['CLASS'] = df_FB['CLASS'].map(lambda x: dico_cl_name[x])\n",
    "print(df_FB.shape)\n",
    "df_FB.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HA1\n"
     ]
    }
   ],
   "source": [
    "#HA - feather & leg injuries/podo/bumble\n",
    "#search in any folder and creat a list of img paths\n",
    "#os.walk: Python method walk() generates the file names in a directory tree by walking the tree top-down or bottom-up\n",
    "#to avoid saving images several times, we will do it specifically for images in the KBF folder:\n",
    "li_df = []\n",
    "for path_ in glob.glob(os.path.join(path_dataoutput, 'HA', 'HA*', 'HA*.csv')):\n",
    "    HAID = path_.split('\\\\')[-1].split('.')[0]\n",
    "    print('---------------------', HAID)\n",
    "    df_ = pd.read_csv(path_, sep=';') \n",
    "    df_['HAID'] = HAID\n",
    "    df_['date'] = dico_HAID_date[HAID]\n",
    "    df_ = df_.rename(columns={'Pen ':'Pen'})\n",
    "    df_ = df_[~df_['FocalLegringName'].isnull()]\n",
    "    print(df_.shape)\n",
    "    #print(df_.columns)\n",
    "    #display(df_.head(2))\n",
    "    li_df.append(df_)\n",
    "df_HA = pd.concat(li_df)\n",
    "#small typo corrections\n",
    "df_HA['BirdType'] = df_HA['BirdType'].fillna('normal')\n",
    "df_HA['BirdType'] = df_HA['BirdType'].map(lambda x: x.strip(' '))\n",
    "df_HA = df_HA.rename(columns={'Pen':'PenID'})\n",
    "df_HA['PenID'] = df_HA['PenID'].map(lambda x: 'pen'+str(int(x)))\n",
    "print(df_HA.columns)\n",
    "print(df_HA.shape)\n",
    "df_HA.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KBF\n",
    "df_KBF = pd.read_csv(os.path.join(path_extracted_HA, 'KBF_ALL_RECORDS.csv'), sep=';', engine='python') \n",
    "df_KBF = df_KBF[~df_KBF['BirdID'].isnull()]\n",
    "df_KBF['Info'] = df_KBF['BirdID'].map(lambda x: re.split('(\\d+)',x.strip(' ')))\n",
    "df_KBF['PenID'] = df_KBF['Info'].map(lambda x: 'pen'+str(int(x[3])))\n",
    "df_KBF['FocalLegringName'] = df_KBF['Info'].map(lambda x: str(int(x[1]))+str(x[2]))\n",
    "df_KBF['backpack'] = df_KBF['Info'].map(lambda x: x[-1])\n",
    "print(df_KBF.shape)\n",
    "df_KBF.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add HenID & weight to HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## create one dataframae with all HA and with HenID!\n",
    "#open info\n",
    "df_FB_daily = FB_daily(config)\n",
    "print(df_FB_daily.shape)\n",
    "df_FB_daily['PenID'] = df_FB_daily['PenID'].map(lambda x: 'pen'+str(int(x)))\n",
    "display(df_FB_daily[['date','FocalLegringName','PenID','HenID']].head(3))\n",
    "df_FB_daily['date'].iloc[0]\n",
    "#remove typos\n",
    "for v in ['FocalLegringName','PenID']:\n",
    "    df_HA[v] = df_HA[v].map(lambda x: x.strip(' '))\n",
    "    df_FB_daily[v] = df_FB_daily[v].map(lambda x: x.strip(' '))\n",
    "#as epi and epi_new are exchanged during HA; we should take the date before/ respec. after HA to match. \n",
    "#to avoid any confusioon we enter them by hand (in column SomeHenID). Same for new legrings!\n",
    "df_HA_HenID = pd.merge(df_HA, df_FB_daily[['date','FocalLegringName','PenID','HenID']], \n",
    "                  on=['date','FocalLegringName','PenID'], how='left') \n",
    "print(df_HA.shape, df_FB_daily.shape, df_HA_HenID.shape)\n",
    "#add the manually the anotated HenID (to avoid mistakes, in the case we would change legring or have two \n",
    "#birds from same pen with same legringID (different tagID), Then we would simply annotated manually the HenID when entering \n",
    "#the HA observations)\n",
    "df_HA_HenID['HenID'] = np.where(~df_HA_HenID['SomeHenID'].isnull(), df_HA_HenID['SomeHenID'], df_HA_HenID['HenID']) \n",
    "#small verification\n",
    "#display(df_HA_HenID[~df_HA_HenID['SomeHenID'].isnull()][['SomeHenID','HenID']])\n",
    "###small verification\n",
    "#print(df_HA_HenID[df_HA_HenID['HenID'].isnull()][['date','HAID','PenID','HenID','FocalLegringName']].shape)\n",
    "#df_ = df_HA_HenID[df_HA_HenID['HenID'].isnull()][['date','HAID','PenID','HenID','FocalLegringName']].groupby(['date','HAID','PenID'])['FocalLegringName'].agg(lambda x: set(x)).reset_index()\n",
    "#df_['nbr_NA_legringID'] = df_['FocalLegringName'].map(lambda x: len(x))\n",
    "#bird 35 black on HA1 pen 8 was excluded due to missfucntioning tag: this it doe snot match any henid, but thats normal\n",
    "#print('HERE IS THE OBSERVAION DURING HA THAT DO NOT MATCH ANY HENID:')\n",
    "#display(df_)\n",
    "display(df_HA_HenID[df_HA_HenID['HenID'].isnull()])\n",
    "df_HA_HenID = df_HA_HenID[~df_HA_HenID['HenID'].isnull()]\n",
    "df_HA_HenID['HenID'] = df_HA_HenID['HenID'].map(lambda x: 'hen_'+str(int(x)))\n",
    "#small typos\n",
    "#for now we replace nan with the one we will select, and then we select randomly\n",
    "df_HA_HenID['Person_toe'] = df_HA_HenID['Person_toe'].fillna('C')\n",
    "df_HA_HenID['Person_feather'] = df_HA_HenID['Person_feather'].fillna('Sabine')\n",
    "df_HA_HenID['Person_toe'] = df_HA_HenID['Person_toe'].map(lambda x: x.strip(' '))\n",
    "df_HA_HenID['Person_feather'] = df_HA_HenID['Person_feather'].map(lambda x: x.strip(' '))\n",
    "print(df_HA_HenID.shape)\n",
    "df_HA_HenID.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(df_HA_HenID[(df_HA_HenID['PenID']=='pen11')&(df_HA_HenID['HAID']=='HA4')&(df_HA_HenID['FocalLegringName']=='28green')][['HAID','Person_feather','Person_toe','Reliability_MC','Reliability_SS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep when its a reliability only Sabine (as Sabine learned to Satar) THEN take randomly among several remaining obs.\n",
    "#ATTENTION when reliability =+: we should have at least two people rating it!\n",
    "df_HA_HenID['toremove'] = df_HA_HenID.apply(lambda x: (x['Reliability_SS']==1)&(x['Person_feather']=='Satar'), axis=1)\n",
    "print(df_HA_HenID.shape)\n",
    "df_HA_HenID = df_HA_HenID[~df_HA_HenID['toremove']]\n",
    "print(df_HA_HenID.shape)\n",
    "#remove random observation if there is two from reliability\n",
    "random.seed(0) #replicable way\n",
    "df_HA_HenID = shuffle(df_HA_HenID)\n",
    "df_HA_HenID.drop_duplicates(subset=['HenID','HAID'], keep='first', inplace=True)\n",
    "#merge all HA from reliability using a random obs for now (then most reliable, once we know and once we have all the name)\n",
    "df_ = df_HA_HenID.groupby(['HenID','HAID'])['Person_toe','Person_feather'].agg(lambda x: list(x)).reset_index()\n",
    "df_['nbr_raters'] = df_['Person_feather'].map(lambda x: len(x))\n",
    "if df_[df_['nbr_raters']>1].shape[0]>0:\n",
    "    print('ERROR, we still have several observation for a/some (HenID,HAID)')\n",
    "    sys.exit()\n",
    "print(df_HA_HenID.shape)\n",
    "df_HA_HenID.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(df_HA_HenID[(df_HA_HenID['PenID']=='pen11')&(df_HA_HenID['HAID']=='HA4')&(df_HA_HenID['FocalLegringName']=='28green')][['HAID','Person_feather','Person_toe','Reliability_MC','Reliability_SS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbr of HA per hen\n",
    "df_HA_HenID.groupby('HenID')['HAID'].count().reset_index().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add weight to (HenID,HAID)\n",
    "dico_HAID_h_weight = {}\n",
    "dico_HAID_h_weight['HA1'] = dict(zip(df_FB['HenID'].tolist(), df_FB['weight 23-11-2020'].tolist()))\n",
    "dico_HAID_h_weight['HA2'] = dict(zip(df_FB['HenID'].tolist(), df_FB['weight 04-01-2021'].tolist()))\n",
    "dico_HAID_h_weight['HA3'] = dict(zip(df_FB['HenID'].tolist(), df_FB['weight 01-02-21'].tolist()))\n",
    "dico_HAID_h_weight['HA4'] = dict(zip(df_FB['HenID'].tolist(), df_FB['weight 12-04-21'].tolist()))\n",
    "df_HA_HenID['weight'] = df_HA_HenID.apply(lambda x: dico_HAID_h_weight[x['HAID']].get(x['HenID'],np.nan), axis=1)\n",
    "print('Note that if a hen is in df_HA_HenID, it also should have the weight, look at exception:')\n",
    "display(df_HA_HenID[df_HA_HenID['weight'].isnull()])\n",
    "df_HA_HenID.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add WOA and DOA\n",
    "df_HA_HenID['DOA'] = df_HA_HenID['date'].map(lambda x: (x-dt.datetime(2020,6,3)).days)\n",
    "df_HA_HenID['WOA'] = df_HA_HenID['DOA'].map(lambda x: math.ceil(x/7))\n",
    "df_HA_HenID['WOA'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add treatment and class\n",
    "df_ = df_FB.groupby(['HenID'])[['Treatment', 'CLASS']].agg(lambda x: set(x)).reset_index()\n",
    "df_['Treatment'] = df_['Treatment'].map(lambda x: list(x)[0] if len(x)==1 else 'error')\n",
    "df_['CLASS'] = df_['CLASS'].map(lambda x: list(x)[0] if len(x)==1 else 'error')\n",
    "df_[(df_['CLASS']=='error')|(df_['Treatment']=='error')]\n",
    "print(df_HA_HenID.shape)\n",
    "df_HA_HenID = pd.merge(df_HA_HenID, df_, on='HenID', how='left')\n",
    "print(df_HA_HenID.shape)\n",
    "df_HA_HenID.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add KBF ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HA_KBF = pd.merge(df_HA_HenID, df_KBF, on=['HAID','PenID','FocalLegringName'], how='outer')\n",
    "print(df_HA_HenID.shape, df_KBF.shape, df_HA_KBF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_HA_KBF[(df_HA_KBF['FocalLegringName']=='9grey')&(df_HA_KBF['HAID']=='HA1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_HA_HenID[(df_HA_HenID['PenID']=='pen4')&(df_HA_HenID['HAID']=='HA4')]#[['FocalLegringName', 'imageID','backpack','HAID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_KBF[(df_KBF['PenID']=='pen11')&(df_KBF['HAID']=='HA4')][['FocalLegringName', 'imageID','backpack','HAID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#HA1, HA3, HA4 for now we have only\n",
    "#df_HA_KBF[df_HA_KBF['severity'].isnull()].groupby(['HAID','PenID'])[['FocalLegringName']].agg(lambda x: set(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#35black from pen 8 in HA1 is removed so its normal (s tag disfunctioning during that time)\n",
    "#df_HA_KBF[df_HA_KBF['HenID'].isnull()].groupby(['HAID','PenID'])['FocalLegringName'].agg(lambda x: set(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "df_HA_KBF.to_csv(os.path.join(path_extracted_data,'df_all_HA.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do visual of HenID KBF over time\n",
    "df_plot = df_HA_KBF.sort_values('HAID').copy()\n",
    "df_plot = df_plot[~df_plot['actual_path_img'].isnull()] #missing KBF\n",
    "df_plot = df_plot.groupby(['HenID'])[['HAID','actual_path_img','severity','has_gap']].agg(lambda x: list(x)).reset_index()\n",
    "for i in tqdm.tqdm(range(df_plot.shape[0])):\n",
    "    x = df_plot.iloc[i]\n",
    "    li_path = x['actual_path_img']\n",
    "    li_HAID = x['HAID']\n",
    "    henid = x['HenID']\n",
    "    li_sev = x['severity']\n",
    "    li_gap = x['has_gap']\n",
    "    c = len(li_path) ; l = 1\n",
    "    fig = plt.figure(figsize=(c*3, l*3)) #/100 si trop grand nbr\n",
    "    for i,path_ in enumerate(li_path):\n",
    "        plt.subplot(l,c,i+1)\n",
    "        plt.tight_layout() #avoid titles superpositions\n",
    "        img = cv2.imread(path_)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([]) #remove xlabel annotations\n",
    "        plt.yticks([])\n",
    "        plt.title(li_HAID[i]+' sev:'+str(li_sev[i])+' gap:'+str(li_gap[i]), size=9)\n",
    "    plt.savefig(os.path.join(path_extracted_HA_visual,'KBF_'+henid+'.png'),dpi=300,format='png',bbox_inches='tight')\n",
    "    plt.close() #otherwise it will print all plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HA_HenID.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HA_HenID['Diarrhea'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_HAvar = ['CombPecks', 'RFPododermatitis', 'RFBumblefoot', 'RFinjuries',\n",
    "            'LFPododermatitis', 'LFBumblefoot', 'LFinjuries','NeckFeather',\n",
    "            'BreastFeather', 'CloacaFeather', 'BackFeather', 'WingFeather','TailFeather','weight'] #Diarrhea\n",
    "df_HA_HenID[['HenID','date']+li_HAvar].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HA_HenID['Toes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HA_HenID['Claw'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for v in li_HAvar:\n",
    "    ax = sns.boxplot(x=\"WOA\", y=v, data=df_HA_HenID);\n",
    "    plt.title(v.upper())\n",
    "    plt.savefig(os.path.join(path_extracted_HA_visual,v+'.png'),dpi=300,format='png',bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HA_KBF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
