{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic package\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import operator\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cdist #for euclidean distance of consistency\n",
    "from numpy import inf\n",
    "import networkx as nx\n",
    "from dtaidistance import dtw\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "import uuid #to generate random id\n",
    "import pickle #to save/load list of selected hens\n",
    "import cv2\n",
    "\n",
    "#test equal variance\n",
    "from scipy.stats import levene\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import AgglomerativeClustering \n",
    "\n",
    "#scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "#interpolate curves for clustering among birds with not exact same days tracked\n",
    "#from scipy.interpolate import interp1d\n",
    "\n",
    "#modelling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, cohen_kappa_score, r2_score,\\\n",
    "mean_squared_error, mean_absolute_error, explained_variance_score#catboost, for a better support of categorical data\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from scipy.stats import pearsonr, spearmanr \n",
    "import scipy.stats as stats\n",
    "\n",
    "#PCA\n",
    "from sklearn import decomposition\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import KMeans #only numerical var\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import kmodes\n",
    "from kmodes.kmodes import KModes #with categorical var as well\n",
    "\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dexplot as dxp #for barplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#network\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "from UTILS import chi2_distance, ts_visual, time_series_henColumn_tsRow, correct_key, FB_daily\n",
    "import config_origins as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change the configuration file if not done yet!\n",
      "correctlightschedule_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print('change the configuration file if not done yet!')\n",
    "path_extracted_data = config.path_extracted_data\n",
    "path_dataoutput = config.path_dataoutput\n",
    "path_initial_data = config.path_initial_data\n",
    "id_run = config.id_run\n",
    "#id_run = 'chapter0_final_'\n",
    "#path_dataoutput = r'G:\\VPHI\\Welfare\\2- Research Projects\\OFHE2.OriginsE2\\DataOutput'\n",
    "#path_extracted_data = os.path.join(path_dataoutput,'TrackingSystem') \n",
    "#path_extracted_data = os.path.join(path_extracted_data, id_run)\n",
    "dico_night_hour = config.dico_night_hour\n",
    "dico_matching = config.dico_matching\n",
    "li_binmn = config.li_binmn\n",
    "penalty = config.penalty\n",
    "dico_HAID_date = config.dico_HAID_date\n",
    "dico_window = config.dico_window\n",
    "path_extracted_HA = config.path_extracted_HA\n",
    "path_extracted_HA_visual = config.path_extracted_HA_visual\n",
    "path_extracted_data_SNA = config.path_extracted_data_SNA\n",
    "#create a director if not existing\n",
    "if not os.path.exists(path_extracted_HA_visual):\n",
    "    os.makedirs(path_extracted_HA_visual)\n",
    "print(id_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#network var\n",
    "df_SNA = pd.read_csv(os.path.join(path_extracted_data,id_run+'_df_SNA.csv'), sep=';', parse_dates=['date'], dayfirst=True) \n",
    "df_SNA.rename({'date':'level'},axis=1,inplace=True)\n",
    "print(df_SNA.shape)\n",
    "display(df_SNA.head(3))\n",
    "li_df = []\n",
    "for v in ['closeness', 'betweeness', 'degree']:\n",
    "    df_n = df_SNA.pivot(index=['HenID', 'level'], columns='distance_measure', values=v).reset_index()\n",
    "    df_n.rename({x:v+'_'+x for x in df_n.columns if x.startswith('DTW_')}, axis=1, inplace=True)\n",
    "    li_df.append(df_n)\n",
    "#only display info on the last df    \n",
    "print(df_n.shape)\n",
    "display(df_n.head(3))\n",
    "df_SNA_daily = pd.merge(li_df[0], li_df[1], on=['HenID','level'], how='outer')\n",
    "df_SNA_daily = pd.merge(df_SNA_daily, li_df[2], on=['HenID','level'], how='outer')\n",
    "print(df_SNA_daily.shape)\n",
    "df_SNA_daily.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#daily variables (one row per (henID, date))\n",
    "df_daily = pd.read_csv(os.path.join(path_extracted_data, 'daily_ALL_Variable_Tranformed.csv'), sep=';',\n",
    "                     parse_dates=['level'], dayfirst=True) \n",
    "df_daily['DOA'] = df_daily['level'].map(lambda x: (x-dt.datetime(2020,6,3)).days) \n",
    "df_daily['weeks_in_laying_barn'] = df_daily['DOA'].map(lambda x: int((x-119)/7)+1)\n",
    "#df_daily['weeks_in_laying_barn'] = df_daily['DOA'].map(lambda x: math.ceil(x/7))\n",
    "#first two month seems good from the %of hens not moving plot! and sounds good too (twice longer thatn they need to start moving)\n",
    "print(df_daily.shape)\n",
    "\n",
    "#filter by date\n",
    "df_daily = df_daily[df_daily['level']>dt.datetime(2020,9,29)]\n",
    "print(df_daily.shape)\n",
    "\n",
    "#remove days that are not fully recorded\n",
    "df_daily['nbr_sec_per_day'] = df_daily['level'].map(lambda x: dico_night_hour[correct_key(x,dico_night_hour)]['nbr_hour']*60*60)\n",
    "df_daily['is_correct_amount_time'] = df_daily.apply(lambda x: x['nbr_sec_per_day']==x['verification_daily_total_duration'], axis=1)\n",
    "df_daily[(~df_daily['is_correct_amount_time'])&(~df_daily['Total_number_transition'].isnull())][['level', 'HenID', 'Total_number_transition', 'dur_values', 'verification_daily_total_duration','nbr_sec_per_day']]\n",
    "display(df_daily = df_daily[~((~df_daily['is_correct_amount_time'])&(~df_daily['Total_number_transition'].isnull()))])\n",
    "print(df_daily.shape)\n",
    "df_daily.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#focal birds info (one row per bird) for weight\n",
    "df_FB = pd.read_csv(os.path.join(path_extracted_data,id_run+'df_FOCALBIRDS.csv'), sep=';', parse_dates=['InitialStartDate'],\n",
    "                     dayfirst=True) \n",
    "df_FB['percentage_of_gain_weight'] = df_FB.apply(lambda x: (x['weight 23-11-2020']-x['29-09 weight'])/x['29-09 weight']*100, axis=1)\n",
    "print(df_FB.shape)\n",
    "df_FB.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#HA - feather & leg injuries/podo/bumble\n",
    "#search in any folder and creat a list of img paths\n",
    "#os.walk: Python method walk() generates the file names in a directory tree by walking the tree top-down or bottom-up\n",
    "#to avoid saving images several times, we will do it specifically for images in the KBF folder:\n",
    "li_df = []\n",
    "for path_ in glob.glob(os.path.join(path_dataoutput, 'HA', 'HA*', 'HA*.csv')):\n",
    "    HAID = path_.split('\\\\')[-1].split('.')[0]\n",
    "    print('---------------------', HAID)\n",
    "    df_ = pd.read_csv(path_, sep=';') \n",
    "    df_['HAID'] = HAID\n",
    "    df_['date'] = dico_HAID_date[HAID]\n",
    "    df_ = df_.rename(columns={'Pen ':'Pen'})\n",
    "    df_ = df_[~df_['FocalLegringName'].isnull()]\n",
    "    print(df_.shape)\n",
    "    #print(df_.columns)\n",
    "    #display(df_.head(2))\n",
    "    li_df.append(df_)\n",
    "df_HA = pd.concat(li_df)\n",
    "#small typo corrections\n",
    "df_HA['BirdType'] = df_HA['BirdType'].fillna('normal')\n",
    "df_HA['BirdType'] = df_HA['BirdType'].map(lambda x: x.strip(' '))\n",
    "df_HA = df_HA.rename(columns={'Pen':'PenID'})\n",
    "df_HA['PenID'] = df_HA['PenID'].map(lambda x: 'pen'+str(int(x)))\n",
    "print(df_HA.columns)\n",
    "print(df_HA.shape)\n",
    "df_HA.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KBF\n",
    "df_KBF = pd.read_csv(os.path.join(path_extracted_HA, 'KBF_ALL_RECORDS.csv'), sep=';', engine='python') \n",
    "df_KBF = df_KBF[~df_KBF['BirdID'].isnull()]\n",
    "df_KBF['Info'] = df_KBF['BirdID'].map(lambda x: re.split('(\\d+)',x.strip(' ')))\n",
    "df_KBF['PenID'] = df_KBF['Info'].map(lambda x: 'pen'+str(int(x[3])))\n",
    "df_KBF['FocalLegringName'] = df_KBF['Info'].map(lambda x: str(int(x[1]))+str(x[2]))\n",
    "df_KBF['backpack'] = df_KBF['Info'].map(lambda x: x[-1])\n",
    "print(df_KBF.shape)\n",
    "df_KBF.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add HenID & weight to HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## create one dataframae with all HA and with HenID!\n",
    "#open info\n",
    "df_FB_daily = FB_daily(config)\n",
    "print(df_FB_daily.shape)\n",
    "df_FB_daily['PenID'] = df_FB_daily['PenID'].map(lambda x: 'pen'+str(int(x)))\n",
    "display(df_FB_daily[['date','FocalLegringName','PenID','HenID']].head(3))\n",
    "df_FB_daily['date'].iloc[0]\n",
    "#remove typos\n",
    "for v in ['FocalLegringName','PenID']:\n",
    "    df_HA[v] = df_HA[v].map(lambda x: x.strip(' '))\n",
    "    df_FB_daily[v] = df_FB_daily[v].map(lambda x: x.strip(' '))\n",
    "#as epi and epi_new are exchanged during HA; we should take the date before/ respec. after HA to match. \n",
    "#to avoid any confusioon we enter them by hand (in column SomeHenID). Same for new legrings!\n",
    "df_HA_HenID = pd.merge(df_HA, df_FB_daily[['date','FocalLegringName','PenID','HenID']], \n",
    "                  on=['date','FocalLegringName','PenID'], how='left') \n",
    "print(df_HA.shape, df_FB_daily.shape, df_HA_HenID.shape)\n",
    "#add the manually the anotated HenID (to avoid mistakes, in the case we would change legring or have two \n",
    "#birds from same pen with same legringID (different tagID), Then we would simply annotated manually the HenID when entering \n",
    "#the HA observations)\n",
    "df_HA_HenID['HenID'] = np.where(~df_HA_HenID['SomeHenID'].isnull(), df_HA_HenID['SomeHenID'], df_HA_HenID['HenID']) \n",
    "#small verification\n",
    "#display(df_HA_HenID[~df_HA_HenID['SomeHenID'].isnull()][['SomeHenID','HenID']])\n",
    "###small verification\n",
    "#print(df_HA_HenID[df_HA_HenID['HenID'].isnull()][['date','HAID','PenID','HenID','FocalLegringName']].shape)\n",
    "#df_ = df_HA_HenID[df_HA_HenID['HenID'].isnull()][['date','HAID','PenID','HenID','FocalLegringName']].groupby(['date','HAID','PenID'])['FocalLegringName'].agg(lambda x: set(x)).reset_index()\n",
    "#df_['nbr_NA_legringID'] = df_['FocalLegringName'].map(lambda x: len(x))\n",
    "#bird 35 black on HA1 pen 8 was excluded due to missfucntioning tag: this it doe snot match any henid, but thats normal\n",
    "#print('HERE IS THE OBSERVAION DURING HA THAT DO NOT MATCH ANY HENID:')\n",
    "#display(df_)\n",
    "display(df_HA_HenID[df_HA_HenID['HenID'].isnull()])\n",
    "df_HA_HenID = df_HA_HenID[~df_HA_HenID['HenID'].isnull()]\n",
    "df_HA_HenID['HenID'] = df_HA_HenID['HenID'].map(lambda x: 'hen_'+str(int(x)))\n",
    "#small typos\n",
    "#for now we replace nan with the one we will select, and then we select randomly\n",
    "df_HA_HenID['Person_toe'] = df_HA_HenID['Person_toe'].fillna('C')\n",
    "df_HA_HenID['Person_feather'] = df_HA_HenID['Person_feather'].fillna('Sabine')\n",
    "df_HA_HenID['Person_toe'] = df_HA_HenID['Person_toe'].map(lambda x: x.strip(' '))\n",
    "df_HA_HenID['Person_feather'] = df_HA_HenID['Person_feather'].map(lambda x: x.strip(' '))\n",
    "print(df_HA_HenID.shape)\n",
    "df_HA_HenID.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(df_HA_HenID[(df_HA_HenID['PenID']=='pen11')&(df_HA_HenID['HAID']=='HA4')&(df_HA_HenID['FocalLegringName']=='28green')][['HAID','Person_feather','Person_toe','Reliability_MC','Reliability_SS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep when its a reliability only Sabine (as Sabine learned to Satar) THEN take randomly among several remaining obs.\n",
    "#ATTENTION when reliability =+: we should have at least two people rating it!\n",
    "df_HA_HenID['toremove'] = df_HA_HenID.apply(lambda x: (x['Reliability_SS']==1)&(x['Person_feather']=='Satar'), axis=1)\n",
    "print(df_HA_HenID.shape)\n",
    "df_HA_HenID = df_HA_HenID[~df_HA_HenID['toremove']]\n",
    "print(df_HA_HenID.shape)\n",
    "#remove random observation if there is two from reliability\n",
    "random.seed(0) #replicable way\n",
    "df_HA_HenID = shuffle(df_HA_HenID)\n",
    "df_HA_HenID.drop_duplicates(subset=['HenID','HAID'], keep='first', inplace=True)\n",
    "#merge all HA from reliability using a random obs for now (then most reliable, once we know and once we have all the name)\n",
    "df_ = df_HA_HenID.groupby(['HenID','HAID'])['Person_toe','Person_feather'].agg(lambda x: list(x)).reset_index()\n",
    "df_['nbr_raters'] = df_['Person_feather'].map(lambda x: len(x))\n",
    "if df_[df_['nbr_raters']>1].shape[0]>0:\n",
    "    print('ERROR, we still have several observation for a/some (HenID,HAID)')\n",
    "    sys.exit()\n",
    "print(df_HA_HenID.shape)\n",
    "df_HA_HenID.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(df_HA_HenID[(df_HA_HenID['PenID']=='pen11')&(df_HA_HenID['HAID']=='HA4')&(df_HA_HenID['FocalLegringName']=='28green')][['HAID','Person_feather','Person_toe','Reliability_MC','Reliability_SS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbr of HA per hen\n",
    "df_HA_HenID.groupby('HenID')['HAID'].count().reset_index().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add weight to (HenID,HAID)\n",
    "dico_HAID_h_weight = {}\n",
    "dico_HAID_h_weight['HA1'] = dict(zip(df_FB['HenID'].tolist(), df_FB['weight 23-11-2020'].tolist()))\n",
    "dico_HAID_h_weight['HA2'] = dict(zip(df_FB['HenID'].tolist(), df_FB['weight 04-01-2021'].tolist()))\n",
    "dico_HAID_h_weight['HA3'] = dict(zip(df_FB['HenID'].tolist(), df_FB['weight 01-02-21'].tolist()))\n",
    "dico_HAID_h_weight['HA4'] = dict(zip(df_FB['HenID'].tolist(), df_FB['weight 12-04-21'].tolist()))\n",
    "df_HA_HenID['weight'] = df_HA_HenID.apply(lambda x: dico_HAID_h_weight[x['HAID']].get(x['HenID'],np.nan), axis=1)\n",
    "print('Note that if a hen is in df_HA_HenID, it also should have the weight, look at exception:')\n",
    "display(df_HA_HenID[df_HA_HenID['weight'].isnull()])\n",
    "df_HA_HenID.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add WOA and DOA\n",
    "df_HA_HenID['DOA'] = df_HA_HenID['date'].map(lambda x: (x-dt.datetime(2020,6,3)).days)\n",
    "df_HA_HenID['WOA'] = df_HA_HenID['DOA'].map(lambda x: math.ceil(x/7))\n",
    "df_HA_HenID['WOA'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add KBF ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HA.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HA_KBF = pd.merge(df_HA_HenID, df_KBF, on=['HAID','PenID','FocalLegringName'], how='outer')\n",
    "print(df_HA_HenID.shape, df_KBF.shape, df_HA_KBF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_HA_KBF[(df_HA_KBF['FocalLegringName']=='9grey')&(df_HA_KBF['HAID']=='HA1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_HA_HenID[(df_HA_HenID['PenID']=='pen4')&(df_HA_HenID['HAID']=='HA4')]#[['FocalLegringName', 'imageID','backpack','HAID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_KBF[(df_KBF['PenID']=='pen11')&(df_KBF['HAID']=='HA4')][['FocalLegringName', 'imageID','backpack','HAID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#HA1, HA3, HA4 for now we have only\n",
    "#df_HA_KBF[df_HA_KBF['severity'].isnull()].groupby(['HAID','PenID'])[['FocalLegringName']].agg(lambda x: set(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#35black from pen 8 in HA1 is removed so its normal (s tag disfunctioning during that time)\n",
    "#37orange is missing in HA1 pen 9?!\n",
    "#pen3 in HA4 is missing in HA: 25green, 49grey, 31grey, 14green\n",
    "#df_HA_KBF[df_HA_KBF['HenID'].isnull()].groupby(['HAID','PenID'])['FocalLegringName'].agg(lambda x: set(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "df_HA_KBF.to_csv(os.path.join(path_extracted_data,'df_all_HA.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HA_KBF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do visual of HenID KBF over time\n",
    "df_plot = df_HA_KBF.sort_values('HAID').copy()\n",
    "df_plot = df_plot[~df_plot['actual_path_img'].isnull()] #missing KBF\n",
    "df_plot = df_plot.groupby(['HenID'])[['HAID','actual_path_img','severity','has_gap']].agg(lambda x: list(x)).reset_index()\n",
    "for i in tqdm.tqdm(range(df_plot.shape[0])):\n",
    "    x = df_plot.iloc[i]\n",
    "    li_path = x['actual_path_img']\n",
    "    li_HAID = x['HAID']\n",
    "    henid = x['HenID']\n",
    "    li_sev = x['severity']\n",
    "    li_gap = x['has_gap']\n",
    "    c = len(li_path) ; l = 1\n",
    "    fig = plt.figure(figsize=(c*3, l*3)) #/100 si trop grand nbr\n",
    "    for i,path_ in enumerate(li_path):\n",
    "        plt.subplot(l,c,i+1)\n",
    "        plt.tight_layout() #avoid titles superpositions\n",
    "        img = cv2.imread(path_)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([]) #remove xlabel annotations\n",
    "        plt.yticks([])\n",
    "        plt.title(li_HAID[i]+' sev:'+str(li_sev[i])+' gap:'+str(li_gap[i]), size=9)\n",
    "    plt.savefig(os.path.join(path_extracted_HA_visual,'KBF_'+henid+'.png'),dpi=300,format='png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add SNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_daily and df_SNA_daily do not have the same lenght, as sometimes we have in df_daily the nights variables, while never\n",
    "#in df_SNA_daily\n",
    "df = pd.merge(df_daily, df_SNA_daily, on=['HenID','level'], how='outer')\n",
    "print(df_daily.shape, df_SNA_daily.shape, df.shape)\n",
    "#check if we have some days with daily var in df_daily, but with no SNA\n",
    "if df[(df['closeness_DTW_30_1_Zone'].isnull())&(~df['verification_daily_total_duration'].isnull())].shape[0]>0:\n",
    "    print('you have not the same amount of SNA and DAILY observations (HenID, level)')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns with list or that was here simply to compute/verify other variables, as wew ill now not use these variables \n",
    "df = df.drop(['dur_values','dur_values_normalized','previous_dur_values_normalized', 'FirstTimestamp_1_Zone', \n",
    "              'FirstTimestamp_2_Zone', 'FirstTimestamp_3_Zone', 'FirstTimestamp_4_Zone', 'FirstTimestamp_5_Zone',\n",
    "              'list_timestamps', 'list_timestamps_seondsOfTheDay','list_of_night20_2_temperature',\n",
    "              'list_of_MovementCounter_day','list_of_durations','list_of_zones','t_DU_missingZone_mvtPerc', \n",
    "              'li_event_chaoticmvt_z_d'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "df.to_csv(os.path.join(path_extracted_data,'daily_ALL_Variable_Tranformed_HA_SNA.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df_SNA_daily.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HA_HenID.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HA_HenID['Diarrhea'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_HAvar = ['CombPecks', 'RFPododermatitis', 'RFBumblefoot', 'RFinjuries',\n",
    "            'LFPododermatitis', 'LFBumblefoot', 'LFinjuries','NeckFeather',\n",
    "            'BreastFeather', 'CloacaFeather', 'BackFeather', 'WingFeather','TailFeather','weight'] #Diarrhea\n",
    "df_HA_HenID[['HenID','date']+li_HAvar].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HA_HenID['Toes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HA_HenID['Claw'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for v in li_HAvar:\n",
    "    ax = sns.boxplot(x=\"WOA\", y=v, data=df_HA_HenID);\n",
    "    plt.title(v.upper())\n",
    "    plt.savefig(os.path.join(path_extracted_HA_visual,v+'.png'),dpi=300,format='png',bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
