{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic package\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "#0-1 normalisation\n",
    "from sklearn import preprocessing\n",
    "#modelling\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#PCA\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#plot\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "from UTILS import cleaning_processing, HenDailyVariable_Origins, plot_scikit_lda, explained_var, plot_cov_ellipse\n",
    "import config_origins as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#initialise parameters\n",
    "path_extracted_data = config.path_extracted_data\n",
    "path_initial_data = config.path_initial_data\n",
    "id_run = config.id_run\n",
    "pal_ = config.pal_\n",
    "dico_matching = config.dico_matching\n",
    "dico_zone_order = config.dico_zone_order\n",
    "dico_night_hour = config.dico_night_hour\n",
    "dico_garden_opening_hour = config.dico_garden_opening_hour\n",
    "path_extracted_data_visual = os.path.join(path_extracted_data,'visual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#for being more reproductible, we open the file that was saved from cleaning\n",
    "df_var = pd.read_csv(os.path.join(path_extracted_data, id_run+'_daily_CLEANING_ALL_variables.csv'), sep=';', \n",
    "                 parse_dates=['level'], dayfirst=True) \n",
    "print(df_var.shape)\n",
    "df_var.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'G:\\\\VPHI\\\\Welfare\\\\2- Research Projects\\\\OFHE2.OriginsE2\\\\DataOutput\\\\TrackingSystem\\\\chapter0_final_\\\\daily_ALL_Variable_Tranformed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5308489bcaa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#for being more reproductible, we open the file that was saved from cleaning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m df_var = pd.read_csv(os.path.join(path_extracted_data, 'daily_ALL_Variable_Tranformed.csv'), sep=';', \n\u001b[0m\u001b[0;32m      3\u001b[0m                  parse_dates=['level'], dayfirst=True) \n\u001b[0;32m      4\u001b[0m \u001b[1;31m#df_var['duration_5_Zone_h'] = df_var['duration_5_Zone'].map(lambda x: x/60/60)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'G:\\\\VPHI\\\\Welfare\\\\2- Research Projects\\\\OFHE2.OriginsE2\\\\DataOutput\\\\TrackingSystem\\\\chapter0_final_\\\\daily_ALL_Variable_Tranformed.csv'"
     ]
    }
   ],
   "source": [
    "#for being more reproductible, we open the file that was saved from cleaning\n",
    "df_var = pd.read_csv(os.path.join(path_extracted_data, 'daily_ALL_Variable_Tranformed.csv'), sep=';', \n",
    "                 parse_dates=['level'], dayfirst=True) \n",
    "#df_var['duration_5_Zone_h'] = df_var['duration_5_Zone'].map(lambda x: x/60/60)\n",
    "print(df_var.shape)\n",
    "df_var.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep hens that have more than \"min_nbr_days\" days\n",
    "min_nbr_days = 80\n",
    "li_lda = ['duration_2_Zone','duration_3_Zone','duration_5_Zone','median_duration','variance_duration','distribution_entropy',\n",
    "          'sucessIntrusion_9','Max_duration_WG', 'in_WG_15mnAfterOpening',\n",
    "          'night_Total_number_transition','vertical_travel_distance',\n",
    "          'nbr_stays_2_Zone','nbr_stays_3_Zone','nbr_stays_5_Zone','nbr_stays_1_Zone']\n",
    "li_lda = ['duration_5_Zone','nbr_stays_1_Zone','nbr_stays_4_Zone','distribution_entropy',\n",
    "          'sucessIntrusion_9','in_WG_15mnAfterOpening','night_Total_number_transition','vertical_travel_distance']\n",
    "#'nbr_stays_4_Zone': is very important!\n",
    "df = df_var[li_lda+['HenID','level']].copy()\n",
    "print(df.shape)\n",
    "df = df[~df.isnull().any(axis=1)]\n",
    "print(df.shape)\n",
    "#select hens\n",
    "df_ = df.groupby(['HenID'])['in_WG_15mnAfterOpening'].count().reset_index()\n",
    "df_['has_more_'+str(min_nbr_days)+'days'] = df_['in_WG_15mnAfterOpening'].map(lambda x: x>min_nbr_days)\n",
    "display(df_.head(3))\n",
    "li_hen2keep = df_[df_['has_more_'+str(min_nbr_days)+'days']]['HenID'].unique()\n",
    "df['in_WG_15mnAfterOpening'] = df['in_WG_15mnAfterOpening'].map(lambda x: int(x))\n",
    "df = df[df['HenID'].isin(li_hen2keep)].copy()\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize\n",
    "#scaling numerical data: necessary due to duration high values\n",
    "sc = StandardScaler()\n",
    "df[li_lda] = sc.fit_transform(df[li_lda])\n",
    "df[li_lda].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from: https://plotly.com/python/pca-visualization/\n",
    "X = df[li_lda]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(X)\n",
    "\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "fig = px.scatter(components, x=0, y=1, color=df['HenID'])\n",
    "\n",
    "for i, feature in enumerate(li_lda):\n",
    "    fig.add_shape(\n",
    "        type='line',\n",
    "        x0=0, y0=0,\n",
    "        x1=loadings[i, 0],\n",
    "        y1=loadings[i, 1]\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        x=loadings[i, 0],\n",
    "        y=loadings[i, 1],\n",
    "        ax=0, ay=0,\n",
    "        xanchor=\"center\",\n",
    "        yanchor=\"bottom\",\n",
    "        text=feature,\n",
    "    )\n",
    "plt.savefig(os.path.join(path_extracted_data_visual, 'PCA.png'), dpi=300, format='png', \n",
    "            bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA (linear discrimnant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df[li_lda].values\n",
    "y = df['HenID'].values\n",
    "print(X.shape, y.shape)\n",
    "df[li_lda][df[li_lda].isnull().sum(axis=1)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\n",
    "lda.fit(X, y)\n",
    "#extract the labels\n",
    "li_labels = lda.classes_\n",
    "print(li_labels[0:10])\n",
    "#defined nbr components to analyse\n",
    "nbr_lda_components = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Hens into the LDA 2Dspace\n",
    "X_lda_sklearn = lda.transform(X)\n",
    "#plot with the first two components\n",
    "plot_scikit_lda(X_lda_sklearn, y, lda, 'TEST', path_extracted_data_cleaning_rawvscleaned_LDA)\n",
    "\n",
    "#save it\n",
    "'''dico_h_dinfo = {}\n",
    "for h in set(y):\n",
    "    for day_ in range(sum(y==h)):\n",
    "        name_ = h+'_day_'+str(day_)\n",
    "        if name_ not in dico_h_dinfo:\n",
    "            dico_h_dinfo[name_] = {'HenID':h,'day':dico_hen_dates[h][day_]}\n",
    "        for i in range(nbr_lda_components):\n",
    "            dico_h_dinfo[name_]['ID'+str(i)] = list(X_lda_sklearn[:,i][y==h])[day_]\n",
    "df_IDS = pd.DataFrame(list(dico_h_dinfo.values()))\n",
    "#df_IDS['max_severity'] = df_IDS['HenID'].map(lambda x: dico_h_ms[x])\n",
    "print(df_IDS.shape, df.shape)\n",
    "df_IDS = pd.merge(df_IDS, df.filter(li_output+['HenID','day']),on=['HenID','day'],how='inner')\n",
    "#df_IDS.to_csv(os.path.join(path_extracted_data_cleaning_rawvscleaned_LDA,'df_IDS_onall.csv'), index=False, sep=';')\n",
    "print(df_IDS.shape)\n",
    "display(df_IDS.head(3))'''\n",
    "\n",
    "#LDA comp explained variance\n",
    "explained_var(lda, nbr_lda_components, path_extracted_data_cleaning_rawvscleaned_LDA)\n",
    "\n",
    "#LDA component & Expl. var\n",
    "#small info on input/ouput shape\n",
    "print('We have %d hens, %d observations (~%d per class), %d expl. variables '%(len(set(y)), \n",
    "                                                                               X.shape[0], X.shape[0]/len(set(y)),\n",
    "                                                                               X.shape[1]))\n",
    "#for each hen we have the coefficient/mean linked to each expl. variables\n",
    "print(lda.coef_.shape, lda.means_.shape)\n",
    "#for each expl. variables we have a row/column in the cov matrix\n",
    "print(lda.covariance_.shape)\n",
    "#for each of the n_components we have the explained_variance_ratio (by default its the min(n_classes - 1, n_features))\n",
    "print(lda.explained_variance_ratio_.shape)\n",
    "\n",
    "#for each of the lda components its link to the associated variables (50,54). Lets do a heatmap of the coefficient strength\n",
    "plt.figure(figsize=(25,12))\n",
    "M = lda.coef_\n",
    "v = max(abs(M.min()),M.max())\n",
    "sns.heatmap(M, xticklabels=li_lda, yticklabels=['comp '+str(i) for i in range(lda.coef_.shape[0])],\n",
    "            vmin=-v, vmax=v, cmap=\"RdBu\")\n",
    "plt.title('heatmap of coefficient LDA')\n",
    "#plt.savefig(os.path.join(path_extracted_data_cleaning_rawvscleaned_LDA, 'heatmap_coefficient_LDA_all_comp.png'),dpi=300,format='png',bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "#for each of the lda components its link to the associated variables (50,54). Lets do a heatmap of the coefficient strength\n",
    "plt.figure(figsize=(20,8))\n",
    "M = lda.coef_\n",
    "M = M[0:nbr_lda_components,:]\n",
    "print(M.shape)\n",
    "v = max(abs(M.min()),M.max())\n",
    "sns.heatmap(M, xticklabels=li_lda, yticklabels=['comp '+str(i) for i in range(M.shape[0])], vmin=-v, vmax=v, cmap=\"RdBu\")\n",
    "plt.title('heatmap of coefficient LDA')\n",
    "plt.savefig(os.path.join(path_extracted_data_visual, 'heatmap_coefficient_LDA_first_comp.png'), dpi=300, format='png', \n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMDS, isomap, pca, pcoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://jkzorz.github.io/2020/04/04/NMDS-extras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://strata.uga.edu/8370/lecturenotes/multidimensionalScaling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isomap"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from: https://jakevdp.github.io/PythonDataScienceHandbook/05.10-manifold-learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Isomap(n_components=2)\n",
    "proj = model.fit_transform(df[li_lda])\n",
    "plt.plot(proj[:, 0], proj[:, 1], 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#incase\n",
    "\n",
    "#for being more reproductible, we open the file that was saved from cleaning\n",
    "df_mlp = pd.read_csv(os.path.join(path_extracted_data, id_run+'_CLEANEDDATA.csv'), sep=';', parse_dates=['Timestamp'], \n",
    "                     dayfirst=True) \n",
    "df_mlp['date'] = df_mlp['Timestamp'].map(lambda x: dt.datetime(x.year,x.month,x.day))\n",
    "print(df_mlp.shape)\n",
    "df_mlp.head(3)\n",
    "\n",
    "def image_mlp(df,Daterange, name):\n",
    "    df_plt = df.copy()\n",
    "    fig = plt.figure(1, figsize=(8, 6))\n",
    "    df_plt.set_index('Timestamp', inplace=True)\n",
    "    df_plt = df_plt.reindex(Daterange, method='ffill').reset_index()\n",
    "    df_plt.rename(columns={'index':'Timestamp'}, inplace=True)\n",
    "    df_plt['date'] = df_plt['Timestamp'].map(lambda x: dt.datetime.date(x))\n",
    "    #take only the actual day\n",
    "    df_plt = df_plt[df_plt['date']==day]\n",
    "    df_plt['Zone'] = df_plt['Zone'].map(lambda x: int(dico_zone_order[x]))       \n",
    "    plt.plot(df_plt['Timestamp'].tolist(), df_plt['Zone'].tolist(), color=pal_['Trackingsystem_Zone'], linewidth=1)\n",
    "    plt.ylim(0, 4.2)     \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig(os.path.join(path_extracted_data_visual,'test'+name+'.png'),format='png',bbox_inches='tight')\n",
    "    img = cv2.imread(os.path.join(path_extracted_data_visual,'test'+name+'.png'))\n",
    "    #img = cv2.resize(image, (10, 20)) \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
