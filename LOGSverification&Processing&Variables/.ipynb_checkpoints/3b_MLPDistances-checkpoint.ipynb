{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic package\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import operator\n",
    "from dtaidistance import dtw\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle #to save/load list of selected hens\n",
    "\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#network\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "from UTILS import chi2_distance, correct_key\n",
    "import config_origins as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change the configuration file if not done yet!\n",
      "correctlightschedule_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print('change the configuration file if not done yet!')\n",
    "path_extracted_data = config.path_extracted_data\n",
    "path_initial_data = config.path_initial_data\n",
    "id_run = config.id_run\n",
    "#id_run = 'chapter0_final_'\n",
    "#path_dataoutput = r'G:\\VPHI\\Welfare\\2- Research Projects\\OFHE2.OriginsE2\\DataOutput'\n",
    "#path_extracted_data = os.path.join(path_dataoutput,'TrackingSystem') \n",
    "#path_extracted_data = os.path.join(path_extracted_data, id_run)\n",
    "dico_night_hour = config.dico_night_hour\n",
    "dico_matching = config.dico_matching\n",
    "li_binmn = config.li_binmn\n",
    "penalty = config.penalty\n",
    "dico_window = config.dico_window\n",
    "path_extracted_data_SNA = config.path_extracted_data_SNA\n",
    "#create a director if not existing\n",
    "if not os.path.exists(path_extracted_data_SNA):\n",
    "    os.makedirs(path_extracted_data_SNA)\n",
    "print(id_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#focal birds info (one row per bird)\n",
    "df_FB = pd.read_csv(os.path.join(path_extracted_data,id_run+'df_FOCALBIRDS.csv'), sep=';', parse_dates=['InitialStartDate'],\n",
    "                     dayfirst=True) \n",
    "df_FB['percentage_of_gain_weight'] = df_FB.apply(lambda x: (x['weight 23-11-2020']-x['29-09 weight'])/x['29-09 weight']*100, axis=1)\n",
    "print(df_FB.shape)\n",
    "df_FB.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cleaned data of the tracking system movements\n",
    "df = pd.read_csv(os.path.join(path_extracted_data, id_run+'_CLEANEDDATA.csv'), sep=';', \n",
    "                 parse_dates=['Timestamp', 'date'], dayfirst=True) \n",
    "df['hour'] = df['Timestamp'].map(lambda x: x.hour)\n",
    "df['time'] = df['Timestamp'].map(lambda x: dt.datetime.time(x-dt.timedelta(seconds=x.second)))\n",
    "df.drop('duration', axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-be5543e42d39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#daily variables (one row per (henID, date))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#daily_ALL_Variable_Tranformed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m df_daily = pd.read_csv(os.path.join(path_extracted_data, id_run+'_daily_ALL_variables.csv'), sep=';',\n\u001b[0m\u001b[0;32m      4\u001b[0m                      parse_dates=['level','FirstTimestamp_1_Zone', 'FirstTimestamp_2_Zone', 'FirstTimestamp_3_Zone',\n\u001b[0;32m      5\u001b[0m                               \u001b[1;34m'FirstTimestamp_4_Zone'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'FirstTimestamp_5_Zone'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2145\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2146\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2147\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \"\"\"\n\u001b[0;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#daily variables (one row per (henID, date))\n",
    "#daily_ALL_Variable_Tranformed\n",
    "df_daily = pd.read_csv(os.path.join(path_extracted_data, id_run+'_daily_ALL_variables.csv'), sep=';',\n",
    "                     parse_dates=['level','FirstTimestamp_1_Zone', 'FirstTimestamp_2_Zone', 'FirstTimestamp_3_Zone',\n",
    "                              'FirstTimestamp_4_Zone', 'FirstTimestamp_5_Zone', \n",
    "                              'Nestbox_time_of_first_staid_longer_than900sec',\n",
    "                              'duration_last-firsttransition_mn'], dayfirst=True) \n",
    "df_daily['DOA'] = df_daily['level'].map(lambda x: (x-dt.datetime(2020,6,3)).days) \n",
    "df_daily['weeks_in_laying_barn'] = df_daily['DOA'].map(lambda x: int((x-119)/7)+1)\n",
    "#df_daily['weeks_in_laying_barn'] = df_daily['DOA'].map(lambda x: math.ceil(x/7))\n",
    "#first two month seems good from the %of hens not moving plot! and sounds good too (twice longer thatn they need to start moving)\n",
    "print(df_daily.shape)\n",
    "\n",
    "#filter by date\n",
    "df_daily = df_daily[df_daily['level']>dt.datetime(2020,9,29)]\n",
    "\n",
    "#remove days that are not fully recorded\n",
    "df_daily['nbr_sec_per_day'] = df_daily['level'].map(lambda x: dico_night_hour[correct_key(x,dico_night_hour)]['nbr_hour']*60*60)\n",
    "df_daily['is_correct_amount_time'] = df_daily.apply(lambda x: x['nbr_sec_per_day']==x['verification_daily_total_duration'], axis=1)\n",
    "df_daily[(~df_daily['is_correct_amount_time'])&(~df_daily['Total_number_transition'].isnull())][['level', 'HenID', 'Total_number_transition', 'dur_values', 'verification_daily_total_duration','nbr_sec_per_day']]\n",
    "print(df_daily.shape)\n",
    "display(df_daily = df_daily[~((~df_daily['is_correct_amount_time'])&(~df_daily['Total_number_transition'].isnull()))])\n",
    "print(df_daily.shape)\n",
    "\n",
    "#remove the days where the night had movement recorded but not the days\n",
    "print(df_daily.shape)\n",
    "#display(df_daily[df_daily.isna().any(axis=1)])\n",
    "df_daily = df_daily[~df_daily['verification_daily_total_duration'].isnull()]\n",
    "print(df_daily.shape)\n",
    "\n",
    "df_daily['dur_values_normalized'].replace('[nan, nan, nan, nan, nan]','[np.nan,np.nan,np.nan,np.nan,np.nan]', inplace=True)\n",
    "df_daily['dur_values_normalized'] = df_daily['dur_values_normalized'].map(lambda x: eval(x))\n",
    "df_daily['duration_last-firsttransition_mn'] = df_daily['duration_last-firsttransition_mn'].astype(float)\n",
    "print(df_daily.shape)\n",
    "df_daily.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance across pairs of MLPS from same day - for daily SNA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lets compute every day a network per zone with nodes=hens, link=DTW_zone_X. For this we need to compute mvt distances between hens on same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the MLPs vectors during day only! as we compare on same day, we can restrict to the exact daily calendar!\n",
    "dico_pen_bin_zone_level_h = pickle.load(open(os.path.join(path_extracted_data, \n",
    "                                                 id_run+'dico_pen_bin_zone_level_h_DAILYLEVEL.pkl'), 'rb'))\n",
    "dico_pen_bin_level_h = pickle.load(open(os.path.join(path_extracted_data, \n",
    "                                                     id_run+'dico_pen_bin_level_h_DAILYLEVEL.pkl'), 'rb'))\n",
    "#dico of hen as keys and pen as values\n",
    "df_daily['PenID'] = df_daily['PenID'].map(lambda x: 'pen'+(str(int(x))))\n",
    "dico_h_p = dict(zip(df_daily['HenID'].tolist(), df_daily['PenID'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_binmn = [30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the CHI2DISTANCE & DTW for each bird with all other birds\n",
    "#start recording the time it last\n",
    "START_TIME = time.perf_counter()\n",
    "li_df = []\n",
    "#df.sort_values(['Timestamp'], inplace=True)\n",
    "#take (subset of) dates and hens that are available (i.e. for which we comptue the variable on)\n",
    "#df_daily_h = df_daily[(df_daily['DOA']>=150)&(df_daily['DOA']<=380)].copy()\n",
    "li_date = set(df_daily['level'].tolist())\n",
    "li_zone = ['1_Zone', '2_Zone', '3_Zone', '4_Zone', '5_Zone']\n",
    "for d in tqdm.tqdm(sorted(list(li_date))):\n",
    "    li_hen_d = df_daily[df_daily['level']==d]['HenID'].unique()\n",
    "    #we use symmetric measures, so we only do each combination once\n",
    "    for i,h1 in enumerate(li_hen_d):\n",
    "\n",
    "        #lets compute for hen h1 its difference with all other birds\n",
    "        for h2 in li_hen_d[i+1:]:\n",
    "            \n",
    "            try:\n",
    "                ############ Overall aviary usage similarity across days\n",
    "                l1_chi2 = df_daily[(df_daily['HenID']==h1)&(df_daily['level']==d)]['dur_values_normalized'].values[0]\n",
    "                l2_chi2 = df_daily[(df_daily['HenID']==h2)&(df_daily['level']==d)]['dur_values_normalized'].values[0]\n",
    "                dico_ = {'date':d, 'henId':h1, 'henID2':h2, 'chi2distance':chi2_distance(l1_chi2,l2_chi2, remove_warning=True)}\n",
    "                #'dur_values_normalized_henId':l1_chi2,'dur_values_normalized_henID2':l2_chi2,\n",
    "\n",
    "                ############# Specifics' zones usage similarity across days\n",
    "                for nbr_binmn in li_binmn:\n",
    "                    #ALL zone together with chi2distance\n",
    "                    l1_chi2 = dico_pen_bin_level_h[dico_h_p[h1]][nbr_binmn][d][h1]\n",
    "                    l2_chi2 = dico_pen_bin_level_h[dico_h_p[h2]][nbr_binmn][d][h2]\n",
    "                    dico_['nbr_obs_chi2all_'+str(nbr_binmn)] = len(l1_chi2)\n",
    "                    li_chi2 = [chi2_distance(l1_chi2[i],l2_chi2[i], remove_warning=True) for i in range(0,len(l1_chi2))]\n",
    "                    dico_['li_chi2_'+str(nbr_binmn)] = li_chi2\n",
    "                    dico_['chi2distance_ALL_'+str(nbr_binmn)] = np.mean(li_chi2)\n",
    "\n",
    "                    #per zone with DTW\n",
    "                    for ZONE in li_zone:\n",
    "                        dtw_value = np.nan\n",
    "                        try:\n",
    "                            l1_dtw = np.array(dico_pen_bin_zone_level_h[dico_h_p[h1]][nbr_binmn][ZONE][d][h1], dtype=np.double)\n",
    "                            l2_dtw = np.array(dico_pen_bin_zone_level_h[dico_h_p[h2]][nbr_binmn][ZONE][d][h2], dtype=np.double)\n",
    "                            #compute distance measure\n",
    "                            #psi: #of start and end points of a sequence can be ignored if this would lead to a lower distance\n",
    "                            dtw_value, paths = dtw.warping_paths(l1_dtw, l2_dtw, window=dico_window[nbr_binmn], psi=0, penalty=penalty)\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            sys.exit()\n",
    "                            pass #dont do anything\n",
    "                        dico_['DTW_'+str(nbr_binmn)+'_'+ZONE] = round(dtw_value,1)  \n",
    "                        dico_['nbr_obs_l1_'+str(nbr_binmn)+'_'+ZONE] = len(l1_dtw)\n",
    "                        dico_['nbr_obs_l2_'+str(nbr_binmn)+'_'+ZONE] = len(l2_dtw)\n",
    "                li_df.append(dico_)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(h1, h2)\n",
    "                print(d)\n",
    "                \n",
    "END_TIME = time.perf_counter()\n",
    "print (\"Total running time: %.2f mn\" %((END_TIME-START_TIME)/60))\n",
    "#turn inter and intra distances into a dataframe                \n",
    "df_inter = pd.DataFrame(li_df)\n",
    "#df_inter = df_inter[~df_inter['chi2distance'].isnull()]\n",
    "df_inter['hen_pair'] = df_inter.apply(lambda x: '-'.join(sorted([x['henId'],x['henID2']])), axis=1)\n",
    "df_inter['DTW_30_all'] = df_inter[['DTW_30_1_Zone', 'DTW_30_2_Zone', 'DTW_30_3_Zone','DTW_30_4_Zone','DTW_30_5_Zone']].sum(axis=1)\n",
    "li_col = list(df_inter.columns)\n",
    "li_remove = [i for i in li_col if i.startswith('li_chi2_')]\n",
    "print(li_remove)\n",
    "li_keep = [i for i in li_col if i not in li_remove]\n",
    "#df_inter.iloc[0:1000].to_csv(os.path.join(path_extracted_data_visual_adap, id_run+'_df_DistanceBetweenHenSim_4verification.csv'), sep=';', index=False)\n",
    "#df_inter.filter(li_keep).to_csv(os.path.join(path_extracted_data_visual_adap ,id_run+'_df_DistanceBetweenHenSim.csv'), sep=';', index=False)\n",
    "df_inter.to_csv(os.path.join(path_extracted_data ,id_run+'_df_DistanceBetweenHenSim_DAILYLEVEL.csv'), sep=';', index=False)\n",
    "print(df_inter.shape)\n",
    "display(df_inter.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_inter.shape)\n",
    "display(df_inter.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance across any pairs of MLPS - for daily clusterID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download MLPs vectors\n",
    "#For now we restrict to 2h-17h in order to always compare same length TS as we will compare across days, the lenght might differ\n",
    "dico_pen_bin_level_h = pickle.load(open(os.path.join(path_extracted_data_visual_adap, \n",
    "                                                     id_run+'dico_pen_bin_level_h_2h-17h59LEVEL.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute mvt distances between any two MLPs on any day from any pen\n",
    "dico_hendate_mvt = {}\n",
    "for p,dico_bin_level_h in dico_pen_bin_level_h.items():\n",
    "    for level, dico_hen_mvt in dico_bin_level_h[15].items():\n",
    "        if level!=dt.datetime(2020,9,29):\n",
    "            for henID,limvt in dico_hen_mvt.items():\n",
    "                dico_hendate_mvt[henID+'/'+str(level).split(' ')[0]] = limvt\n",
    "dico_hendate_mvt = OrderedDict(dico_hendate_mvt)\n",
    "li_hendate = list(dico_hendate_mvt.keys())\n",
    "li_mvt = list(dico_hendate_mvt.values())\n",
    "len(dico_hendate_mvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_df = []\n",
    "for i in tqdm.tqdm(range(0,len(li_mvt))):\n",
    "    li_mvt1 = li_mvt[i]\n",
    "    for j in range(i+1,len(li_mvt)):\n",
    "        dico_ = {}\n",
    "        li_mvt2 = li_mvt[j]\n",
    "        li_chi2 = [chi2_distance(li_mvt1[i],li_mvt2[i]) for i in range(0,len(li_mvt1))]\n",
    "        dico_['chi2distance_ALL_15'] = np.mean(li_chi2)\n",
    "        dico_['h1'] = li_hendate[i]\n",
    "        dico_['h2'] = li_hendate[j]\n",
    "        li_df.append(dico_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anypair = pd.DataFrame(li_df)\n",
    "display(df_anypair[df_anypair['chi2distance_ALL_15'].isnull()])\n",
    "df_anypair['henID1'] = df_anypair['h1'].map(lambda x: x.split('/')[0])\n",
    "df_anypair['henID2'] = df_anypair['h2'].map(lambda x: x.split('/')[0])\n",
    "df_anypair['level1'] = df_anypair['h1'].map(lambda x: dt.datetime.strptime(x.split('/')[1], '%Y-%m-%d'))\n",
    "df_anypair['level2'] = df_anypair['h2'].map(lambda x: dt.datetime.strptime(x.split('/')[1], '%Y-%m-%d'))\n",
    "#df_interintra['weeks_in_laying_barn'] = df_interintra['DOA'].map(lambda x: int((x-119)/7)+1)\n",
    "#df_interintra['hen_pair'] = df_interintra.apply(lambda x: '-'.join(sorted([x['henID1'],x['henID2']])), axis=1)\n",
    "df_anypair.to_csv(os.path.join(path_extracted_data_visual_adap ,id_run+'_df_DistanceBetweenAnyDaysHenSim_2h-17h59LEVEL.csv'), sep=';', index=False)\n",
    "print(df_anypair.shape)\n",
    "display(df_anypair.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_anypair['chi2distance_ALL_15']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
