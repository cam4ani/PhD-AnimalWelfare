{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic package\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import operator\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cdist #for euclidean distance of consistency\n",
    "from numpy import inf\n",
    "import networkx as nx\n",
    "from dtaidistance import dtw\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "import uuid #to generate random id\n",
    "import pickle #to save/load list of selected hens\n",
    "\n",
    "#test equal variance\n",
    "from scipy.stats import levene\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import AgglomerativeClustering \n",
    "\n",
    "#scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "#interpolate curves for clustering among birds with not exact same days tracked\n",
    "#from scipy.interpolate import interp1d\n",
    "\n",
    "#modelling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, cohen_kappa_score, r2_score,\\\n",
    "mean_squared_error, mean_absolute_error, explained_variance_score#catboost, for a better support of categorical data\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from scipy.stats import pearsonr, spearmanr \n",
    "import scipy.stats as stats\n",
    "\n",
    "#PCA\n",
    "from sklearn import decomposition\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import KMeans #only numerical var\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import kmodes\n",
    "from kmodes.kmodes import KModes #with categorical var as well\n",
    "\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dexplot as dxp #for barplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#network\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "from UTILS import chi2_distance, ts_visual, time_series_henColumn_tsRow, correct_key\n",
    "import config_origins as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change the configuration file if not done yet!\n",
      "correctlightschedule_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print('change the configuration file if not done yet!')\n",
    "path_extracted_data = config.path_extracted_data\n",
    "path_initial_data = config.path_initial_data\n",
    "id_run = config.id_run\n",
    "#id_run = 'chapter0_final_'\n",
    "#path_dataoutput = r'G:\\VPHI\\Welfare\\2- Research Projects\\OFHE2.OriginsE2\\DataOutput'\n",
    "#path_extracted_data = os.path.join(path_dataoutput,'TrackingSystem') \n",
    "#path_extracted_data = os.path.join(path_extracted_data, id_run)\n",
    "dico_night_hour = config.dico_night_hour\n",
    "dico_matching = config.dico_matching\n",
    "li_binmn = config.li_binmn\n",
    "penalty = config.penalty\n",
    "dico_window = config.dico_window\n",
    "path_extracted_data_SNA = config.path_extracted_data_SNA\n",
    "#create a director if not existing\n",
    "if not os.path.exists(path_extracted_data_SNA):\n",
    "    os.makedirs(path_extracted_data_SNA)\n",
    "print(id_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HenID</th>\n",
       "      <th>PenID</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>R-Pen</th>\n",
       "      <th>InitialStartDate</th>\n",
       "      <th>29-09 weight</th>\n",
       "      <th>10-12 juin weight</th>\n",
       "      <th>weight 23-11-2020</th>\n",
       "      <th>weight 04-01-2021</th>\n",
       "      <th>weight 01-02-21</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>percentage_of_gain_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>9</td>\n",
       "      <td>EPI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1696.5</td>\n",
       "      <td>1787.8</td>\n",
       "      <td>1800.9</td>\n",
       "      <td>OFH</td>\n",
       "      <td>49.603175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hen_10</td>\n",
       "      <td>11</td>\n",
       "      <td>LEXP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>70.4</td>\n",
       "      <td>1488.3</td>\n",
       "      <td>1628.4</td>\n",
       "      <td>1602.1</td>\n",
       "      <td>OFH</td>\n",
       "      <td>39.093458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hen_100</td>\n",
       "      <td>5</td>\n",
       "      <td>EPI</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>1182.0</td>\n",
       "      <td>80.5</td>\n",
       "      <td>1642.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OFH</td>\n",
       "      <td>38.959391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     HenID  PenID CLASS  R-Pen InitialStartDate  29-09 weight  \\\n",
       "0    hen_1      9   EPI    1.0       2020-06-10        1134.0   \n",
       "1   hen_10     11  LEXP    1.0       2020-06-10        1070.0   \n",
       "2  hen_100      5   EPI    3.0       2020-06-10        1182.0   \n",
       "\n",
       "   10-12 juin weight  weight 23-11-2020  weight 04-01-2021  weight 01-02-21  \\\n",
       "0               57.0             1696.5             1787.8           1800.9   \n",
       "1               70.4             1488.3             1628.4           1602.1   \n",
       "2               80.5             1642.5                NaN              NaN   \n",
       "\n",
       "  Treatment  percentage_of_gain_weight  \n",
       "0       OFH                  49.603175  \n",
       "1       OFH                  39.093458  \n",
       "2       OFH                  38.959391  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#focal birds info (one row per bird)\n",
    "df_FB = pd.read_csv(os.path.join(path_extracted_data,id_run+'df_FOCALBIRDS.csv'), sep=';', parse_dates=['InitialStartDate'],\n",
    "                     dayfirst=True) \n",
    "df_FB['percentage_of_gain_weight'] = df_FB.apply(lambda x: (x['weight 23-11-2020']-x['29-09 weight'])/x['29-09 weight']*100, axis=1)\n",
    "print(df_FB.shape)\n",
    "df_FB.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_date_adaptability' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-bf3c7e15119d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m df = pd.read_csv(os.path.join(path_extracted_data, id_run+'_CLEANEDDATA.csv'), sep=';', \n\u001b[0;32m      3\u001b[0m                  parse_dates=['Timestamp', 'date'], dayfirst=True) \n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mmax_date_adaptability\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#add two days to have the next observations as well when doing the 1sec ts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hour'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msecond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_date_adaptability' is not defined"
     ]
    }
   ],
   "source": [
    "#cleaned data of the tracking system movements\n",
    "df = pd.read_csv(os.path.join(path_extracted_data, id_run+'_CLEANEDDATA.csv'), sep=';', \n",
    "                 parse_dates=['Timestamp', 'date'], dayfirst=True) \n",
    "df['hour'] = df['Timestamp'].map(lambda x: x.hour)\n",
    "df['time'] = df['Timestamp'].map(lambda x: dt.datetime.time(x-dt.timedelta(seconds=x.second)))\n",
    "df = df[df['HenID'].isin(li_selected_hens)] \n",
    "df.drop('duration', axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#daily variables (one row per (henID, date))\n",
    "df_daily = pd.read_csv(os.path.join(path_extracted_data, 'daily_ALL_Variable_Tranformed.csv'), sep=';',\n",
    "                     parse_dates=['level','FirstTimestamp_1_Zone', 'FirstTimestamp_2_Zone', 'FirstTimestamp_3_Zone',\n",
    "                              'FirstTimestamp_4_Zone', 'FirstTimestamp_5_Zone', \n",
    "                              'Nestbox_time_of_first_staid_longer_than900sec',\n",
    "                              'duration_last-firsttransition_mn'], dayfirst=True) \n",
    "df_daily['DOA'] = df_daily['level'].map(lambda x: (x-dt.datetime(2020,6,3)).days) \n",
    "df_daily['weeks_in_laying_barn'] = df_daily['DOA'].map(lambda x: int((x-119)/7)+1)\n",
    "#df_daily['weeks_in_laying_barn'] = df_daily['DOA'].map(lambda x: math.ceil(x/7))\n",
    "#first two month seems good from the %of hens not moving plot! and sounds good too (twice longer thatn they need to start moving)\n",
    "print(df_daily.shape)\n",
    "\n",
    "#filter by dates\n",
    "df_daily = df_daily[df_daily['level']<=max_date_adaptability] \n",
    "df_daily = df_daily[df_daily['level']>dt.datetime(2020,9,29)]\n",
    "\n",
    "#filter the selected hens\n",
    "df_daily = df_daily[df_daily['HenID'].isin(li_selected_hens)] \n",
    "\n",
    "#remove days that are not fully recorded\n",
    "df_daily['nbr_sec_per_day'] = df_daily['level'].map(lambda x: dico_night_hour[correct_key(x,dico_night_hour)]['nbr_hour']*60*60)\n",
    "df_daily['is_correct_amount_time'] = df_daily.apply(lambda x: x['nbr_sec_per_day']==x['verification_daily_total_duration'], axis=1)\n",
    "df_daily[(~df_daily['is_correct_amount_time'])&(~df_daily['Total_number_transition'].isnull())][['level', 'HenID', 'Total_number_transition', 'dur_values', 'verification_daily_total_duration','nbr_sec_per_day']]\n",
    "print(df_daily.shape)\n",
    "display(df_daily = df_daily[~((~df_daily['is_correct_amount_time'])&(~df_daily['Total_number_transition'].isnull()))])\n",
    "print(df_daily.shape)\n",
    "\n",
    "#remove the days where the night had movement recorded but not the days\n",
    "print(df_daily.shape)\n",
    "#display(df_daily[df_daily.isna().any(axis=1)])\n",
    "df_daily = df_daily[~df_daily['verification_daily_total_duration'].isnull()]\n",
    "print(df_daily.shape)\n",
    "\n",
    "df_daily['dur_values_normalized'].replace('[nan, nan, nan, nan, nan]','[np.nan,np.nan,np.nan,np.nan,np.nan]', inplace=True)\n",
    "df_daily['dur_values_normalized'] = df_daily['dur_values_normalized'].map(lambda x: eval(x))\n",
    "df_daily['duration_last-firsttransition_mn'] = df_daily['duration_last-firsttransition_mn'].astype(float)\n",
    "print(df_daily.shape)\n",
    "df_daily.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the MLPs vectors during day only! as we compare on same day, we can restrict to the exact daily calendar!\n",
    "dico_pen_bin_zone_level_h = pickle.load(open(os.path.join(path_extracted_data, \n",
    "                                                 id_run+'dico_pen_bin_zone_level_h_DAILYLEVEL.pkl'), 'rb'))\n",
    "dico_pen_bin_level_h = pickle.load(open(os.path.join(path_extracted_data, \n",
    "                                                     id_run+'dico_pen_bin_level_h_DAILYLEVEL.pkl'), 'rb'))\n",
    "#dico of hen as keys and pen as values\n",
    "df_daily['PenID'] = df_daily['PenID'].map(lambda x: 'pen'+(str(int(x))))\n",
    "dico_h_p = dict(zip(df_daily['HenID'].tolist(), df_daily['PenID'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the distance across pairs of MLPS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lets compute every day a network per zone with nodes=hens, link=DTW_zone_X. For this we need to compute mvt distances between hens on same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 52/52 [6:25:33<00:00, 444.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['li_chi2_5', 'li_chi2_10', 'li_chi2_15', 'li_chi2_20', 'li_chi2_30']\n",
      "(397789, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level1</th>\n",
       "      <th>level2</th>\n",
       "      <th>henID1</th>\n",
       "      <th>henID2</th>\n",
       "      <th>chi2distance</th>\n",
       "      <th>DOA</th>\n",
       "      <th>WOA</th>\n",
       "      <th>nbr_obs_chi2all_5</th>\n",
       "      <th>li_chi2_5</th>\n",
       "      <th>chi2distance_ALL_5</th>\n",
       "      <th>...</th>\n",
       "      <th>nbr_obs_l2_30_5_Zone</th>\n",
       "      <th>DTW_30_4_Zone</th>\n",
       "      <th>nbr_obs_l1_30_4_Zone</th>\n",
       "      <th>nbr_obs_l2_30_4_Zone</th>\n",
       "      <th>DTW_30_1_Zone</th>\n",
       "      <th>nbr_obs_l1_30_1_Zone</th>\n",
       "      <th>nbr_obs_l2_30_1_Zone</th>\n",
       "      <th>weeks_in_laying_barn</th>\n",
       "      <th>hen_pair</th>\n",
       "      <th>DTW_15_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>hen_1</td>\n",
       "      <td>hen_10</td>\n",
       "      <td>0.233889</td>\n",
       "      <td>119</td>\n",
       "      <td>17</td>\n",
       "      <td>97</td>\n",
       "      <td>[0.9999999999989999, 0.9999999999989999, 0.999...</td>\n",
       "      <td>0.676595</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>20.8</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>hen_1-hen_10</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5686</th>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>hen_2</td>\n",
       "      <td>hen_47</td>\n",
       "      <td>0.341557</td>\n",
       "      <td>119</td>\n",
       "      <td>17</td>\n",
       "      <td>97</td>\n",
       "      <td>[0.9999999999989999, 0.9999999999989999, 0.023...</td>\n",
       "      <td>0.687179</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>hen_2-hen_47</td>\n",
       "      <td>158.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5685</th>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>hen_2</td>\n",
       "      <td>hen_46</td>\n",
       "      <td>0.110694</td>\n",
       "      <td>119</td>\n",
       "      <td>17</td>\n",
       "      <td>97</td>\n",
       "      <td>[0.0, 0.0948905109484006, 0.5267175572512697, ...</td>\n",
       "      <td>0.647620</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>hen_2-hen_46</td>\n",
       "      <td>152.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         level1     level2 henID1  henID2  chi2distance  DOA  WOA  \\\n",
       "0    2020-09-30 2020-09-30  hen_1  hen_10      0.233889  119   17   \n",
       "5686 2020-09-30 2020-09-30  hen_2  hen_47      0.341557  119   17   \n",
       "5685 2020-09-30 2020-09-30  hen_2  hen_46      0.110694  119   17   \n",
       "\n",
       "      nbr_obs_chi2all_5                                          li_chi2_5  \\\n",
       "0                    97  [0.9999999999989999, 0.9999999999989999, 0.999...   \n",
       "5686                 97  [0.9999999999989999, 0.9999999999989999, 0.023...   \n",
       "5685                 97  [0.0, 0.0948905109484006, 0.5267175572512697, ...   \n",
       "\n",
       "      chi2distance_ALL_5  ...  nbr_obs_l2_30_5_Zone  DTW_30_4_Zone  \\\n",
       "0               0.676595  ...                    17           20.8   \n",
       "5686            0.687179  ...                    17           23.0   \n",
       "5685            0.647620  ...                    17           23.0   \n",
       "\n",
       "      nbr_obs_l1_30_4_Zone  nbr_obs_l2_30_4_Zone  DTW_30_1_Zone  \\\n",
       "0                       17                    17            0.0   \n",
       "5686                    17                    17            0.0   \n",
       "5685                    17                    17            0.0   \n",
       "\n",
       "      nbr_obs_l1_30_1_Zone  nbr_obs_l2_30_1_Zone  weeks_in_laying_barn  \\\n",
       "0                       17                    17                     1   \n",
       "5686                    17                    17                     1   \n",
       "5685                    17                    17                     1   \n",
       "\n",
       "          hen_pair  DTW_15_all  \n",
       "0     hen_1-hen_10       154.0  \n",
       "5686  hen_2-hen_47       158.6  \n",
       "5685  hen_2-hen_46       152.9  \n",
       "\n",
       "[3 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#compute the CHI2DISTANCE & DTW for each bird with all other birds\n",
    "li_df = []\n",
    "df.sort_values(['Timestamp'], inplace=True)\n",
    "#take (subset of) dates and hens that are available (i.e. for which we comptue the variable on)\n",
    "#df_daily_h = df_daily[(df_daily['DOA']>=150)&(df_daily['DOA']<=380)].copy()\n",
    "li_date = set(df_daily_h['level'].tolist())\n",
    "for d in tqdm.tqdm(sorted(list(li_date))):\n",
    "    li_hen_d = df_daily_h[df_daily_h['level']==d]['HenID'].unique()\n",
    "    #its a symmetric measure, so we only do each combination once\n",
    "    for i,h1 in enumerate(li_hen_d):\n",
    "\n",
    "        #lets compute for hen h1 its difference with all other birds\n",
    "        for h2 in li_hen_d[i+1:]:\n",
    "            \n",
    "            ############ Overall aviary usage similarity across days\n",
    "            l1_chi2 = df_daily_h[(df_daily_h['HenID']==h1)&(df_daily_h['level']==d)]['dur_values_normalized'].values[0]\n",
    "            l2_chi2 = df_daily_h[(df_daily_h['HenID']==h2)&(df_daily_h['level']==d)]['dur_values_normalized'].values[0]\n",
    "            dico_ = {'date':d, 'henId':h1, 'henID2':h2, 'chi2distance':chi2_distance(l1_chi2,l2_chi2)}\n",
    "            #'dur_values_normalized_henId':l1_chi2,'dur_values_normalized_henID2':l2_chi2,\n",
    "\n",
    "            ############# Specifics' zones usage similarity across days\n",
    "            for nbr_binmn in li_binmn:\n",
    "                #ALL zone together with chi2distance\n",
    "                l1_chi2 = dico_pen_bin_level_h[dico_h_p[h1]][nbr_binmn][d][h1]\n",
    "                l2_chi2 = dico_pen_bin_level_h[dico_h_p[h2]][nbr_binmn][d][h2]\n",
    "                dico_['nbr_obs_chi2all_'+str(nbr_binmn)] = len(l1_chi2)\n",
    "                li_chi2 = [chi2_distance(l1_chi2[i],l2_chi2[i]) for i in range(0,len(l1_chi2))]\n",
    "                dico_['li_chi2_'+str(nbr_binmn)] = li_chi2\n",
    "                dico_['chi2distance_ALL_'+str(nbr_binmn)] = np.mean(li_chi2)\n",
    "                \n",
    "                #per zone with DTW\n",
    "                for ZONE in li_zone:\n",
    "                    dtw_value = np.nan\n",
    "                    try:\n",
    "                        l1_dtw = np.array(dico_pen_bin_zone_level_h[dico_h_p[h1]][nbr_binmn][ZONE][d][h1], dtype=np.double)\n",
    "                        l2_dtw = np.array(dico_pen_bin_zone_level_h[dico_h_p[h2]][nbr_binmn][ZONE][d][h2], dtype=np.double)\n",
    "                        #compute distance measure\n",
    "                        #psi: #of start and end points of a sequence can be ignored if this would lead to a lower distance\n",
    "                        dtw_value, paths = dtw.warping_paths(l1_dtw, l2_dtw, window=dico_window[nbr_binmn], psi=0, penalty=penalty)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        sys.exit()\n",
    "                        pass #dont do anything\n",
    "                    dico_['DTW_'+str(nbr_binmn)+'_'+ZONE] = round(dtw_value,1)  \n",
    "                    dico_['nbr_obs_l1_'+str(nbr_binmn)+'_'+ZONE] = len(l1_dtw)\n",
    "                    dico_['nbr_obs_l2_'+str(nbr_binmn)+'_'+ZONE] = len(l2_dtw)\n",
    "            li_df.append(dico_)\n",
    "\n",
    "#turn inter and intra distances into a dataframe                \n",
    "df_inter = pd.DataFrame(li_df)\n",
    "#df_inter = df_inter[~df_inter['chi2distance'].isnull()]\n",
    "df_inter = df_inter.sort_values('DOA', ascending=True)\n",
    "df_inter['hen_pair'] = df_inter.apply(lambda x: '-'.join(sorted([x['henId'],x['henID2']])), axis=1)\n",
    "df_inter['DTW_15_all'] = df_inter[['DTW_15_1_Zone', 'DTW_15_2_Zone', 'DTW_15_3_Zone','DTW_15_4_Zone','DTW_15_5_Zone']].sum(axis=1)\n",
    "li_col = list(df_inter.columns)\n",
    "li_remove = [i for i in li_col if i.startswith('li_chi2_')]\n",
    "print(li_remove)\n",
    "li_keep = [i for i in li_col if i not in li_remove]\n",
    "#df_inter.iloc[0:1000].to_csv(os.path.join(path_extracted_data_visual_adap, id_run+'_df_DistanceBetweenHenSim_4verification.csv'), sep=';', index=False)\n",
    "#df_inter.filter(li_keep).to_csv(os.path.join(path_extracted_data_visual_adap ,id_run+'_df_DistanceBetweenHenSim.csv'), sep=';', index=False)\n",
    "df_inter.to_csv(os.path.join(path_extracted_data ,id_run+'_df_DistanceBetweenHenSim_DAILYLEVEL.csv'), sep=';', index=False)\n",
    "print(df_inter.shape)\n",
    "display(df_inter.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-13a8aff00e49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath_extracted_data_visual_adap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_extracted_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'visual'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Treatment&Classs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'adaptability'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_interintra\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_extracted_data_visual_adap\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mid_run\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_df_DistanceBetweenHenSim_DAILYLEVEL.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_interintra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_interintra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2145\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2146\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2147\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \"\"\"\n\u001b[0;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_extracted_data_visual_adap = os.path.join(path_extracted_data,'visual','Treatment&Classs','adaptability')\n",
    "df_inter = pd.read_csv(os.path.join(path_extracted_data_visual_adap ,id_run+'_df_DistanceBetweenHenSim_DAILYLEVEL.csv'), sep=';')\n",
    "display(df_inter.head(3))\n",
    "df_inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_inter['DTW_15_all']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_inter['DTW_15_1_Zone']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_inter['DTW_15_2_Zone']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_inter['DTW_15_3_Zone']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_inter['DTW_15_4_Zone']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_inter['DTW_15_5_Zone']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_distance_mindist = {'DTW_15_all':, \n",
    "                         'DTW_15_1_Zone':, \n",
    "                         'DTW_15_2_Zone':, \n",
    "                         'DTW_15_3_Zone':,\n",
    "                         'DTW_15_4_Zone':,\n",
    "                         'DTW_15_5_Zone':}\n",
    "li_df = []\n",
    "for d in tqdm.tqdm(sorted(list(li_date))):\n",
    "    \n",
    "    df_graph = df_inter[df_inter['date']==d].copy()\n",
    "    \n",
    "    #one graph per distance\n",
    "    for D,min_dist in dico_distance_mindist.items():\n",
    "        #initialise graph\n",
    "        G = nx.Graph()\n",
    "        #assign graph attributes (i.e. similarity computation)\n",
    "        G.graph['similarity computation']= D\n",
    "\n",
    "        #add nodes (s: with attributes if needed)\n",
    "        li_hen_d = df_daily_h[df_daily_h['level']==d]['HenID'].unique()\n",
    "        for h in li_hen_d:\n",
    "            #s = max(df_movement[df_movement['HenID']==h]['severity'].tolist())\n",
    "            G.add_node(h) #attributes:, max_severity=int(round(s,0)))\n",
    "\n",
    "        #add edges\n",
    "        li_edges = [] \n",
    "        #we use a symmetric measure, so we only do each combination once\n",
    "        for i,h1 in enumerate(li_hen_d):\n",
    "            #lets compute for hen h1 its difference with all other birds\n",
    "            for h2 in li_hen_d[i+1:]:\n",
    "                dist = df_graph[df_graph['hen_pair']=='-'.join(sorted([x['h1'],x['h2']]))][D].values\n",
    "                pprint(dist)\n",
    "                #link if dist < min_dist\n",
    "                if dist<min_dist:\n",
    "                    li_edges.append((h1, h2, {'similarity':int(round(dist,2))}))     \n",
    "        G.add_edges_from(li_edges)\n",
    "\n",
    "        #save G (networkX graph) in GEXF format for gephi\n",
    "        nx.write_gexf(G, os.path.join(path_extracted_data_SNA,D+str(d).split(' ')[0]+'.gexf'))\n",
    "        \n",
    "        #centrality measures\n",
    "        #returns a Dictionary of nodes with degree centrality as the value\n",
    "        #The degree centrality values are normalized by dividing by the maximum possible degree in a simple graph n-1 where n is \n",
    "        #the number of nodes in G.\n",
    "        dico_v_dg = degree_centrality(G)\n",
    "        #Compute the shortest-path betweenness centrality for nodes. see doc for additional info\n",
    "        #return a Dictionary of nodes with betweenness centrality as the value.\n",
    "        dico_v_bw = betweenness_centrality(G, seed=0)\n",
    "        #Compute closeness centrality for nodesee doc for additional info\n",
    "        #return a Dictionary of nodes with closeness centrality as the value\n",
    "        dico_v_cl = closeness_centrality(G)\n",
    "        #into a datfarme\n",
    "        df_ = pd.DataFrame(list(dico_v_cl.items()),columns = ['HenID','closeness'])\n",
    "        df_['betweeness'] = df_['HenID'].map(lambda x: dico_v_bw[x])\n",
    "        df_['degree'] = df_['HenID'].map(lambda x: dico_v_dg[x])\n",
    "        df_['date'] = d  \n",
    "        df_['distance_measure'] = D\n",
    "        li_df.append(df_)\n",
    "        sys.exit()\n",
    "df_SNA = pd.concat(li_df,axis=1)\n",
    "print(df_SNA.shape)\n",
    "df_SNA.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
