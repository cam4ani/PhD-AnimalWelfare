{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic package\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import operator\n",
    "from dtaidistance import dtw\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle #to save/load list of selected hens\n",
    "\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#network\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "from UTILS import chi2_distance, correct_key\n",
    "import config_origins as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change the configuration file if not done yet!\n",
      "correctlightschedule_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print('change the configuration file if not done yet!')\n",
    "path_extracted_data = config.path_extracted_data\n",
    "path_initial_data = config.path_initial_data\n",
    "id_run = config.id_run\n",
    "#id_run = 'chapter0_final_'\n",
    "#path_dataoutput = r'G:\\VPHI\\Welfare\\2- Research Projects\\OFHE2.OriginsE2\\DataOutput'\n",
    "#path_extracted_data = os.path.join(path_dataoutput,'TrackingSystem') \n",
    "#path_extracted_data = os.path.join(path_extracted_data, id_run)\n",
    "dico_night_hour = config.dico_night_hour\n",
    "dico_matching = config.dico_matching\n",
    "li_binmn = config.li_binmn\n",
    "penalty = config.penalty\n",
    "dico_window = config.dico_window\n",
    "path_extracted_data_SNA = config.path_extracted_data_SNA\n",
    "#create a director if not existing\n",
    "if not os.path.exists(path_extracted_data_SNA):\n",
    "    os.makedirs(path_extracted_data_SNA)\n",
    "print(id_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43285, 165)\n",
      "(42905, 165)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HenID</th>\n",
       "      <th>level</th>\n",
       "      <th>duration_1_Zone</th>\n",
       "      <th>duration_2_Zone</th>\n",
       "      <th>duration_3_Zone</th>\n",
       "      <th>duration_4_Zone</th>\n",
       "      <th>duration_5_Zone</th>\n",
       "      <th>verification_daily_total_duration</th>\n",
       "      <th>verification_daily_total_nbr_hour</th>\n",
       "      <th>dur_values</th>\n",
       "      <th>...</th>\n",
       "      <th>nbr_transition_at_h21</th>\n",
       "      <th>nbr_transition_at_h22</th>\n",
       "      <th>nbr_transition_at_h23</th>\n",
       "      <th>nbr_transition_next1hafterlightoff</th>\n",
       "      <th>nbr_h_per_day</th>\n",
       "      <th>correct_amount_of_hour</th>\n",
       "      <th>DOA</th>\n",
       "      <th>WOA</th>\n",
       "      <th>DIB</th>\n",
       "      <th>WIB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hen_130</td>\n",
       "      <td>2020-10-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 32400.0]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>124</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hen_130</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>31295.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[0.0, 0.0, 780.0, 325.0, 31295.0]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>125</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hen_130</td>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>31815.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[0.0, 206.0, 249.0, 130.0, 31815.0]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>126</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HenID      level  duration_1_Zone  duration_2_Zone  duration_3_Zone  \\\n",
       "0  hen_130 2020-10-05              0.0              0.0              0.0   \n",
       "1  hen_130 2020-10-06              0.0              0.0            780.0   \n",
       "2  hen_130 2020-10-07              0.0            206.0            249.0   \n",
       "\n",
       "   duration_4_Zone  duration_5_Zone  verification_daily_total_duration  \\\n",
       "0              0.0          32400.0                            32400.0   \n",
       "1            325.0          31295.0                            32400.0   \n",
       "2            130.0          31815.0                            32400.0   \n",
       "\n",
       "   verification_daily_total_nbr_hour                           dur_values  \\\n",
       "0                                9.0        [0.0, 0.0, 0.0, 0.0, 32400.0]   \n",
       "1                                9.0    [0.0, 0.0, 780.0, 325.0, 31295.0]   \n",
       "2                                9.0  [0.0, 206.0, 249.0, 130.0, 31815.0]   \n",
       "\n",
       "   ... nbr_transition_at_h21  nbr_transition_at_h22  nbr_transition_at_h23  \\\n",
       "0  ...                   0.0                    0.0                    0.0   \n",
       "1  ...                   0.0                    0.0                    0.0   \n",
       "2  ...                   0.0                    0.0                    0.0   \n",
       "\n",
       "   nbr_transition_next1hafterlightoff  nbr_h_per_day  correct_amount_of_hour  \\\n",
       "0                                 0.0              9                    True   \n",
       "1                                 0.0              9                    True   \n",
       "2                                 0.0              9                    True   \n",
       "\n",
       "   DOA  WOA DIB WIB  \n",
       "0  124   18   6   1  \n",
       "1  125   18   7   1  \n",
       "2  126   18   8   2  \n",
       "\n",
       "[3 rows x 165 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#daily variables (one row per (henID, date))\n",
    "#daily_ALL_Variable_Tranformed\n",
    "df_daily = pd.read_csv(os.path.join(path_extracted_data, id_run+'_daily_ALL_variables.csv'), sep=';',\n",
    "                     parse_dates=['level'], dayfirst=True) \n",
    "print(df_daily.shape)\n",
    "\n",
    "#filter by date\n",
    "df_daily = df_daily[df_daily['level']>dt.datetime(2020,9,29)]\n",
    "\n",
    "#dico of hen as keys and pen as values\n",
    "df_daily['PenID'] = df_daily['PenID'].map(lambda x: 'pen'+(str(int(x))))\n",
    "dico_h_p = dict(zip(df_daily['HenID'].tolist(), df_daily['PenID'].tolist()))\n",
    "print(df_daily.shape)\n",
    "\n",
    "#removing data over night but not existing over days\n",
    "df_daily = df_daily[~df_daily['verification_daily_total_duration'].isnull()]\n",
    "\n",
    "print(df_daily.shape)\n",
    "df_daily.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance across pairs of MLPS from same day with %time in a same zone- for daily SNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#download the MLPs vectors during day only! as we compare on same day, we can restrict to the exact daily calendar!\n",
    "dico_pen_level_h = pickle.load(open(os.path.join(path_extracted_data, \n",
    "                                                 id_run+'dico_pen_level_h_allzoneidseclevel_DAILYLEVEL.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lets compute every day a weighted network per zone with nodes=hens, link=DTW_zone_X. For this we need to compute mvt distances between hens on same day.\n",
    "same as the one for the ranging study of Mike (without random pens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#dico of hen as keys and pen as values\n",
    "dico_h_p = dict(zip(df_daily['HenID'].tolist(), df_daily['PenID'].tolist()))\n",
    "#df_ = df_daily.groupby('PenID')['HenID'].agg(lambda x: list(set(x))).reset_index()\n",
    "#dico_penid_lihen = dict(zip(df_['PenID'].tolist(), df_['HenID'].tolist()))\n",
    "#dico_cla_cla = {'EPI':'other', 'MEXP':'MEXP', 'LEXP':'LEXP', 'NewAfterEpi':'other', 'LEXPLOST':'other', 'MEXPLOST':'other'}\n",
    "#df_daily['CLASS'] = df_daily['CLASS'].map(lambda x: dico_cla_cla[x])\n",
    "#dico_h_class = dict(zip(df_daily['HenID'].tolist(), df_daily['CLASS'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3625200.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max size\n",
    "(160*159/2)*285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Timestamp('2020-09-30 00:00:00'),\n",
       " Timestamp('2020-10-01 00:00:00'),\n",
       " Timestamp('2020-10-02 00:00:00'),\n",
       " Timestamp('2020-10-03 00:00:00'),\n",
       " Timestamp('2020-10-04 00:00:00'),\n",
       " Timestamp('2020-10-05 00:00:00'),\n",
       " Timestamp('2020-10-06 00:00:00'),\n",
       " Timestamp('2020-10-07 00:00:00'),\n",
       " Timestamp('2020-10-10 00:00:00'),\n",
       " Timestamp('2020-10-11 00:00:00'),\n",
       " Timestamp('2020-10-12 00:00:00'),\n",
       " Timestamp('2020-10-13 00:00:00'),\n",
       " Timestamp('2020-10-14 00:00:00'),\n",
       " Timestamp('2020-10-15 00:00:00'),\n",
       " Timestamp('2020-10-16 00:00:00'),\n",
       " Timestamp('2020-10-17 00:00:00'),\n",
       " Timestamp('2020-10-18 00:00:00'),\n",
       " Timestamp('2020-10-19 00:00:00'),\n",
       " Timestamp('2020-10-20 00:00:00'),\n",
       " Timestamp('2020-10-21 00:00:00'),\n",
       " Timestamp('2020-10-22 00:00:00'),\n",
       " Timestamp('2020-10-23 00:00:00'),\n",
       " Timestamp('2020-10-24 00:00:00'),\n",
       " Timestamp('2020-10-25 00:00:00'),\n",
       " Timestamp('2020-10-26 00:00:00'),\n",
       " Timestamp('2020-10-27 00:00:00'),\n",
       " Timestamp('2020-10-28 00:00:00'),\n",
       " Timestamp('2020-10-29 00:00:00'),\n",
       " Timestamp('2020-10-30 00:00:00'),\n",
       " Timestamp('2020-10-31 00:00:00'),\n",
       " Timestamp('2020-11-01 00:00:00'),\n",
       " Timestamp('2020-11-02 00:00:00'),\n",
       " Timestamp('2020-11-03 00:00:00'),\n",
       " Timestamp('2020-11-04 00:00:00'),\n",
       " Timestamp('2020-11-05 00:00:00'),\n",
       " Timestamp('2020-11-06 00:00:00'),\n",
       " Timestamp('2020-11-07 00:00:00'),\n",
       " Timestamp('2020-11-08 00:00:00'),\n",
       " Timestamp('2020-11-09 00:00:00'),\n",
       " Timestamp('2020-11-10 00:00:00'),\n",
       " Timestamp('2020-11-11 00:00:00'),\n",
       " Timestamp('2020-11-12 00:00:00'),\n",
       " Timestamp('2020-11-13 00:00:00'),\n",
       " Timestamp('2020-11-14 00:00:00'),\n",
       " Timestamp('2020-11-15 00:00:00'),\n",
       " Timestamp('2020-11-16 00:00:00'),\n",
       " Timestamp('2020-11-17 00:00:00'),\n",
       " Timestamp('2020-11-18 00:00:00'),\n",
       " Timestamp('2020-11-19 00:00:00'),\n",
       " Timestamp('2020-11-20 00:00:00'),\n",
       " Timestamp('2020-11-21 00:00:00'),\n",
       " Timestamp('2020-11-22 00:00:00'),\n",
       " Timestamp('2020-11-24 00:00:00'),\n",
       " Timestamp('2020-11-25 00:00:00'),\n",
       " Timestamp('2020-11-26 00:00:00'),\n",
       " Timestamp('2020-11-27 00:00:00'),\n",
       " Timestamp('2020-11-28 00:00:00'),\n",
       " Timestamp('2020-11-29 00:00:00'),\n",
       " Timestamp('2020-11-30 00:00:00'),\n",
       " Timestamp('2020-12-01 00:00:00'),\n",
       " Timestamp('2020-12-02 00:00:00'),\n",
       " Timestamp('2020-12-03 00:00:00'),\n",
       " Timestamp('2020-12-04 00:00:00'),\n",
       " Timestamp('2020-12-05 00:00:00'),\n",
       " Timestamp('2020-12-06 00:00:00'),\n",
       " Timestamp('2020-12-07 00:00:00'),\n",
       " Timestamp('2020-12-08 00:00:00'),\n",
       " Timestamp('2020-12-09 00:00:00'),\n",
       " Timestamp('2020-12-10 00:00:00'),\n",
       " Timestamp('2020-12-11 00:00:00'),\n",
       " Timestamp('2020-12-12 00:00:00'),\n",
       " Timestamp('2020-12-13 00:00:00'),\n",
       " Timestamp('2020-12-14 00:00:00'),\n",
       " Timestamp('2020-12-15 00:00:00'),\n",
       " Timestamp('2020-12-16 00:00:00'),\n",
       " Timestamp('2020-12-17 00:00:00'),\n",
       " Timestamp('2020-12-18 00:00:00'),\n",
       " Timestamp('2020-12-19 00:00:00'),\n",
       " Timestamp('2020-12-20 00:00:00'),\n",
       " Timestamp('2020-12-21 00:00:00'),\n",
       " Timestamp('2020-12-23 00:00:00'),\n",
       " Timestamp('2020-12-24 00:00:00'),\n",
       " Timestamp('2020-12-25 00:00:00'),\n",
       " Timestamp('2020-12-26 00:00:00'),\n",
       " Timestamp('2020-12-27 00:00:00'),\n",
       " Timestamp('2020-12-28 00:00:00'),\n",
       " Timestamp('2020-12-29 00:00:00'),\n",
       " Timestamp('2020-12-30 00:00:00'),\n",
       " Timestamp('2020-12-31 00:00:00'),\n",
       " Timestamp('2021-01-01 00:00:00'),\n",
       " Timestamp('2021-01-02 00:00:00'),\n",
       " Timestamp('2021-01-03 00:00:00'),\n",
       " Timestamp('2021-01-05 00:00:00'),\n",
       " Timestamp('2021-01-06 00:00:00'),\n",
       " Timestamp('2021-01-07 00:00:00'),\n",
       " Timestamp('2021-01-08 00:00:00'),\n",
       " Timestamp('2021-01-09 00:00:00'),\n",
       " Timestamp('2021-01-10 00:00:00'),\n",
       " Timestamp('2021-01-11 00:00:00'),\n",
       " Timestamp('2021-01-12 00:00:00'),\n",
       " Timestamp('2021-01-13 00:00:00'),\n",
       " Timestamp('2021-01-14 00:00:00'),\n",
       " Timestamp('2021-01-15 00:00:00'),\n",
       " Timestamp('2021-01-16 00:00:00'),\n",
       " Timestamp('2021-01-17 00:00:00'),\n",
       " Timestamp('2021-01-18 00:00:00'),\n",
       " Timestamp('2021-01-19 00:00:00'),\n",
       " Timestamp('2021-01-20 00:00:00'),\n",
       " Timestamp('2021-01-21 00:00:00'),\n",
       " Timestamp('2021-01-22 00:00:00'),\n",
       " Timestamp('2021-01-23 00:00:00'),\n",
       " Timestamp('2021-01-24 00:00:00'),\n",
       " Timestamp('2021-01-30 00:00:00'),\n",
       " Timestamp('2021-01-31 00:00:00'),\n",
       " Timestamp('2021-02-03 00:00:00'),\n",
       " Timestamp('2021-02-04 00:00:00'),\n",
       " Timestamp('2021-02-05 00:00:00'),\n",
       " Timestamp('2021-02-06 00:00:00'),\n",
       " Timestamp('2021-02-07 00:00:00'),\n",
       " Timestamp('2021-02-08 00:00:00'),\n",
       " Timestamp('2021-02-09 00:00:00'),\n",
       " Timestamp('2021-02-10 00:00:00'),\n",
       " Timestamp('2021-02-11 00:00:00'),\n",
       " Timestamp('2021-02-12 00:00:00'),\n",
       " Timestamp('2021-02-13 00:00:00'),\n",
       " Timestamp('2021-02-14 00:00:00'),\n",
       " Timestamp('2021-02-15 00:00:00'),\n",
       " Timestamp('2021-02-16 00:00:00'),\n",
       " Timestamp('2021-02-17 00:00:00'),\n",
       " Timestamp('2021-02-18 00:00:00'),\n",
       " Timestamp('2021-02-19 00:00:00'),\n",
       " Timestamp('2021-02-20 00:00:00'),\n",
       " Timestamp('2021-02-21 00:00:00'),\n",
       " Timestamp('2021-02-22 00:00:00'),\n",
       " Timestamp('2021-02-23 00:00:00'),\n",
       " Timestamp('2021-02-24 00:00:00'),\n",
       " Timestamp('2021-02-25 00:00:00'),\n",
       " Timestamp('2021-02-26 00:00:00'),\n",
       " Timestamp('2021-02-27 00:00:00'),\n",
       " Timestamp('2021-02-28 00:00:00'),\n",
       " Timestamp('2021-03-01 00:00:00'),\n",
       " Timestamp('2021-03-02 00:00:00'),\n",
       " Timestamp('2021-03-03 00:00:00'),\n",
       " Timestamp('2021-03-04 00:00:00'),\n",
       " Timestamp('2021-03-05 00:00:00'),\n",
       " Timestamp('2021-03-06 00:00:00'),\n",
       " Timestamp('2021-03-07 00:00:00'),\n",
       " Timestamp('2021-03-08 00:00:00'),\n",
       " Timestamp('2021-03-09 00:00:00'),\n",
       " Timestamp('2021-03-10 00:00:00'),\n",
       " Timestamp('2021-03-11 00:00:00'),\n",
       " Timestamp('2021-03-12 00:00:00'),\n",
       " Timestamp('2021-03-13 00:00:00'),\n",
       " Timestamp('2021-03-14 00:00:00'),\n",
       " Timestamp('2021-03-15 00:00:00'),\n",
       " Timestamp('2021-03-16 00:00:00'),\n",
       " Timestamp('2021-03-17 00:00:00'),\n",
       " Timestamp('2021-03-18 00:00:00'),\n",
       " Timestamp('2021-03-19 00:00:00'),\n",
       " Timestamp('2021-03-20 00:00:00'),\n",
       " Timestamp('2021-03-21 00:00:00'),\n",
       " Timestamp('2021-03-22 00:00:00'),\n",
       " Timestamp('2021-03-23 00:00:00'),\n",
       " Timestamp('2021-03-24 00:00:00'),\n",
       " Timestamp('2021-03-25 00:00:00'),\n",
       " Timestamp('2021-03-26 00:00:00'),\n",
       " Timestamp('2021-03-27 00:00:00'),\n",
       " Timestamp('2021-03-29 00:00:00'),\n",
       " Timestamp('2021-03-30 00:00:00'),\n",
       " Timestamp('2021-03-31 00:00:00'),\n",
       " Timestamp('2021-04-01 00:00:00'),\n",
       " Timestamp('2021-04-02 00:00:00'),\n",
       " Timestamp('2021-04-03 00:00:00'),\n",
       " Timestamp('2021-04-04 00:00:00'),\n",
       " Timestamp('2021-04-05 00:00:00'),\n",
       " Timestamp('2021-04-06 00:00:00'),\n",
       " Timestamp('2021-04-07 00:00:00'),\n",
       " Timestamp('2021-04-08 00:00:00'),\n",
       " Timestamp('2021-04-09 00:00:00'),\n",
       " Timestamp('2021-04-10 00:00:00'),\n",
       " Timestamp('2021-04-11 00:00:00'),\n",
       " Timestamp('2021-04-12 00:00:00'),\n",
       " Timestamp('2021-04-13 00:00:00'),\n",
       " Timestamp('2021-04-14 00:00:00'),\n",
       " Timestamp('2021-04-15 00:00:00'),\n",
       " Timestamp('2021-04-16 00:00:00'),\n",
       " Timestamp('2021-04-17 00:00:00'),\n",
       " Timestamp('2021-04-18 00:00:00'),\n",
       " Timestamp('2021-04-19 00:00:00'),\n",
       " Timestamp('2021-04-20 00:00:00'),\n",
       " Timestamp('2021-04-21 00:00:00'),\n",
       " Timestamp('2021-04-22 00:00:00'),\n",
       " Timestamp('2021-04-23 00:00:00'),\n",
       " Timestamp('2021-04-24 00:00:00'),\n",
       " Timestamp('2021-04-25 00:00:00'),\n",
       " Timestamp('2021-04-26 00:00:00'),\n",
       " Timestamp('2021-04-27 00:00:00'),\n",
       " Timestamp('2021-04-28 00:00:00'),\n",
       " Timestamp('2021-04-29 00:00:00'),\n",
       " Timestamp('2021-04-30 00:00:00'),\n",
       " Timestamp('2021-05-01 00:00:00'),\n",
       " Timestamp('2021-05-02 00:00:00'),\n",
       " Timestamp('2021-05-03 00:00:00'),\n",
       " Timestamp('2021-05-04 00:00:00'),\n",
       " Timestamp('2021-05-05 00:00:00'),\n",
       " Timestamp('2021-05-06 00:00:00'),\n",
       " Timestamp('2021-05-07 00:00:00'),\n",
       " Timestamp('2021-05-08 00:00:00'),\n",
       " Timestamp('2021-05-09 00:00:00'),\n",
       " Timestamp('2021-05-10 00:00:00'),\n",
       " Timestamp('2021-05-11 00:00:00'),\n",
       " Timestamp('2021-05-12 00:00:00'),\n",
       " Timestamp('2021-05-13 00:00:00'),\n",
       " Timestamp('2021-05-14 00:00:00'),\n",
       " Timestamp('2021-05-15 00:00:00'),\n",
       " Timestamp('2021-05-16 00:00:00'),\n",
       " Timestamp('2021-05-17 00:00:00'),\n",
       " Timestamp('2021-05-18 00:00:00'),\n",
       " Timestamp('2021-05-19 00:00:00'),\n",
       " Timestamp('2021-05-20 00:00:00'),\n",
       " Timestamp('2021-05-21 00:00:00'),\n",
       " Timestamp('2021-05-22 00:00:00'),\n",
       " Timestamp('2021-05-23 00:00:00'),\n",
       " Timestamp('2021-05-24 00:00:00'),\n",
       " Timestamp('2021-05-25 00:00:00'),\n",
       " Timestamp('2021-05-26 00:00:00'),\n",
       " Timestamp('2021-05-27 00:00:00'),\n",
       " Timestamp('2021-05-28 00:00:00'),\n",
       " Timestamp('2021-05-29 00:00:00'),\n",
       " Timestamp('2021-05-30 00:00:00'),\n",
       " Timestamp('2021-05-31 00:00:00'),\n",
       " Timestamp('2021-06-01 00:00:00'),\n",
       " Timestamp('2021-06-03 00:00:00'),\n",
       " Timestamp('2021-06-04 00:00:00'),\n",
       " Timestamp('2021-06-05 00:00:00'),\n",
       " Timestamp('2021-06-06 00:00:00'),\n",
       " Timestamp('2021-06-07 00:00:00'),\n",
       " Timestamp('2021-06-08 00:00:00'),\n",
       " Timestamp('2021-06-09 00:00:00'),\n",
       " Timestamp('2021-06-10 00:00:00'),\n",
       " Timestamp('2021-06-11 00:00:00'),\n",
       " Timestamp('2021-06-12 00:00:00'),\n",
       " Timestamp('2021-06-13 00:00:00'),\n",
       " Timestamp('2021-06-14 00:00:00'),\n",
       " Timestamp('2021-06-15 00:00:00'),\n",
       " Timestamp('2021-06-16 00:00:00'),\n",
       " Timestamp('2021-06-17 00:00:00'),\n",
       " Timestamp('2021-06-18 00:00:00'),\n",
       " Timestamp('2021-06-19 00:00:00'),\n",
       " Timestamp('2021-06-20 00:00:00'),\n",
       " Timestamp('2021-06-21 00:00:00'),\n",
       " Timestamp('2021-06-22 00:00:00'),\n",
       " Timestamp('2021-06-23 00:00:00'),\n",
       " Timestamp('2021-06-24 00:00:00'),\n",
       " Timestamp('2021-06-25 00:00:00'),\n",
       " Timestamp('2021-06-26 00:00:00'),\n",
       " Timestamp('2021-06-27 00:00:00'),\n",
       " Timestamp('2021-06-28 00:00:00'),\n",
       " Timestamp('2021-06-29 00:00:00'),\n",
       " Timestamp('2021-06-30 00:00:00'),\n",
       " Timestamp('2021-07-01 00:00:00'),\n",
       " Timestamp('2021-07-02 00:00:00'),\n",
       " Timestamp('2021-07-03 00:00:00'),\n",
       " Timestamp('2021-07-04 00:00:00'),\n",
       " Timestamp('2021-07-05 00:00:00'),\n",
       " Timestamp('2021-07-06 00:00:00'),\n",
       " Timestamp('2021-07-07 00:00:00'),\n",
       " Timestamp('2021-07-08 00:00:00'),\n",
       " Timestamp('2021-07-09 00:00:00'),\n",
       " Timestamp('2021-07-10 00:00:00'),\n",
       " Timestamp('2021-07-11 00:00:00'),\n",
       " Timestamp('2021-07-12 00:00:00'),\n",
       " Timestamp('2021-07-13 00:00:00'),\n",
       " Timestamp('2021-07-14 00:00:00'),\n",
       " Timestamp('2021-07-15 00:00:00'),\n",
       " Timestamp('2021-07-16 00:00:00'),\n",
       " Timestamp('2021-07-17 00:00:00'),\n",
       " Timestamp('2021-07-18 00:00:00'),\n",
       " Timestamp('2021-07-19 00:00:00'),\n",
       " Timestamp('2021-07-20 00:00:00'),\n",
       " Timestamp('2021-07-21 00:00:00'),\n",
       " Timestamp('2021-07-22 00:00:00'),\n",
       " Timestamp('2021-07-23 00:00:00'),\n",
       " Timestamp('2021-07-24 00:00:00'),\n",
       " Timestamp('2021-07-25 00:00:00')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daily[df_daily['level']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time: 106.97 mn\n",
      "(99626, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HenID1</th>\n",
       "      <th>HenID2</th>\n",
       "      <th>date</th>\n",
       "      <th>total_nbr_sec_involved</th>\n",
       "      <th>nbr_sec_CommunZone</th>\n",
       "      <th>PenID1</th>\n",
       "      <th>PenID2</th>\n",
       "      <th>perc_time_communZone</th>\n",
       "      <th>hen_pair</th>\n",
       "      <th>pen_pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hen_131</td>\n",
       "      <td>hen_134</td>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>32400</td>\n",
       "      <td>17427</td>\n",
       "      <td>pen10</td>\n",
       "      <td>pen10</td>\n",
       "      <td>53.787037</td>\n",
       "      <td>hen_131-hen_134</td>\n",
       "      <td>pen10-pen10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hen_131</td>\n",
       "      <td>hen_138</td>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>32400</td>\n",
       "      <td>7520</td>\n",
       "      <td>pen10</td>\n",
       "      <td>pen10</td>\n",
       "      <td>23.209877</td>\n",
       "      <td>hen_131-hen_138</td>\n",
       "      <td>pen10-pen10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hen_131</td>\n",
       "      <td>hen_139</td>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>32400</td>\n",
       "      <td>11292</td>\n",
       "      <td>pen10</td>\n",
       "      <td>pen10</td>\n",
       "      <td>34.851852</td>\n",
       "      <td>hen_131-hen_139</td>\n",
       "      <td>pen10-pen10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    HenID1   HenID2       date  total_nbr_sec_involved  nbr_sec_CommunZone  \\\n",
       "0  hen_131  hen_134 2020-10-02                   32400               17427   \n",
       "1  hen_131  hen_138 2020-10-02                   32400                7520   \n",
       "2  hen_131  hen_139 2020-10-02                   32400               11292   \n",
       "\n",
       "  PenID1 PenID2  perc_time_communZone         hen_pair     pen_pair  \n",
       "0  pen10  pen10             53.787037  hen_131-hen_134  pen10-pen10  \n",
       "1  pen10  pen10             23.209877  hen_131-hen_138  pen10-pen10  \n",
       "2  pen10  pen10             34.851852  hen_131-hen_139  pen10-pen10  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start recording the time it last\n",
    "START_TIME = time.perf_counter()\n",
    "li_df = []\n",
    "li_date = sorted(set(df_daily['level'].tolist()))\n",
    "for d in tqdm.tqdm(sorted(list(li_date))):\n",
    "    #list of hens available that day (i.e. functioning tags)\n",
    "    li_hen_d = df_daily[df_daily['level']==d]['HenID'].unique()\n",
    "    #we use symmetric measures, so we only do each combination once\n",
    "    for i,h1 in enumerate(li_hen_d):\n",
    "        #lets compute for hen h1 its similarity with all other birds\n",
    "        for h2 in li_hen_d[i+1:]:\n",
    "            l1 = np.array(dico_pen_level_h[dico_h_p[h1]][d][h1])\n",
    "            l2 = np.array(dico_pen_level_h[dico_h_p[h2]][d][h2])\n",
    "            if len(l1)==len(l2):\n",
    "                li_df.append({'HenID1':h1,'HenID2':h2,'date':d,\n",
    "                              'total_nbr_sec_involved':len(l1),'nbr_sec_CommunZone':sum(l1==l2),\n",
    "                              'PenID1':dico_h_p[h1], 'PenID2':dico_h_p[h2]})\n",
    "            else: \n",
    "                print(e)\n",
    "                print(h1, h2)\n",
    "                print(d)\n",
    "END_TIME = time.perf_counter()\n",
    "print (\"Total running time: %.2f mn\" %((END_TIME-START_TIME)/60))\n",
    "df_prox = pd.DataFrame(li_df)\n",
    "df_prox['perc_time_communZone'] = df_prox.apply(lambda x: x['nbr_sec_CommunZone']/x['total_nbr_sec_involved']*100, axis=1)\n",
    "df_prox['hen_pair'] = df_prox.apply(lambda x: '-'.join(sorted([x['HenID1'],x['HenID2']])), axis=1)\n",
    "df_prox['pen_pair'] = df_prox.apply(lambda x: '-'.join(sorted([x['PenID1'],x['PenID2']])), axis=1)\n",
    "df_prox.to_csv(os.path.join(path_extracted_data, id_run+'df_proximity_perctime.csv'), index=False, sep=';')\n",
    "print(df_prox.shape)\n",
    "df_prox.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance across pairs of MLPS from same day with Chi2&DTW- for daily SNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#compute the CHI2DISTANCE & DTW for each bird with all other birds\n",
    "#start recording the time it last\n",
    "START_TIME = time.perf_counter()\n",
    "li_df = []\n",
    "#df.sort_values(['Timestamp'], inplace=True)\n",
    "#take (subset of) dates and hens that are available (i.e. for which we comptue the variable on)\n",
    "#df_daily_h = df_daily[(df_daily['DOA']>=150)&(df_daily['DOA']<=380)].copy()\n",
    "li_date = set(df_daily['level'].tolist())\n",
    "li_zone = ['1_Zone', '2_Zone', '3_Zone', '4_Zone', '5_Zone']\n",
    "for d in tqdm.tqdm(sorted(list(li_date))):\n",
    "    li_hen_d = df_daily[df_daily['level']==d]['HenID'].unique()\n",
    "    #we use symmetric measures, so we only do each combination once\n",
    "    for i,h1 in enumerate(li_hen_d):\n",
    "\n",
    "        #lets compute for hen h1 its difference with all other birds\n",
    "        for h2 in li_hen_d[i+1:]:\n",
    "            \n",
    "            try:\n",
    "                ############ Overall aviary usage similarity across days\n",
    "                l1_chi2 = df_daily[(df_daily['HenID']==h1)&(df_daily['level']==d)]['dur_values_normalized'].values[0]\n",
    "                l2_chi2 = df_daily[(df_daily['HenID']==h2)&(df_daily['level']==d)]['dur_values_normalized'].values[0]\n",
    "                dico_ = {'date':d, 'henId':h1, 'henID2':h2, 'chi2distance':chi2_distance(l1_chi2,l2_chi2, remove_warning=True)}\n",
    "                #'dur_values_normalized_henId':l1_chi2,'dur_values_normalized_henID2':l2_chi2,\n",
    "\n",
    "                ############# Specifics' zones usage similarity across days\n",
    "                for nbr_binmn in li_binmn:\n",
    "                    #ALL zone together with chi2distance\n",
    "                    l1_chi2 = dico_pen_bin_level_h[dico_h_p[h1]][nbr_binmn][d][h1]\n",
    "                    l2_chi2 = dico_pen_bin_level_h[dico_h_p[h2]][nbr_binmn][d][h2]\n",
    "                    dico_['nbr_obs_chi2all_'+str(nbr_binmn)] = len(l1_chi2)\n",
    "                    li_chi2 = [chi2_distance(l1_chi2[i],l2_chi2[i], remove_warning=True) for i in range(0,len(l1_chi2))]\n",
    "                    dico_['li_chi2_'+str(nbr_binmn)] = li_chi2\n",
    "                    dico_['chi2distance_ALL_'+str(nbr_binmn)] = np.mean(li_chi2)\n",
    "\n",
    "                    #per zone with DTW\n",
    "                    for ZONE in li_zone:\n",
    "                        dtw_value = np.nan\n",
    "                        try:\n",
    "                            l1_dtw = np.array(dico_pen_bin_zone_level_h[dico_h_p[h1]][nbr_binmn][ZONE][d][h1], dtype=np.double)\n",
    "                            l2_dtw = np.array(dico_pen_bin_zone_level_h[dico_h_p[h2]][nbr_binmn][ZONE][d][h2], dtype=np.double)\n",
    "                            #compute distance measure\n",
    "                            #psi: #of start and end points of a sequence can be ignored if this would lead to a lower distance\n",
    "                            dtw_value, paths = dtw.warping_paths(l1_dtw, l2_dtw, window=dico_window[nbr_binmn], psi=0, penalty=penalty)\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            sys.exit()\n",
    "                            pass #dont do anything\n",
    "                        dico_['DTW_'+str(nbr_binmn)+'_'+ZONE] = round(dtw_value,1)  \n",
    "                        dico_['nbr_obs_l1_'+str(nbr_binmn)+'_'+ZONE] = len(l1_dtw)\n",
    "                        dico_['nbr_obs_l2_'+str(nbr_binmn)+'_'+ZONE] = len(l2_dtw)\n",
    "                li_df.append(dico_)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(h1, h2)\n",
    "                print(d)\n",
    "                \n",
    "END_TIME = time.perf_counter()\n",
    "print (\"Total running time: %.2f mn\" %((END_TIME-START_TIME)/60))\n",
    "#turn inter and intra distances into a dataframe                \n",
    "df_inter = pd.DataFrame(li_df)\n",
    "#df_inter = df_inter[~df_inter['chi2distance'].isnull()]\n",
    "df_inter['hen_pair'] = df_inter.apply(lambda x: '-'.join(sorted([x['henId'],x['henID2']])), axis=1)\n",
    "df_inter['DTW_30_all'] = df_inter[['DTW_30_1_Zone', 'DTW_30_2_Zone', 'DTW_30_3_Zone','DTW_30_4_Zone','DTW_30_5_Zone']].sum(axis=1)\n",
    "li_col = list(df_inter.columns)\n",
    "li_remove = [i for i in li_col if i.startswith('li_chi2_')]\n",
    "print(li_remove)\n",
    "li_keep = [i for i in li_col if i not in li_remove]\n",
    "#df_inter.iloc[0:1000].to_csv(os.path.join(path_extracted_data_visual_adap, id_run+'_df_DistanceBetweenHenSim_4verification.csv'), sep=';', index=False)\n",
    "#df_inter.filter(li_keep).to_csv(os.path.join(path_extracted_data_visual_adap ,id_run+'_df_DistanceBetweenHenSim.csv'), sep=';', index=False)\n",
    "df_inter.to_csv(os.path.join(path_extracted_data ,id_run+'_df_DistanceBetweenHenSim_DAILYLEVEL.csv'), sep=';', index=False)\n",
    "print(df_inter.shape)\n",
    "display(df_inter.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_inter.shape)\n",
    "display(df_inter.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance across any pairs of MLPS (i.e. any days) - for daily clusterID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#download MLPs vectors\n",
    "#For now we restrict to 2h-17h in order to always compare same length TS as we will compare across days, the lenght might differ\n",
    "dico_pen_bin_level_h = pickle.load(open(os.path.join(path_extracted_data_visual_adap, \n",
    "                                                     id_run+'dico_pen_bin_level_h_2h-17h59LEVEL.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#compute mvt distances between any two MLPs on any day from any pen\n",
    "dico_hendate_mvt = {}\n",
    "for p,dico_bin_level_h in dico_pen_bin_level_h.items():\n",
    "    for level, dico_hen_mvt in dico_bin_level_h[15].items():\n",
    "        if level!=dt.datetime(2020,9,29):\n",
    "            for henID,limvt in dico_hen_mvt.items():\n",
    "                dico_hendate_mvt[henID+'/'+str(level).split(' ')[0]] = limvt\n",
    "dico_hendate_mvt = OrderedDict(dico_hendate_mvt)\n",
    "li_hendate = list(dico_hendate_mvt.keys())\n",
    "li_mvt = list(dico_hendate_mvt.values())\n",
    "len(dico_hendate_mvt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "li_df = []\n",
    "for i in tqdm.tqdm(range(0,len(li_mvt))):\n",
    "    li_mvt1 = li_mvt[i]\n",
    "    for j in range(i+1,len(li_mvt)):\n",
    "        dico_ = {}\n",
    "        li_mvt2 = li_mvt[j]\n",
    "        li_chi2 = [chi2_distance(li_mvt1[i],li_mvt2[i]) for i in range(0,len(li_mvt1))]\n",
    "        dico_['chi2distance_ALL_15'] = np.mean(li_chi2)\n",
    "        dico_['h1'] = li_hendate[i]\n",
    "        dico_['h2'] = li_hendate[j]\n",
    "        li_df.append(dico_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_anypair = pd.DataFrame(li_df)\n",
    "display(df_anypair[df_anypair['chi2distance_ALL_15'].isnull()])\n",
    "df_anypair['henID1'] = df_anypair['h1'].map(lambda x: x.split('/')[0])\n",
    "df_anypair['henID2'] = df_anypair['h2'].map(lambda x: x.split('/')[0])\n",
    "df_anypair['level1'] = df_anypair['h1'].map(lambda x: dt.datetime.strptime(x.split('/')[1], '%Y-%m-%d'))\n",
    "df_anypair['level2'] = df_anypair['h2'].map(lambda x: dt.datetime.strptime(x.split('/')[1], '%Y-%m-%d'))\n",
    "#df_interintra['weeks_in_laying_barn'] = df_interintra['DOA'].map(lambda x: int((x-119)/7)+1)\n",
    "#df_interintra['hen_pair'] = df_interintra.apply(lambda x: '-'.join(sorted([x['henID1'],x['henID2']])), axis=1)\n",
    "df_anypair.to_csv(os.path.join(path_extracted_data_visual_adap ,id_run+'_df_DistanceBetweenAnyDaysHenSim_2h-17h59LEVEL.csv'), sep=';', index=False)\n",
    "print(df_anypair.shape)\n",
    "display(df_anypair.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(df_anypair['chi2distance_ALL_15']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l1 = np.array(['ad','ad','er','ww'])\n",
    "l2 = np.array(['ad','ed','er','ww'])\n",
    "sum(l1==l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
