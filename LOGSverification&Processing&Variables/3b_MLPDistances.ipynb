{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic package\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import operator\n",
    "from dtaidistance import dtw\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle #to save/load list of selected hens\n",
    "\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#network\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "from UTILS import chi2_distance, correct_key\n",
    "import config_origins as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change the configuration file if not done yet!\n",
      "ALLDATA_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print('change the configuration file if not done yet!')\n",
    "path_extracted_data = config.path_extracted_data\n",
    "path_initial_data = config.path_initial_data\n",
    "id_run = config.id_run\n",
    "#id_run = 'chapter0_final_'\n",
    "#path_dataoutput = r'G:\\VPHI\\Welfare\\2- Research Projects\\OFHE2.OriginsE2\\DataOutput'\n",
    "#path_extracted_data = os.path.join(path_dataoutput,'TrackingSystem') \n",
    "#path_extracted_data = os.path.join(path_extracted_data, id_run)\n",
    "dico_night_hour = config.dico_night_hour\n",
    "dico_matching = config.dico_matching\n",
    "li_binmn = config.li_binmn\n",
    "penalty = config.penalty\n",
    "dico_window = config.dico_window\n",
    "path_extracted_data_SNA = config.path_extracted_data_SNA\n",
    "#create a director if not existing\n",
    "if not os.path.exists(path_extracted_data_SNA):\n",
    "    os.makedirs(path_extracted_data_SNA)\n",
    "print(id_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42752, 158)\n",
      "(42752, 158)\n",
      "(42752, 158)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HenID</th>\n",
       "      <th>level</th>\n",
       "      <th>duration_1_Zone</th>\n",
       "      <th>duration_2_Zone</th>\n",
       "      <th>duration_3_Zone</th>\n",
       "      <th>duration_4_Zone</th>\n",
       "      <th>duration_5_Zone</th>\n",
       "      <th>verification_daily_total_duration</th>\n",
       "      <th>verification_daily_total_nbr_hour</th>\n",
       "      <th>dur_values</th>\n",
       "      <th>...</th>\n",
       "      <th>nbr_transition_at_h21</th>\n",
       "      <th>nbr_transition_at_h22</th>\n",
       "      <th>nbr_transition_at_h23</th>\n",
       "      <th>nbr_transition_next1hafterlightoff</th>\n",
       "      <th>nbr_h_per_day</th>\n",
       "      <th>correct_amount_of_hour</th>\n",
       "      <th>DOA</th>\n",
       "      <th>WOA</th>\n",
       "      <th>DIB</th>\n",
       "      <th>WIB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hen_130</td>\n",
       "      <td>2020-10-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 32400.0]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>124</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hen_130</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>31295.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[0.0, 0.0, 780.0, 325.0, 31295.0]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>125</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hen_130</td>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>31815.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[0.0, 206.0, 249.0, 130.0, 31815.0]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>126</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HenID      level  duration_1_Zone  duration_2_Zone  duration_3_Zone  \\\n",
       "0  hen_130 2020-10-05              NaN              0.0              0.0   \n",
       "1  hen_130 2020-10-06              NaN              0.0            780.0   \n",
       "2  hen_130 2020-10-07              NaN            206.0            249.0   \n",
       "\n",
       "   duration_4_Zone  duration_5_Zone  verification_daily_total_duration  \\\n",
       "0              0.0          32400.0                            32400.0   \n",
       "1            325.0          31295.0                            32400.0   \n",
       "2            130.0          31815.0                            32400.0   \n",
       "\n",
       "   verification_daily_total_nbr_hour                           dur_values  \\\n",
       "0                                9.0        [0.0, 0.0, 0.0, 0.0, 32400.0]   \n",
       "1                                9.0    [0.0, 0.0, 780.0, 325.0, 31295.0]   \n",
       "2                                9.0  [0.0, 206.0, 249.0, 130.0, 31815.0]   \n",
       "\n",
       "   ... nbr_transition_at_h21  nbr_transition_at_h22  nbr_transition_at_h23  \\\n",
       "0  ...                   0.0                    0.0                    0.0   \n",
       "1  ...                   0.0                    0.0                    0.0   \n",
       "2  ...                   0.0                    0.0                    0.0   \n",
       "\n",
       "   nbr_transition_next1hafterlightoff  nbr_h_per_day  correct_amount_of_hour  \\\n",
       "0                                 0.0              9                    True   \n",
       "1                                 0.0              9                    True   \n",
       "2                                 0.0              9                    True   \n",
       "\n",
       "   DOA  WOA DIB WIB  \n",
       "0  124   18   6   1  \n",
       "1  125   18   7   1  \n",
       "2  126   18   8   2  \n",
       "\n",
       "[3 rows x 158 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#daily variables (one row per (henID, date))\n",
    "#daily_ALL_Variable_Tranformed\n",
    "df_daily = pd.read_csv(os.path.join(path_extracted_data, id_run+'_daily_ALL_variables.csv'), sep=';',\n",
    "                     parse_dates=['level'], dayfirst=True) \n",
    "#dico of hen as keys and pen as values\n",
    "dico_h_p = dict(zip(df_daily['HenID'].tolist(), df_daily['PenID'].tolist()))\n",
    "print(df_daily.shape)\n",
    "df_daily.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance across pairs of MLPS from same day with %time in a same zone- for daily SNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#download the MLPs vectors during day only! as we compare on same day, we can restrict to the exact daily calendar!\n",
    "dico_pen_level_h = pickle.load(open(os.path.join(path_extracted_data, \n",
    "                                                 id_run+'dico_pen_level_h_allzoneidseclevel_DAILYLEVEL.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lets compute every day a weighted network per zone with nodes=hens, link=DTW_zone_X. For this we need to compute mvt distances between hens on same day.\n",
    "same as the one for the ranging study of Mike (without random pens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#dico of hen as keys and pen as values\n",
    "dico_h_p = dict(zip(df_daily['HenID'].tolist(), df_daily['PenID'].tolist()))\n",
    "#df_ = df_daily.groupby('PenID')['HenID'].agg(lambda x: list(set(x))).reset_index()\n",
    "#dico_penid_lihen = dict(zip(df_['PenID'].tolist(), df_['HenID'].tolist()))\n",
    "#dico_cla_cla = {'EPI':'other', 'MEXP':'MEXP', 'LEXP':'LEXP', 'NewAfterEpi':'other', 'LEXPLOST':'other', 'MEXPLOST':'other'}\n",
    "#df_daily['CLASS'] = df_daily['CLASS'].map(lambda x: dico_cla_cla[x])\n",
    "#dico_h_class = dict(zip(df_daily['HenID'].tolist(), df_daily['CLASS'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3625200.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max size\n",
    "(160*159/2)*285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "  1%|â–Œ                                                                             | 2/285 [25:43<60:41:14, 771.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-89f7cf3dfaa9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 li_df.append({'HenID1':h1,'HenID2':h2,'date':d,\n\u001b[0;32m     16\u001b[0m                               \u001b[1;34m'total_nbr_sec_involved'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'nbr_sec_CommunZone'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                               'PenID1':dico_h_p[h1], 'PenID2':dico_h_p[h2]})\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#start recording the time it last\n",
    "START_TIME = time.perf_counter()\n",
    "li_df = []\n",
    "li_date = sorted(set(df_daily['level'].tolist()))\n",
    "for d in tqdm.tqdm(sorted(list(li_date))):\n",
    "    #list of hens available that day (i.e. functioning tags)\n",
    "    li_hen_d = df_daily[df_daily['level']==d]['HenID'].unique()\n",
    "    #we use symmetric measures, so we only do each combination once\n",
    "    for i,h1 in enumerate(li_hen_d):\n",
    "        #lets compute for hen h1 its similarity with all other birds\n",
    "        for h2 in li_hen_d[i+1:]:\n",
    "            l1 = np.array(dico_pen_level_h[dico_h_p[h1]][d][h1])\n",
    "            l2 = np.array(dico_pen_level_h[dico_h_p[h2]][d][h2])\n",
    "            if len(l1)==len(l2):\n",
    "                li_df.append({'HenID1':h1,'HenID2':h2,'date':d,\n",
    "                              'total_nbr_sec_involved':len(l1),'nbr_sec_CommunZone':sum(l1==l2),\n",
    "                              'PenID1':dico_h_p[h1], 'PenID2':dico_h_p[h2]})\n",
    "            else: \n",
    "                print(e)\n",
    "                print(h1, h2)\n",
    "                print(d)\n",
    "END_TIME = time.perf_counter()\n",
    "print (\"Total running time: %.2f mn\" %((END_TIME-START_TIME)/60))\n",
    "df_prox = pd.DataFrame(li_df)\n",
    "df_prox['perc_time_communZone'] = df_prox.apply(lambda x: x['nbr_sec_CommunZone']/x['total_nbr_sec_involved']*100, axis=1)\n",
    "df_prox['hen_pair'] = df_prox.apply(lambda x: '-'.join(sorted([x['HenID1'],x['HenID2']])), axis=1)\n",
    "df_prox['pen_pair'] = df_prox.apply(lambda x: '-'.join(sorted([x['PenID1'],x['PenID2']])), axis=1)\n",
    "df_prox.to_csv(os.path.join(path_extracted_data, id_run+'df_proximity_perctime.csv'), index=False, sep=';')\n",
    "print(df_prox.shape)\n",
    "df_prox.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance across pairs of MLPS from same day with Chi2&DTW- for daily SNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#compute the CHI2DISTANCE & DTW for each bird with all other birds\n",
    "#start recording the time it last\n",
    "START_TIME = time.perf_counter()\n",
    "li_df = []\n",
    "#df.sort_values(['Timestamp'], inplace=True)\n",
    "#take (subset of) dates and hens that are available (i.e. for which we comptue the variable on)\n",
    "#df_daily_h = df_daily[(df_daily['DOA']>=150)&(df_daily['DOA']<=380)].copy()\n",
    "li_date = set(df_daily['level'].tolist())\n",
    "li_zone = ['1_Zone', '2_Zone', '3_Zone', '4_Zone', '5_Zone']\n",
    "for d in tqdm.tqdm(sorted(list(li_date))):\n",
    "    li_hen_d = df_daily[df_daily['level']==d]['HenID'].unique()\n",
    "    #we use symmetric measures, so we only do each combination once\n",
    "    for i,h1 in enumerate(li_hen_d):\n",
    "\n",
    "        #lets compute for hen h1 its difference with all other birds\n",
    "        for h2 in li_hen_d[i+1:]:\n",
    "            \n",
    "            try:\n",
    "                ############ Overall aviary usage similarity across days\n",
    "                l1_chi2 = df_daily[(df_daily['HenID']==h1)&(df_daily['level']==d)]['dur_values_normalized'].values[0]\n",
    "                l2_chi2 = df_daily[(df_daily['HenID']==h2)&(df_daily['level']==d)]['dur_values_normalized'].values[0]\n",
    "                dico_ = {'date':d, 'henId':h1, 'henID2':h2, 'chi2distance':chi2_distance(l1_chi2,l2_chi2, remove_warning=True)}\n",
    "                #'dur_values_normalized_henId':l1_chi2,'dur_values_normalized_henID2':l2_chi2,\n",
    "\n",
    "                ############# Specifics' zones usage similarity across days\n",
    "                for nbr_binmn in li_binmn:\n",
    "                    #ALL zone together with chi2distance\n",
    "                    l1_chi2 = dico_pen_bin_level_h[dico_h_p[h1]][nbr_binmn][d][h1]\n",
    "                    l2_chi2 = dico_pen_bin_level_h[dico_h_p[h2]][nbr_binmn][d][h2]\n",
    "                    dico_['nbr_obs_chi2all_'+str(nbr_binmn)] = len(l1_chi2)\n",
    "                    li_chi2 = [chi2_distance(l1_chi2[i],l2_chi2[i], remove_warning=True) for i in range(0,len(l1_chi2))]\n",
    "                    dico_['li_chi2_'+str(nbr_binmn)] = li_chi2\n",
    "                    dico_['chi2distance_ALL_'+str(nbr_binmn)] = np.mean(li_chi2)\n",
    "\n",
    "                    #per zone with DTW\n",
    "                    for ZONE in li_zone:\n",
    "                        dtw_value = np.nan\n",
    "                        try:\n",
    "                            l1_dtw = np.array(dico_pen_bin_zone_level_h[dico_h_p[h1]][nbr_binmn][ZONE][d][h1], dtype=np.double)\n",
    "                            l2_dtw = np.array(dico_pen_bin_zone_level_h[dico_h_p[h2]][nbr_binmn][ZONE][d][h2], dtype=np.double)\n",
    "                            #compute distance measure\n",
    "                            #psi: #of start and end points of a sequence can be ignored if this would lead to a lower distance\n",
    "                            dtw_value, paths = dtw.warping_paths(l1_dtw, l2_dtw, window=dico_window[nbr_binmn], psi=0, penalty=penalty)\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            sys.exit()\n",
    "                            pass #dont do anything\n",
    "                        dico_['DTW_'+str(nbr_binmn)+'_'+ZONE] = round(dtw_value,1)  \n",
    "                        dico_['nbr_obs_l1_'+str(nbr_binmn)+'_'+ZONE] = len(l1_dtw)\n",
    "                        dico_['nbr_obs_l2_'+str(nbr_binmn)+'_'+ZONE] = len(l2_dtw)\n",
    "                li_df.append(dico_)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(h1, h2)\n",
    "                print(d)\n",
    "                \n",
    "END_TIME = time.perf_counter()\n",
    "print (\"Total running time: %.2f mn\" %((END_TIME-START_TIME)/60))\n",
    "#turn inter and intra distances into a dataframe                \n",
    "df_inter = pd.DataFrame(li_df)\n",
    "#df_inter = df_inter[~df_inter['chi2distance'].isnull()]\n",
    "df_inter['hen_pair'] = df_inter.apply(lambda x: '-'.join(sorted([x['henId'],x['henID2']])), axis=1)\n",
    "df_inter['DTW_30_all'] = df_inter[['DTW_30_1_Zone', 'DTW_30_2_Zone', 'DTW_30_3_Zone','DTW_30_4_Zone','DTW_30_5_Zone']].sum(axis=1)\n",
    "li_col = list(df_inter.columns)\n",
    "li_remove = [i for i in li_col if i.startswith('li_chi2_')]\n",
    "print(li_remove)\n",
    "li_keep = [i for i in li_col if i not in li_remove]\n",
    "#df_inter.iloc[0:1000].to_csv(os.path.join(path_extracted_data_visual_adap, id_run+'_df_DistanceBetweenHenSim_4verification.csv'), sep=';', index=False)\n",
    "#df_inter.filter(li_keep).to_csv(os.path.join(path_extracted_data_visual_adap ,id_run+'_df_DistanceBetweenHenSim.csv'), sep=';', index=False)\n",
    "df_inter.to_csv(os.path.join(path_extracted_data ,id_run+'_df_DistanceBetweenHenSim_DAILYLEVEL.csv'), sep=';', index=False)\n",
    "print(df_inter.shape)\n",
    "display(df_inter.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_inter.shape)\n",
    "display(df_inter.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance across any pairs of MLPS (i.e. any days) - for daily clusterID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#download MLPs vectors\n",
    "#For now we restrict to 2h-17h in order to always compare same length TS as we will compare across days, the lenght might differ\n",
    "dico_pen_bin_level_h = pickle.load(open(os.path.join(path_extracted_data_visual_adap, \n",
    "                                                     id_run+'dico_pen_bin_level_h_2h-17h59LEVEL.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#compute mvt distances between any two MLPs on any day from any pen\n",
    "dico_hendate_mvt = {}\n",
    "for p,dico_bin_level_h in dico_pen_bin_level_h.items():\n",
    "    for level, dico_hen_mvt in dico_bin_level_h[15].items():\n",
    "        if level!=dt.datetime(2020,9,29):\n",
    "            for henID,limvt in dico_hen_mvt.items():\n",
    "                dico_hendate_mvt[henID+'/'+str(level).split(' ')[0]] = limvt\n",
    "dico_hendate_mvt = OrderedDict(dico_hendate_mvt)\n",
    "li_hendate = list(dico_hendate_mvt.keys())\n",
    "li_mvt = list(dico_hendate_mvt.values())\n",
    "len(dico_hendate_mvt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "li_df = []\n",
    "for i in tqdm.tqdm(range(0,len(li_mvt))):\n",
    "    li_mvt1 = li_mvt[i]\n",
    "    for j in range(i+1,len(li_mvt)):\n",
    "        dico_ = {}\n",
    "        li_mvt2 = li_mvt[j]\n",
    "        li_chi2 = [chi2_distance(li_mvt1[i],li_mvt2[i]) for i in range(0,len(li_mvt1))]\n",
    "        dico_['chi2distance_ALL_15'] = np.mean(li_chi2)\n",
    "        dico_['h1'] = li_hendate[i]\n",
    "        dico_['h2'] = li_hendate[j]\n",
    "        li_df.append(dico_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_anypair = pd.DataFrame(li_df)\n",
    "display(df_anypair[df_anypair['chi2distance_ALL_15'].isnull()])\n",
    "df_anypair['henID1'] = df_anypair['h1'].map(lambda x: x.split('/')[0])\n",
    "df_anypair['henID2'] = df_anypair['h2'].map(lambda x: x.split('/')[0])\n",
    "df_anypair['level1'] = df_anypair['h1'].map(lambda x: dt.datetime.strptime(x.split('/')[1], '%Y-%m-%d'))\n",
    "df_anypair['level2'] = df_anypair['h2'].map(lambda x: dt.datetime.strptime(x.split('/')[1], '%Y-%m-%d'))\n",
    "#df_interintra['weeks_in_laying_barn'] = df_interintra['DOA'].map(lambda x: int((x-119)/7)+1)\n",
    "#df_interintra['hen_pair'] = df_interintra.apply(lambda x: '-'.join(sorted([x['henID1'],x['henID2']])), axis=1)\n",
    "df_anypair.to_csv(os.path.join(path_extracted_data_visual_adap ,id_run+'_df_DistanceBetweenAnyDaysHenSim_2h-17h59LEVEL.csv'), sep=';', index=False)\n",
    "print(df_anypair.shape)\n",
    "display(df_anypair.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(df_anypair['chi2distance_ALL_15']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l1 = np.array(['ad','ad','er','ww'])\n",
    "l2 = np.array(['ad','ed','er','ww'])\n",
    "sum(l1==l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
