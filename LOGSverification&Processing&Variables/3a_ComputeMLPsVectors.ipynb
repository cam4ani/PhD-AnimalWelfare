{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic package\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import operator\n",
    "from scipy import stats\n",
    "from numpy import inf\n",
    "import networkx as nx\n",
    "from dtaidistance import dtw\n",
    "import random\n",
    "\n",
    "#save and load dictionaries/lists\n",
    "import pickle\n",
    "\n",
    "#scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "#PCA\n",
    "from sklearn import decomposition\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import KMeans #only numerical var\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import kmodes\n",
    "from kmodes.kmodes import KModes #with categorical var as well\n",
    "\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "from UTILS import kmeans_clustering, ZoneVariable, time_series_henColumn_tsRow, FB_daily, corr_from_dep2feature,\\\n",
    "corr_from_feature2feature, correlationGraph, ZoneVariable, DataRepresentation1, sampen, chi2_distance, is_day, correct_key\n",
    "import config_origins as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change the configuration file if not done yet!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print('change the configuration file if not done yet!')\n",
    "path_extracted_data = config.path_extracted_data\n",
    "id_run = config.id_run\n",
    "dico_pen_tr = config.dico_pen_tr\n",
    "li_binmn = config.li_binmn\n",
    "penalty = config.penalty\n",
    "dico_window = config.dico_window\n",
    "birth_date = config.birth_date\n",
    "dico_night_hour = config.dico_night_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1c32ecbc63d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#download the cleaned-movement data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m df = pd.read_csv(os.path.join(path_extracted_data, id_run+'_CLEANEDDATA.csv'), sep=';',\n\u001b[0m\u001b[0;32m      3\u001b[0m                  parse_dates=['Timestamp', 'date'], dayfirst=True) \n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hour'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msecond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2145\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2146\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2147\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \"\"\"\n\u001b[0;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#download the cleaned-movement data\n",
    "df = pd.read_csv(os.path.join(path_extracted_data, id_run+'_CLEANEDDATA.csv'), sep=';',\n",
    "                 parse_dates=['Timestamp', 'date'], dayfirst=True) \n",
    "df['hour'] = df['Timestamp'].map(lambda x: x.hour)\n",
    "df['time'] = df['Timestamp'].map(lambda x: dt.datetime.time(x-dt.timedelta(seconds=x.second)))\n",
    "df.drop('duration', axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute MLPS vectors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Attention: note that here we use all mlps, we should be removing the one we dont want for each analysis afterwards. Indeed if no mvt one days: is it because unwanted days or because no mvt provided by the animal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on the day only - list of zones /day/animal at second level"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "same as for Mikes ranging study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "\r",
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this time series there is 28 hens\n",
      "The initial starting date in over all is: 2020-09-29 09:20:21, and the ending date will be: 2021-07-25 18:46:46\n",
      "But note that birds may have different ending and starting date which should be taken into account when computing variables\n",
      "and after ending the last day at midnight : 2020-09-29 09:20:21, and the ending date will be: 2021-07-25 23:59:59\n",
      "Total running time: 6.86 mn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████▏                                                                      | 1/8 [12:22<1:26:35, 742.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this time series there is 28 hens\n",
      "The initial starting date in over all is: 2020-09-29 09:08:12, and the ending date will be: 2021-07-25 22:58:37\n",
      "But note that birds may have different ending and starting date which should be taken into account when computing variables\n",
      "and after ending the last day at midnight : 2020-09-29 09:08:12, and the ending date will be: 2021-07-25 23:59:59\n",
      "Total running time: 5.50 mn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████▎                                                            | 2/8 [22:35<1:10:21, 703.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this time series there is 31 hens\n",
      "The initial starting date in over all is: 2020-09-29 09:07:00, and the ending date will be: 2021-07-25 17:08:42\n",
      "But note that birds may have different ending and starting date which should be taken into account when computing variables\n",
      "and after ending the last day at midnight : 2020-09-29 09:07:00, and the ending date will be: 2021-07-25 23:59:59\n",
      "Total running time: 5.52 mn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▏                                                   | 3/8 [33:08<56:52, 682.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this time series there is 28 hens\n",
      "The initial starting date in over all is: 2020-09-29 09:32:45, and the ending date will be: 2021-07-25 17:30:26\n",
      "But note that birds may have different ending and starting date which should be taken into account when computing variables\n",
      "and after ending the last day at midnight : 2020-09-29 09:32:45, and the ending date will be: 2021-07-25 23:59:59\n",
      "Total running time: 5.83 mn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 4/8 [44:05<44:58, 674.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this time series there is 27 hens\n",
      "The initial starting date in over all is: 2020-09-29 09:33:43, and the ending date will be: 2021-07-25 23:50:25\n",
      "But note that birds may have different ending and starting date which should be taken into account when computing variables\n",
      "and after ending the last day at midnight : 2020-09-29 09:33:43, and the ending date will be: 2021-07-25 23:59:59\n",
      "Total running time: 6.58 mn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████▉                               | 5/8 [56:00<34:20, 686.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this time series there is 29 hens\n",
      "The initial starting date in over all is: 2020-09-29 09:33:48, and the ending date will be: 2021-07-25 20:05:09\n",
      "But note that birds may have different ending and starting date which should be taken into account when computing variables\n",
      "and after ending the last day at midnight : 2020-09-29 09:33:48, and the ending date will be: 2021-07-25 23:59:59\n",
      "Total running time: 7.52 mn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|████████████████████████████████████████████████████████████▊                    | 6/8 [1:08:45<23:40, 710.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this time series there is 29 hens\n",
      "The initial starting date in over all is: 2020-09-29 09:52:12, and the ending date will be: 2021-07-25 23:56:18\n",
      "But note that birds may have different ending and starting date which should be taken into account when computing variables\n",
      "and after ending the last day at midnight : 2020-09-29 09:52:12, and the ending date will be: 2021-07-25 23:59:59\n",
      "Total running time: 8.53 mn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|██████████████████████████████████████████████████████████████████████▉          | 7/8 [1:22:43<12:28, 748.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this time series there is 28 hens\n",
      "The initial starting date in over all is: 2020-09-29 09:36:51, and the ending date will be: 2021-07-25 23:53:11\n",
      "But note that birds may have different ending and starting date which should be taken into account when computing variables\n",
      "and after ending the last day at midnight : 2020-09-29 09:36:51, and the ending date will be: 2021-07-25 23:59:59\n",
      "Total running time: 10.44 mn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 8/8 [1:39:04<00:00, 743.10s/it]\n"
     ]
    }
   ],
   "source": [
    "#note that we will have more entries than needed, as the disturbances days are not removed in the raw-cleaned movements\n",
    "#dataframe.\n",
    "dico_pen_level_h = {}\n",
    "#we have to loop across pens, due to memory issue\n",
    "for p, df_pen in tqdm.tqdm(df.groupby('PenID')):\n",
    "        \n",
    "    #update results\n",
    "    dico_pen_level_h[p] = {}\n",
    "    \n",
    "    #compute time series\n",
    "    df_ts = time_series_henColumn_tsRow(df_pen, config, col_ts='Zone', ts_with_all_hen_value=False, save=False, \n",
    "                                        hen_time_series=False)\n",
    "    \n",
    "    #restrict to the daylight ONLY\n",
    "    df_ts['is_day'] = df_ts['Timestamp'].map(lambda x: is_day(x, config.dico_night_hour))\n",
    "    df_ts = df_ts[df_ts['is_day']]\n",
    "    \n",
    "    #ensure df is sorted by timestamp\n",
    "    df_ts = df_ts.sort_values('Timestamp',ascending=True)\n",
    "\n",
    "    #list of all hen present in this pen\n",
    "    li_hen = [v for v in df_ts.columns if 'hen_' in v]\n",
    "\n",
    "    ################# create one list per animal #################\n",
    "    #groupby date to have a list of zones per day (rows) for the hens (columns)\n",
    "    df_ts = df_ts.groupby('date')[li_hen].agg(lambda x: list(x)).reset_index()\n",
    "    #melt to have one row per (day, hens) to avoid looping to create the dictionary\n",
    "    df_ts_ = pd.melt(df_ts, id_vars=['date'], value_vars=li_hen)\n",
    "    for d, df__ in df_ts_.groupby(['date']):\n",
    "        #update results: value column is the list of zone\n",
    "        dico_pen_level_h[p][d] = dict(zip(df__['HenID'].tolist(), df__['value'].tolist()))   \n",
    "    \n",
    "#save dictionaries\n",
    "pickle.dump(dico_pen_level_h, open(os.path.join(path_extracted_data, \n",
    "                                                id_run+'dico_pen_level_h_allzoneidseclevel_DAILYLEVEL.pkl'), 'wb'), \n",
    "            pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>HenID</th>\n",
       "      <th>date</th>\n",
       "      <th>hen_1</th>\n",
       "      <th>hen_105</th>\n",
       "      <th>hen_106</th>\n",
       "      <th>hen_110</th>\n",
       "      <th>hen_113</th>\n",
       "      <th>hen_114</th>\n",
       "      <th>hen_116</th>\n",
       "      <th>hen_120</th>\n",
       "      <th>hen_164</th>\n",
       "      <th>...</th>\n",
       "      <th>hen_230</th>\n",
       "      <th>hen_233</th>\n",
       "      <th>hen_24</th>\n",
       "      <th>hen_29</th>\n",
       "      <th>hen_3</th>\n",
       "      <th>hen_31</th>\n",
       "      <th>hen_5</th>\n",
       "      <th>hen_90</th>\n",
       "      <th>hen_93</th>\n",
       "      <th>hen_98</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2021-07-24</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[2_Zone, 2_Zone, 2_Zone, 2_Zone, 2_Zone, 2_Zon...</td>\n",
       "      <td>[4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...</td>\n",
       "      <td>[4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>2021-07-25</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[2_Zone, 2_Zone, 2_Zone, 2_Zone, 2_Zone, 2_Zon...</td>\n",
       "      <td>[4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...</td>\n",
       "      <td>[4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...</td>\n",
       "      <td>[4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "HenID       date                                              hen_1  \\\n",
       "298   2021-07-24  [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "299   2021-07-25  [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "\n",
       "HenID                                            hen_105  \\\n",
       "298    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "299    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "\n",
       "HenID                                            hen_106  \\\n",
       "298    [3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zon...   \n",
       "299    [3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zon...   \n",
       "\n",
       "HenID                                            hen_110  \\\n",
       "298    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "299    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "\n",
       "HenID                                            hen_113  \\\n",
       "298    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "299    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "\n",
       "HenID                                            hen_114  \\\n",
       "298    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "299    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "\n",
       "HenID                                            hen_116  \\\n",
       "298    [3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zon...   \n",
       "299    [3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zone, 3_Zon...   \n",
       "\n",
       "HenID                                            hen_120  \\\n",
       "298    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "299    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "\n",
       "HenID                                            hen_164  ...  \\\n",
       "298    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...  ...   \n",
       "299    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...  ...   \n",
       "\n",
       "HenID                                            hen_230  \\\n",
       "298    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "299    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "\n",
       "HenID                                            hen_233  \\\n",
       "298    [2_Zone, 2_Zone, 2_Zone, 2_Zone, 2_Zone, 2_Zon...   \n",
       "299    [2_Zone, 2_Zone, 2_Zone, 2_Zone, 2_Zone, 2_Zon...   \n",
       "\n",
       "HenID                                             hen_24  \\\n",
       "298    [4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...   \n",
       "299    [4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...   \n",
       "\n",
       "HenID                                             hen_29  \\\n",
       "298    [4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...   \n",
       "299    [4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...   \n",
       "\n",
       "HenID                                              hen_3  \\\n",
       "298    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "299    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "\n",
       "HenID                                             hen_31  \\\n",
       "298    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "299    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "\n",
       "HenID                                              hen_5  \\\n",
       "298    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "299    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "\n",
       "HenID                                             hen_90  \\\n",
       "298    [4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...   \n",
       "299    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "\n",
       "HenID                                             hen_93  \\\n",
       "298    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "299    [5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zone, 5_Zon...   \n",
       "\n",
       "HenID                                             hen_98  \n",
       "298    [4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...  \n",
       "299    [4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>HenID</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8398</th>\n",
       "      <td>2021-07-24</td>\n",
       "      <td>hen_98</td>\n",
       "      <td>[4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8399</th>\n",
       "      <td>2021-07-25</td>\n",
       "      <td>hen_98</td>\n",
       "      <td>[4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   HenID                                              value\n",
       "8398 2021-07-24  hen_98  [4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon...\n",
       "8399 2021-07-25  hen_98  [4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zone, 4_Zon..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#small check\n",
    "display(df_ts.tail(2))\n",
    "display(df_ts_.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on the day - a bin-list of duration in each bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for efficiency purpose let's compute the bined time series first\n",
    "#note that we will have more entries than needed, as the distrubances days are not removed in the raw-cleaned movements\n",
    "#dataframe.\n",
    "dico_pen_bin_zone_level_h = {}\n",
    "dico_pen_bin_level_h = {}\n",
    "def duration_normalized_perZone(x):\n",
    "    c = Counter(x)\n",
    "    t = len(x)\n",
    "    return [c['1_Zone']/t, c['2_Zone']/t, c['3_Zone']/t, c['4_Zone']/t, c['5_Zone']/t]\n",
    "#small example\n",
    "#li = ['1_Zone','3_Zone','3_Zone','4_Zone','5_Zone','5_Zone','1_Zone']\n",
    "#duration_normalized_perZone(li)\n",
    "\n",
    "for p, df_pen in tqdm.tqdm(df.groupby('PenID')):\n",
    "    \n",
    "    #update results\n",
    "    dico_pen_bin_zone_level_h[p] = {}\n",
    "    dico_pen_bin_level_h[p] = {}\n",
    "    \n",
    "    #compute time series\n",
    "    df_ts = time_series_henColumn_tsRow(df_pen, config, col_ts='Zone', ts_with_all_hen_value=False, save=False, \n",
    "                                        hen_time_series=False)\n",
    "    \n",
    "    for nbr_binmn in tqdm.tqdm(li_binmn):\n",
    "        \n",
    "        #update results\n",
    "        dico_pen_bin_zone_level_h[p][nbr_binmn] = {}\n",
    "        dico_pen_bin_level_h[p][nbr_binmn] = {}\n",
    "                \n",
    "        #reduce to the interval we want\n",
    "        mi = min(df_ts['Timestamp'].tolist())\n",
    "        ma = max(df_ts['Timestamp'].tolist())\n",
    "        #extend the end to the end of the day in case it case the last day available fo the chicken\n",
    "        Daterange = pd.date_range(start = mi, end = ma, freq = str(nbr_binmn)+'MIN')    \n",
    "        df_date = pd.DataFrame({str(nbr_binmn)+'mn_timestamp':Daterange})\n",
    "        new_timestamp = str(nbr_binmn)+'mn_timestamp'\n",
    "        df_date[new_timestamp] = df_date[new_timestamp].map(lambda x: pd.to_datetime(x))\n",
    "        df_ts_ = pd.merge_asof(df_ts, df_date, left_on=['Timestamp'], right_on=[new_timestamp], direction='forward')\n",
    "        \n",
    "        #restrict to the day ONLY\n",
    "        df_ts_['is_day'] = df_ts_['Timestamp'].map(lambda x: is_day(x, config.dico_night_hour))\n",
    "        df_ts_ = df_ts_[df_ts_['is_day']]\n",
    "        \n",
    "        #groupby the interval that we want with the number of minutes in nestbox\n",
    "        li_hen = [v for v in df_ts.columns if 'hen_' in v]\n",
    "        \n",
    "        ################# overall mlp #################\n",
    "        df_sim = df_ts_.groupby(new_timestamp)[li_hen].agg(lambda x: duration_normalized_perZone(x)).reset_index()\n",
    "        df_sim['date'] = df_sim[new_timestamp].map(lambda x: dt.datetime(x.year,x.month,x.day))\n",
    "        #groupby date to have a list of zones per day (rows) for the hens (columns)\n",
    "        df_sim = df_sim.groupby('date')[li_hen].agg(lambda x: list(x)).reset_index()\n",
    "        #print(df_zone_sim.shape)\n",
    "        #display(df_zone_sim.head(3))\n",
    "\n",
    "        #melt to have one row per (day, hens) to avoid looping to create the dictionary\n",
    "        df_sim_ = pd.melt(df_sim, id_vars=['date'], value_vars=li_hen)\n",
    "        #variable column has the henIDs\n",
    "        #print(df_zone_sim_.shape)\n",
    "        #display(df_zone_sim_.head(3))\n",
    "        for d, df__ in df_sim_.groupby(['date']):\n",
    "            #update results\n",
    "            dico_pen_bin_level_h[p][nbr_binmn][d] = dict(zip(df__['variable'].tolist(), df__['value'].tolist()))   \n",
    "            \n",
    "            \n",
    "        ################# zone-ts over each zone #################                \n",
    "        for ZONE in df['Zone'].unique():\n",
    "            \n",
    "            #update results\n",
    "            dico_pen_bin_zone_level_h[p][nbr_binmn][ZONE] = {}\n",
    "                \n",
    "            df_zone_sim = df_ts_.groupby(new_timestamp)[li_hen].agg(lambda x: sum([i==ZONE for i in x])/60).reset_index()\n",
    "            df_zone_sim['date'] = df_zone_sim[new_timestamp].map(lambda x: dt.datetime(x.year,x.month,x.day))\n",
    "\n",
    "            #groupby date to have a list of zones per day (rows) for the hens (columns)\n",
    "            df_zone_sim = df_zone_sim.groupby('date')[li_hen].agg(lambda x: list(x)).reset_index()\n",
    "            #print(df_zone_sim.shape)\n",
    "            #display(df_zone_sim.head(3))\n",
    "            \n",
    "            #melt to have one row per (day, hens) to avoid looping to create the dictionary\n",
    "            df_zone_sim_ = pd.melt(df_zone_sim, id_vars=['date'], value_vars=li_hen)\n",
    "            #variable column has the henIDs\n",
    "            #print(df_zone_sim_.shape)\n",
    "            #display(df_zone_sim_.head(3))\n",
    "            for d, df__ in df_zone_sim_.groupby(['date']):\n",
    "                #update results\n",
    "                dico_pen_bin_zone_level_h[p][nbr_binmn][ZONE][d] = dict(zip(df__['variable'].tolist(), df__['value'].tolist()))\n",
    "#save dictionaries\n",
    "pickle.dump(dico_pen_bin_zone_level_h, open(os.path.join(path_extracted_data, \n",
    "                                                     id_run+'dico_pen_bin_zone_level_h_DAILYLEVEL.pkl'), 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(dico_pen_bin_level_h, open(os.path.join(path_extracted_data, \n",
    "                                                 id_run+'dico_pen_bin_level_h_DAILYLEVEL.pkl'), 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on [2h, 17h59] - a bin-list of duration in each bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for efficiency purpose let's compute the bined time series first\n",
    "#note that we will have more entries than needed, as the distrubances days are not removed in the raw-cleaned movements\n",
    "#dataframe.\n",
    "dico_pen_bin_zone_level_h = {}\n",
    "dico_pen_bin_level_h = {}\n",
    "def duration_normalized_perZone(x):\n",
    "    c = Counter(x)\n",
    "    t = len(x)\n",
    "    return [c['1_Zone']/t, c['2_Zone']/t, c['3_Zone']/t, c['4_Zone']/t, c['5_Zone']/t]\n",
    "#small example\n",
    "#li = ['1_Zone','3_Zone','3_Zone','4_Zone','5_Zone','5_Zone','1_Zone']\n",
    "#duration_normalized_perZone(li)\n",
    "\n",
    "for p, df_pen in tqdm.tqdm(df.groupby('PenID')):\n",
    "    \n",
    "    #update results\n",
    "    dico_pen_bin_zone_level_h[p] = {}\n",
    "    dico_pen_bin_level_h[p] = {}\n",
    "    \n",
    "    #compute time series\n",
    "    df_ts = time_series_henColumn_tsRow(df_pen, config, col_ts='Zone', ts_with_all_hen_value=False, save=False, \n",
    "                                        hen_time_series=False)\n",
    "    \n",
    "    for nbr_binmn in tqdm.tqdm(li_binmn):\n",
    "        \n",
    "        #update results\n",
    "        dico_pen_bin_zone_level_h[p][nbr_binmn] = {}\n",
    "        dico_pen_bin_level_h[p][nbr_binmn] = {}\n",
    "                \n",
    "        #reduce to the interval we want\n",
    "        mi = min(df_ts['Timestamp'].tolist())\n",
    "        ma = max(df_ts['Timestamp'].tolist())\n",
    "        #extend the end to the end of the day in case it case the last day available fo the chicken\n",
    "        Daterange = pd.date_range(start = mi, end = ma, freq = str(nbr_binmn)+'MIN')    \n",
    "        df_date = pd.DataFrame({str(nbr_binmn)+'mn_timestamp':Daterange})\n",
    "        new_timestamp = str(nbr_binmn)+'mn_timestamp'\n",
    "        df_date[new_timestamp] = df_date[new_timestamp].map(lambda x: pd.to_datetime(x))\n",
    "        df_ts_ = pd.merge_asof(df_ts, df_date, left_on=['Timestamp'], right_on=[new_timestamp], direction='forward')\n",
    "        \n",
    "        #restrict to 2h-17h59 ONLY\n",
    "        df_ts_['is_2h17'] = df_ts_['Timestamp'].map(lambda x: (x.hour>=2)&(x.hour<18))\n",
    "        df_ts_ = df_ts_[df_ts_['is_2h17']]\n",
    "        \n",
    "        #groupby the interval that we want with the number of minutes in nestbox\n",
    "        li_hen = [v for v in df_ts.columns if 'hen_' in v]\n",
    "        \n",
    "        ################# overall mlp #################\n",
    "        df_sim = df_ts_.groupby(new_timestamp)[li_hen].agg(lambda x: duration_normalized_perZone(x)).reset_index()\n",
    "        df_sim['date'] = df_sim[new_timestamp].map(lambda x: dt.datetime(x.year,x.month,x.day))\n",
    "        #groupby date to have a list of zones per day (rows) for the hens (columns)\n",
    "        df_sim = df_sim.groupby('date')[li_hen].agg(lambda x: list(x)).reset_index()\n",
    "        #print(df_zone_sim.shape)\n",
    "        #display(df_zone_sim.head(3))\n",
    "\n",
    "        #melt to have one row per (day, hens) to avoid looping to create the dictionary\n",
    "        df_sim_ = pd.melt(df_sim, id_vars=['date'], value_vars=li_hen)\n",
    "        #variable column has the henIDs\n",
    "        #print(df_zone_sim_.shape)\n",
    "        #display(df_zone_sim_.head(3))\n",
    "        for d, df__ in df_sim_.groupby(['date']):\n",
    "            #update results\n",
    "            dico_pen_bin_level_h[p][nbr_binmn][d] = dict(zip(df__['variable'].tolist(), df__['value'].tolist()))   \n",
    "            \n",
    "            \n",
    "        ################# zone-ts over each zone #################                \n",
    "        for ZONE in df['Zone'].unique():\n",
    "            \n",
    "            #update results\n",
    "            dico_pen_bin_zone_level_h[p][nbr_binmn][ZONE] = {}\n",
    "                \n",
    "            df_zone_sim = df_ts_.groupby(new_timestamp)[li_hen].agg(lambda x: sum([i==ZONE for i in x])/60).reset_index()\n",
    "            df_zone_sim['date'] = df_zone_sim[new_timestamp].map(lambda x: dt.datetime(x.year,x.month,x.day))\n",
    "\n",
    "            #groupby date to have a list of zones per day (rows) for the hens (columns)\n",
    "            df_zone_sim = df_zone_sim.groupby('date')[li_hen].agg(lambda x: list(x)).reset_index()\n",
    "            #print(df_zone_sim.shape)\n",
    "            #display(df_zone_sim.head(3))\n",
    "            \n",
    "            #melt to have one row per (day, hens) to avoid looping to create the dictionary\n",
    "            df_zone_sim_ = pd.melt(df_zone_sim, id_vars=['date'], value_vars=li_hen)\n",
    "            #variable column has the henIDs\n",
    "            #print(df_zone_sim_.shape)\n",
    "            #display(df_zone_sim_.head(3))\n",
    "            for d, df__ in df_zone_sim_.groupby(['date']):\n",
    "                #update results\n",
    "                dico_pen_bin_zone_level_h[p][nbr_binmn][ZONE][d] = dict(zip(df__['variable'].tolist(), df__['value'].tolist()))\n",
    "#save dictionaries\n",
    "pickle.dump(dico_pen_bin_zone_level_h, open(os.path.join(path_extracted_data, \n",
    "                                                     id_run+'dico_pen_bin_zone_level_h_2h-17h59LEVEL.pkl'), 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(dico_pen_bin_level_h, open(os.path.join(path_extracted_data, \n",
    "                                                 id_run+'dico_pen_bin_level_h_2h-17h59LEVEL.pkl'), 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the entire 24h period for generality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#for efficiency purpose let's compute the bined time series first\n",
    "#note that we will have more entries than needed, as the distrubances days are not removed in the raw-cleaned movements\n",
    "#dataframe.\n",
    "dico_pen_bin_zone_level_h = {}\n",
    "dico_pen_bin_level_h = {}\n",
    "def duration_normalized_perZone(x):\n",
    "    c = Counter(x)\n",
    "    t = len(x)\n",
    "    return [c['1_Zone']/t, c['2_Zone']/t, c['3_Zone']/t, c['4_Zone']/t, c['5_Zone']/t]\n",
    "#small example\n",
    "#li = ['1_Zone','3_Zone','3_Zone','4_Zone','5_Zone','5_Zone','1_Zone']\n",
    "#duration_normalized_perZone(li)\n",
    "\n",
    "for p, df_pen in tqdm.tqdm(df.groupby('PenID')):\n",
    "    \n",
    "    #update results\n",
    "    dico_pen_bin_zone_level_h[p] = {}\n",
    "    dico_pen_bin_level_h[p] = {}\n",
    "    \n",
    "    #compute time series\n",
    "    df_ts = time_series_henColumn_tsRow(df_pen, config, col_ts='Zone', ts_with_all_hen_value=False, save=False, \n",
    "                                        hen_time_series=False)\n",
    "    \n",
    "    for nbr_binmn in tqdm.tqdm(li_binmn):\n",
    "        \n",
    "        #update results\n",
    "        dico_pen_bin_zone_level_h[p][nbr_binmn] = {}\n",
    "        dico_pen_bin_level_h[p][nbr_binmn] = {}\n",
    "                \n",
    "        #reduce to the interval we want\n",
    "        mi = min(df_ts['Timestamp'].tolist())\n",
    "        ma = max(df_ts['Timestamp'].tolist())\n",
    "        #extend the end to the end of the day in case it case the last day available fo the chicken\n",
    "        Daterange = pd.date_range(start = mi, end = ma, freq = str(nbr_binmn)+'MIN')    \n",
    "        df_date = pd.DataFrame({str(nbr_binmn)+'mn_timestamp':Daterange})\n",
    "        new_timestamp = str(nbr_binmn)+'mn_timestamp'\n",
    "        df_date[new_timestamp] = df_date[new_timestamp].map(lambda x: pd.to_datetime(x))\n",
    "        df_ts_ = pd.merge_asof(df_ts, df_date, left_on=['Timestamp'], right_on=[new_timestamp], direction='forward')\n",
    "        #groupby the interval that we want with the number of minutes in nestbox\n",
    "        li_hen = [v for v in df_ts.columns if 'hen_' in v]\n",
    "        \n",
    "        ################# overall mlp #################\n",
    "        df_sim = df_ts_.groupby(new_timestamp)[li_hen].agg(lambda x: duration_normalized_perZone(x)).reset_index()\n",
    "        df_sim['date'] = df_sim[new_timestamp].map(lambda x: dt.datetime(x.year,x.month,x.day))\n",
    "        #groupby date to have a list of zones per day (rows) for the hens (columns)\n",
    "        df_sim = df_sim.groupby('date')[li_hen].agg(lambda x: list(x)).reset_index()\n",
    "        #print(df_zone_sim.shape)\n",
    "        #display(df_zone_sim.head(3))\n",
    "\n",
    "        #melt to have one row per (day, hens) to avoid looping to create the dictionary\n",
    "        df_sim_ = pd.melt(df_sim, id_vars=['date'], value_vars=li_hen)\n",
    "        #variable column has the henIDs\n",
    "        #print(df_zone_sim_.shape)\n",
    "        #display(df_zone_sim_.head(3))\n",
    "        for d, df__ in df_sim_.groupby(['date']):\n",
    "            #update results\n",
    "            dico_pen_bin_level_h[p][nbr_binmn][d] = dict(zip(df__['variable'].tolist(), df__['value'].tolist()))   \n",
    "            \n",
    "            \n",
    "        ################# zone-ts over each zone #################                \n",
    "        for ZONE in df['Zone'].unique():\n",
    "            \n",
    "            #update results\n",
    "            dico_pen_bin_zone_level_h[p][nbr_binmn][ZONE] = {}\n",
    "                \n",
    "            df_zone_sim = df_ts_.groupby(new_timestamp)[li_hen].agg(lambda x: sum([i==ZONE for i in x])/60).reset_index()\n",
    "            df_zone_sim['date'] = df_zone_sim[new_timestamp].map(lambda x: dt.datetime(x.year,x.month,x.day))\n",
    "\n",
    "            #groupby date to have a list of zones per day (rows) for the hens (columns)\n",
    "            df_zone_sim = df_zone_sim.groupby('date')[li_hen].agg(lambda x: list(x)).reset_index()\n",
    "            #print(df_zone_sim.shape)\n",
    "            #display(df_zone_sim.head(3))\n",
    "            \n",
    "            #melt to have one row per (day, hens) to avoid looping to create the dictionary\n",
    "            df_zone_sim_ = pd.melt(df_zone_sim, id_vars=['date'], value_vars=li_hen)\n",
    "            #variable column has the henIDs\n",
    "            #print(df_zone_sim_.shape)\n",
    "            #display(df_zone_sim_.head(3))\n",
    "            for d, df__ in df_zone_sim_.groupby(['date']):\n",
    "                #update results\n",
    "                dico_pen_bin_zone_level_h[p][nbr_binmn][ZONE][d] = dict(zip(df__['variable'].tolist(), df__['value'].tolist()))\n",
    "#save dictionaries\n",
    "pickle.dump(dico_pen_bin_zone_level_h, open(os.path.join(path_extracted_data, \n",
    "                                                     id_run+'dico_pen_bin_zone_level_h.pkl'), 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(dico_pen_bin_level_h, open(os.path.join(path_extracted_data, \n",
    "                                                 id_run+'dico_pen_bin_level_h.pkl'), 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#download the two dictionaries\n",
    "dico_pen_bin_zone_level_h = pickle.load(open(os.path.join(path_extracted_data, \n",
    "                                                 id_run+'dico_pen_bin_zone_level_h.pkl'), 'rb'))\n",
    "dico_pen_bin_level_h = pickle.load(open(os.path.join(path_extracted_data, \n",
    "                                                     id_run+'dico_pen_bin_level_h.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
