{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic package\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import operator\n",
    "from scipy import stats\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import random\n",
    "import statistics as st #for the mode\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "#PCA\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "from UTILS import kmeans_clustering, sampen, chi2_distance, ts_visual, correct_key, chi2_distance, most_frequent, mssd\n",
    "import config_origins as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('change the configuration file if not done yet!')\n",
    "path_extracted_data = config.path_extracted_data\n",
    "path_initial_data = config.path_initial_data\n",
    "id_run = config.id_run\n",
    "dico_matching = config.dico_matching\n",
    "li_binmn = config.li_binmn\n",
    "pal_class_treat = config.pal_class_treat\n",
    "pal_treat = config.pal_treat\n",
    "pal_class = config.pal_class\n",
    "birth_date = config.birth_date\n",
    "dico_night_hour = config.dico_night_hour\n",
    "pal_interintre_treatment = config.pal_interintre_treatment\n",
    "max_date_adaptability = config.max_date_adaptability\n",
    "dico_pen_tr = config.dico_pen_tr\n",
    "path_extracted_data_visual = os.path.join(path_extracted_data,'visual')\n",
    "path_extracted_data_visual_corr = os.path.join(path_extracted_data,'visual','correlation')\n",
    "if not os.path.exists(path_extracted_data_visual_corr):\n",
    "    os.makedirs(path_extracted_data_visual_corr)\n",
    "path_extracted_data_adap = os.path.join(path_extracted_data,'Adaptability')\n",
    "print(id_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_selected_hens = pickle.load(open(os.path.join(path_extracted_data_adap, 'li_selected_hens.pkl'), 'rb'))\n",
    "print(len(li_selected_hens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#focal birds info to control for some variables\n",
    "df_FB = pd.read_csv(os.path.join(config.path_extracted_data, id_run+'df_FOCALBIRDS.csv'), sep=';')\n",
    "df_FB['early_death'] = df_FB['early_death'].fillna(0).replace(2,1) #2 means not sure\n",
    "df_FB.rename(columns={'29-09 weight':'InitialWeight','R-Pen':'RearingPenID'}, inplace=True)\n",
    "display(df_FB['early_death'].value_counts())\n",
    "print(df_FB.shape)\n",
    "df_FB.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HA&KBF&weight\n",
    "df_HA = pd.read_csv(os.path.join(path_extracted_data, 'df_all_HA.csv'), sep=';', parse_dates=['date'], dayfirst=True) \n",
    "print(df_HA.shape)\n",
    "df_HA = df_HA[df_HA['HenID'].isin(li_selected_hens)]\n",
    "#display(df_HA.groupby(['HAID'])['date'].agg(lambda x: set(x)).reset_index())\n",
    "print(df_HA.shape)\n",
    "df_HA.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dailyMVT var\n",
    "df_MVT_ALL = pd.read_csv(os.path.join(path_extracted_data, id_run+'_daily_ALL_variables_verified.csv'), sep=';', \n",
    "                     parse_dates=['level'], dayfirst=True) \n",
    "print(df_MVT_ALL.shape)\n",
    "df_MVT_ALL = df_MVT_ALL[df_MVT_ALL['HenID'].isin(li_selected_hens)]\n",
    "#restrict to the adaptability study timeframe\n",
    "df_MVT_ALL = df_MVT_ALL[df_MVT_ALL['level']<=config.max_date_adaptability]\n",
    "print(df_MVT_ALL.shape)\n",
    "#make sure we have their mvt\n",
    "df_MVT_ALL = df_MVT_ALL[~df_MVT_ALL['perc_duration_5_Zone'].isnull()]\n",
    "print(df_MVT_ALL.shape)\n",
    "#df_HA[['HAID','date']].drop_duplicates()\n",
    "df_MVT_ALL.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df_MVT_ALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for var in ['vertical_travel_distance_perh']:\n",
    "    df_corr = df_MVT_ALL.groupby(['WIB','HenID'])[var].agg(lambda x: np.mean(x)).reset_index()\n",
    "    df_corr = df_corr.pivot(index='HenID', columns='WIB', values=var).reset_index()\n",
    "    df_corr\n",
    "    li_dico = []\n",
    "    M = np.zeros(shape=(8,8))\n",
    "    for i in range(1,8):\n",
    "        for j in range(i+1,9):\n",
    "            coeff, p_val = pearsonr(df_corr[i].tolist(), df_corr[j].tolist())\n",
    "            coeff_np, p_val_np = spearmanr(df_corr[i].tolist(), df_corr[j].tolist())\n",
    "            if p_val<0.05:\n",
    "                M[i-1][j-1] = coeff_np\n",
    "                M[j-1][i-1] = coeff_np\n",
    "            else:\n",
    "                M[i-1][j-1] = 0\n",
    "                M[j-1][i-1] = 0\n",
    "            li_dico.append({'WIB1':i,'WIB2':j,'coeff':coeff, 'p_val':p_val, 'coeff_np':coeff_np, 'p_val_np':p_val_np})\n",
    "    df_corrres = pd.DataFrame(li_dico)\n",
    "    #display(df_corrres)\n",
    "    plt.title(var)\n",
    "    sns.heatmap(M);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(y='vertical_travel_distance_perh', x='DIB', data=df_MVT_ALL, hue='PenID');#,legend=False);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can observe that there is a general positive trend in the average vertical travel distance per hour as number of days post-transfer increases, but also that variance in the average vertical travel distance per hour is greater at the first days post-transferd compared to the 40th day post-transfer onward and there appears to be some non-linearity, and therefore we will investigate whether the fixed effect of number of days post-transfer on the average vertical travel distance per hour could be modelled with either a linear model or a quadratic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose data for individuality variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a useful id\n",
    "df_MVT_ALL['WIB_HenID'] = df_MVT_ALL.apply(lambda x: str(x['WIB'])+'_'+x['HenID'], axis=1)\n",
    "df_MVT_ALL.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep only the TRAN birds to better represent reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_MVT_ALL = df_MVT_ALL[df_MVT_ALL['Treatment']=='TRAN'].copy()\n",
    "len(df_MVT_ALL['HenID'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep only days since WG is open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_MVT_ALL.shape)\n",
    "df_MVT_ALL = df_MVT_ALL[df_MVT_ALL['level']>=config.date_first_opening_WG]\n",
    "df_MVT_ALL.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add variable to account for difference between individual: mean DIB over the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inddiff = df_MVT_ALL.groupby(['WIB','HenID','WIB_HenID'])['DIB'].agg(lambda x: list(x)).reset_index()\n",
    "df_inddiff['avgDIB'] = df_inddiff['DIB'].map(lambda x: np.mean(x))\n",
    "display(df_inddiff.head(3))\n",
    "display(df_inddiff.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inddiff[df_inddiff['HenID']=='hen_131']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inddiff[df_inddiff['HenID']=='hen_123']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_WIBhenID_avgDIB = dict(zip(df_inddiff.WIB_HenID, df_inddiff.avgDIB))\n",
    "#dico_WIBhenID_avgDIB\n",
    "df_MVT_ALL['avgDIB'] = df_MVT_ALL['WIB_HenID'].map(lambda x: dico_WIBhenID_avgDIB[x])\n",
    "df_MVT_ALL.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add external temperature as environemntal factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather conditions\n",
    "path_weather = os.path.join(r'C:\\Users\\camil\\Desktop\\vm_exchange\\PhD_Data\\Weather','Weather_process.csv')\n",
    "df_weather_h = pd.read_csv(path_weather, sep=';', parse_dates=['date','Timestamp'])\n",
    "df_weather_h = df_weather_h.sort_values('Timestamp') \n",
    "df_weather_h = df_weather_h.drop(['date'], axis=1)\n",
    "df_weather_h['Timestamp_weather'] = df_weather_h['Timestamp'].copy()\n",
    "print(df_weather_h.shape)\n",
    "display(df_weather_h.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check variation over the day... exist very much\n",
    "df_weather_h['date'] = df_weather_h['Timestamp'].map(lambda x: dt.datetime.date(x))\n",
    "df_wday = df_weather_h[df_weather_h['hour'].isin([11,12,13,14,15,16])].groupby(['date'])['temperature_C'].agg(lambda x: list(x)).reset_index()\n",
    "df_wday['temperature_C_avg'] = df_wday['temperature_C'].map(lambda x: np.mean(x))\n",
    "df_wday.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_MVT_ALL.shape)\n",
    "df_MVT_ALL['date'] = df_MVT_ALL['level'].map(lambda x: dt.datetime.date(x))\n",
    "df_MVT_ALL= pd.merge(df_MVT_ALL, df_wday, on='date', how='left')\n",
    "print(df_MVT_ALL.shape)\n",
    "df_MVT_ALL.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FB[['HenID','early_death','RearingPenID','InitialWeight']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_MVT_ALL.shape)\n",
    "df_MVT_ALL = pd.merge(df_MVT_ALL, df_FB[['HenID','early_death','RearingPenID','InitialWeight']], on='HenID', how='left')\n",
    "print(df_MVT_ALL.shape)\n",
    "df_MVT_ALL.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_cl = {'EPI':'Other','MEXP':'MEXP','LEXP':'LEXP','LEXPLOST':'Other','MEXPLOST':'Other'}\n",
    "df_MVT_ALL['CLASS'] = df_MVT_ALL['CLASS'].map(lambda x: dico_cl[x])\n",
    "df_MVT_ALL['CLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PCA but we have repeated measure per individual: avg per week, PCA over all to be comparable, assuming each (HenID,WIBavg) is different Extract linear composite of observed variables “to reduce the number of variables while retaining as much of the original variance as possible”"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"The PCA calculates a new projection of your data set. And the new axis are based on the standard deviation of your variables. So a variable with a high standard deviation will have a higher weight for the calculation of axis than a variable with a low standard deviation. If you normalize your data, all variables have the same standard deviation, thus all variables have the same weight and your PCA calculates relevant axis.\"\n",
    "--> normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df_MVT_ALL.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce to what we need and add normalized data (keeping both)\n",
    "#without WG as not existing in week 1\n",
    "li_var = ['perc_duration_5_Zone', 'perc_duration_3_Zone', 'perc_duration_2_Zone','perc_1_Zone_while_WG_open',\n",
    "          'nbr_stays_2_Zone_perh', 'nbr_stays_3_Zone_perh', 'nbr_stays_5_Zone_perh','nbr_stays_1_Zone_perh',\n",
    "          'in_WG_15mnAfterOpening','distribution_entropy', 'SleepingHeight','vertical_travel_distance_perh']\n",
    "li_pca = ['normalize_'+i for i in li_var]\n",
    "df_MVT_ALL[li_pca] = preprocessing.normalize(df_MVT_ALL[li_var])\n",
    "df_MVT = df_MVT_ALL[['WIB','HenID','DIB','avgDIB','CLASS','TrackingSystemID','PenID','temperature_C_avg',\n",
    "                     'early_death','RearingPenID','InitialWeight','Treatment']+li_var+li_pca].copy()\n",
    "df_MVT.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA on first obs. / week / ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df to do PCA on first observation per week per bird \n",
    "df_pca = df_MVT.groupby(['WIB','HenID'])[li_pca].agg(lambda x: list(x)[0]).reset_index()\n",
    "df_pca.head(3)\n",
    "\n",
    "#PCA with two components\n",
    "pca = PCA(n_components=2)\n",
    "X = df_pca[li_pca]\n",
    "pca.fit(X)\n",
    "\n",
    "#eigenvectores and eigenvules: pca.components_ has shape [n_components, n_features]\n",
    "df_pca_res = pd.DataFrame(pca.components_, columns=li_pca)\n",
    "df_pca_res['component_importance'] = pca.explained_variance_ratio_\n",
    "df_pca_res['explained variance'] = pca.explained_variance_\n",
    "df_pca_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "loading_matrix = pd.DataFrame(loadings, columns=['PC1', 'PC2'], index=li_pca)\n",
    "loading_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add PCA component per ind per day\n",
    "df_MVT['tuple_PCA'] = df_MVT.apply(lambda x: pca.transform(np.array([x[li_pca]])), axis=1)\n",
    "#only keep first one as explaines more than 80% of the var tot\n",
    "df_MVT['PC1'] = df_MVT['tuple_PCA'].map(lambda x: x[0][0])\n",
    "display(df_MVT.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "print(pearsonr(df_MVT['PC1'].tolist(), df_MVT['normalize_SleepingHeight'].tolist()),\n",
    "pearsonr(df_MVT['PC1'].tolist(), df_MVT['normalize_perc_duration_2_Zone'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validity of PC1 - PCA on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df to do PCA on full dataset\n",
    "df_pca_validity = df_MVT[li_pca].copy()\n",
    "df_pca_validity.head(3)\n",
    "\n",
    "#PCA with two components\n",
    "pca_validity = PCA(n_components=2)\n",
    "X = df_pca_validity[li_pca]\n",
    "pca_validity.fit(X)\n",
    "\n",
    "loadings_validity = pca_validity.components_.T * np.sqrt(pca_validity.explained_variance_)\n",
    "loading_matrix_validity = pd.DataFrame(loadings_validity, columns=['PC1', 'PC2'], index=li_pca)\n",
    "\n",
    "sns.heatmap(loading_matrix_validity[['PC1','PC2']], annot=True)\n",
    "plt.title('Loadings of PCA on full dataset')\n",
    "plt.savefig(os.path.join(path_extracted_data_adap,'Loadings of PCA performed on full dataset.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "sns.heatmap(loading_matrix[['PC1','PC2']], annot=True)\n",
    "plt.title('Loadings of PCA on first obs per week per ind');\n",
    "plt.savefig(os.path.join(path_extracted_data_adap,'Loadings of PCA performed on first obs per week per ind.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MVT['PC1_standardized'] = preprocessing.scale(df_MVT['PC1'])\n",
    "print(df_MVT.shape)\n",
    "display(df_MVT.head(3))\n",
    "df_MVT.to_csv(os.path.join(path_extracted_data_adap,'df_MVT_4individuality.csv'), index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of individual per class per week\n",
    "df_MVT.groupby(['WIB','CLASS'])['HenID'].agg(lambda x: len(set(x))).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_MVT, x=\"PC1_standardized\", hue=\"WIB\", element=\"step\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_MVT, x=\"PC1\", element=\"step\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_MVT, x=\"PC1_standardized\", element=\"step\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot plasticity over time\n",
    "sns.lineplot(x='DIB', y='PC1', data=df_MVT, hue='PenID');\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(y='vertical_travel_distance_perh', x='DIB', data=df_MVT, hue='PenID');#,legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
