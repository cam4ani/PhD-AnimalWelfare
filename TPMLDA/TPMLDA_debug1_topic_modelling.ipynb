{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\camil\\anaconda3\\lib\\site-packages\\_pytest\\fixtures.py:844: DeprecationWarning: The `convert` argument is deprecated in favor of `converter`.  It will be removed after 2019/01.\n",
      "  params = attr.ib(convert=attr.converters.optional(tuple))\n",
      "c:\\users\\camil\\anaconda3\\lib\\site-packages\\_pytest\\fixtures.py:846: DeprecationWarning: The `convert` argument is deprecated in favor of `converter`.  It will be removed after 2019/01.\n",
      "  ids = attr.ib(default=None, convert=_ensure_immutable_ids)\n",
      "c:\\users\\camil\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "#standard package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "import time \n",
    "import glob\n",
    "import tqdm\n",
    "import sys\n",
    "import shutil\n",
    "import itertools\n",
    "import pickle\n",
    "import operator\n",
    "from operator import itemgetter\n",
    "import datetime as dt\n",
    "\n",
    "#topics modeling\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import gensim \n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "#plot\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\camil\\anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we restrict the time series to one value per 60 seconds \n",
      "we compute the complexity variables each 30 minutes \n",
      "each variables includes the values of at least the last 120.00 minutes (i.e. are using 120.00 values)\n"
     ]
    }
   ],
   "source": [
    "from UTILS import word_from_MLP, word_from_MLP_end_and_begin_atlongDuration\n",
    "import config_mobility as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This topic modeling will be save under the name: LB_fixedLength_lengthwords_3_sess_9_10_11\n"
     ]
    }
   ],
   "source": [
    "#comment on this run of the script\n",
    "comment = ''\n",
    "\n",
    "### choose hybrid type\n",
    "type_hybrid = 'LB' #LSL, all, LB\n",
    "\n",
    "### choose type of word definition\n",
    "type_ = 'fixedLength' ; length_words = 3 ; comment = comment+'lengthwords_'+str(length_words)\n",
    "#type_ = 'varyinglength' ; morethan4_in1 = True ; comment = comment+'_morethan4in1_'+str(int(morethan4_in1))\n",
    "\n",
    "### choose session to take into account\n",
    "#Sess2keep = [1,2,3] \n",
    "Sess2keep = [2,3,4] \n",
    "Sess2keep = [3,4,5] \n",
    "Sess2keep = [4,5,6] \n",
    "Sess2keep = [5,6,7] \n",
    "Sess2keep = [6,7,8] \n",
    "Sess2keep = [7,8,9] \n",
    "Sess2keep = [8,9,10] \n",
    "Sess2keep = [9,10,11]\n",
    "\n",
    "### unique naming\n",
    "title_ = type_hybrid+'_'+type_+'_'+comment+'_sess_'+'_'.join([str(x) for x in Sess2keep]) \n",
    "#'_LB', '_all','_LSL' '_LB_newword_def' '_all_newword_def'\n",
    "print('This topic modeling will be save under the name: '+title_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise parameters\n",
    "path_extracted_data = config.path_extracted_data\n",
    "path_initial_data = config.path_initial_data\n",
    "id_run = config.id_run\n",
    "#create a director if not existing\n",
    "path_save = os.path.join(path_extracted_data,'visual','LDA', title_)\n",
    "if not os.path.exists(path_save):\n",
    "    os.makedirs(path_save)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO\n",
    "LDA per session per bird and with different meaning of mvt\n",
    "Lets define a movt to be: moving from a place where it staid more than x(60?)sec, to another where it stays more than 60seconds\n",
    "\n",
    "différente longeur: \n",
    "- Pas bien d'utiliser climate change, et climate et change! donc se sticker à une size c'est peut être mieux, aussi ca peut faire pus de sens pour l'interpretation.\n",
    "+ peut identifier les birds qui font plus de chaotic transition? (i.e. of length=2) & ceux qui font des move interessant (i.e. length >2)\n",
    "\n",
    "keep the time of the day where each words starts\n",
    "should we keep duration?\n",
    "if any interpretation came then one can verify in the video to confirm or not\n",
    "ethogram to predefine the words in a meaning full manners\n",
    "--> define in a good way to avoid fishing and help interpretation\n",
    "remove transitional mvt? remove all bouts < than 60 seconds? I think the first purpose is to see what is of interest, wihtout knowing.\n",
    "\n",
    "1. bag of words: each documents will be converted to a vector of length of the vocabulary from all documents, and will for each voc give the count of this word in the particular documents ([0,0,2,1,0,0]: document has twice the 3 element from the voc and once the fourth element from the document)\n",
    "2.When training an LDA model, you start with a collection of documents and each of these is represented by a fixed-length vector (bag-of-words). LDA is a general Machine Learning (ML) technique, which means that it can also be used for other unsupervised ML problems where the input is a collection of fixed-length vectors and the goal is to explore the structure of this data.\n",
    "\n",
    "TO READ ABOUT: topic modelling with small dictionary, ethogram to define words, \n",
    "\n",
    "TODO: define trois size of duration length (small, medium, long) that will be associated to each movement additionally to the hour of the day.\n",
    "TOTRY: should we keep the mouvement during the night as well?? and add night hours? but those are less frequent so if yes, then dont remove them as easily than the other movement when preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7920, 43)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HenID</th>\n",
       "      <th>timepoint</th>\n",
       "      <th>group</th>\n",
       "      <th>age</th>\n",
       "      <th>DayID</th>\n",
       "      <th>day</th>\n",
       "      <th>file</th>\n",
       "      <th>severity</th>\n",
       "      <th>pen</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>...</th>\n",
       "      <th>meanchange</th>\n",
       "      <th>mean1</th>\n",
       "      <th>medianchange</th>\n",
       "      <th>transitions</th>\n",
       "      <th>real0</th>\n",
       "      <th>sumdur</th>\n",
       "      <th>nest_vs_total</th>\n",
       "      <th>severity_diff</th>\n",
       "      <th>KBF_new</th>\n",
       "      <th>indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>27.10.2016</td>\n",
       "      <td>1A01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LSL</td>\n",
       "      <td>...</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57067.0</td>\n",
       "      <td>0.177419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>28.10.2016</td>\n",
       "      <td>1A01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LSL</td>\n",
       "      <td>...</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56854.0</td>\n",
       "      <td>0.171053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>29.10.2016</td>\n",
       "      <td>1A01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LSL</td>\n",
       "      <td>...</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56721.0</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HenID  timepoint group  age DayID         day  file  severity  pen hybrid  \\\n",
       "0  hen_1          1     A   21     1  27.10.2016  1A01       0.0    1    LSL   \n",
       "1  hen_1          1     A   21     2  28.10.2016  1A01       0.0    1    LSL   \n",
       "2  hen_1          1     A   21     3  29.10.2016  1A01       0.0    1    LSL   \n",
       "\n",
       "     ...      meanchange  mean1  medianchange  transitions  real0   sumdur  \\\n",
       "0    ...            1.23    1.0           1.0         91.0    NaN  57067.0   \n",
       "1    ...            1.28    1.0           1.0        108.0    NaN  56854.0   \n",
       "2    ...            1.17    1.0           1.0        128.0    NaN  56721.0   \n",
       "\n",
       "   nest_vs_total  severity_diff  KBF_new  indicator  \n",
       "0       0.177419            NaN      0.0        NaN  \n",
       "1       0.171053            0.0      0.0        NaN  \n",
       "2       0.255319            0.0      0.0        NaN  \n",
       "\n",
       "[3 rows x 43 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['hen_41', 'hen_42'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master = pd.read_csv(os.path.join(path_initial_data,'Mo_Masterfile.csv'), sep=';',parse_dates=['day']) #, parse_dates=['date']) wrong!\n",
    "df_master.rename(columns={'time':'Timestamp','day':'DayID','hen':'HenID','date':'day'}, inplace=True)\n",
    "df_master['HenID'] = df_master['HenID'].map(lambda x: 'hen_'+str(x))\n",
    "print(df_master.shape)\n",
    "display(df_master.head(3))\n",
    "#dico_henid_hybrid = dict(zip(df_master['HenID'].tolist(),df_master['hybrid'].tolist()))\n",
    "li_LB = df_master[df_master['hybrid']=='LB']['HenID'].unique()\n",
    "li_LSL = df_master[df_master['hybrid']=='LSL']['HenID'].unique()\n",
    "print(len(li_LB), len(li_LSL))\n",
    "li_LB[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 10A\n",
      "(513000, 54)\n",
      "(324000, 54)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███▊                                                                               | 1/22 [00:05<02:03,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 10B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\camil\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,45,46,47,48,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513000, 54)\n",
      "(324000, 54)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████▌                                                                           | 2/22 [00:11<01:54,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 11A\n",
      "(513000, 46)\n",
      "(324000, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|███████████▎                                                                       | 3/22 [00:15<01:41,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 11B\n",
      "(513000, 48)\n",
      "(324000, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████████████                                                                    | 4/22 [00:20<01:32,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 1A\n",
      "(513000, 64)\n",
      "(324000, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██████████████████▊                                                                | 5/22 [00:27<01:35,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 1B\n",
      "(513000, 63)\n",
      "(324000, 63)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██████████████████████▋                                                            | 6/22 [00:33<01:34,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 2A\n",
      "(513000, 60)\n",
      "(324000, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|██████████████████████████▍                                                        | 7/22 [00:40<01:31,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 2B\n",
      "(513000, 65)\n",
      "(324000, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|██████████████████████████████▏                                                    | 8/22 [00:45<01:23,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 3A\n",
      "(513000, 63)\n",
      "(324000, 63)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|█████████████████████████████████▉                                                 | 9/22 [00:51<01:18,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 3B\n",
      "(513000, 64)\n",
      "(324000, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████████▎                                            | 10/22 [00:58<01:13,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 4A\n",
      "(513000, 59)\n",
      "(324000, 59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████                                         | 11/22 [01:05<01:09,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 4B\n",
      "(513000, 64)\n",
      "(324000, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|████████████████████████████████████████████▋                                     | 12/22 [01:11<01:03,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 5A\n",
      "(513000, 59)\n",
      "(324000, 59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████▍                                 | 13/22 [01:17<00:56,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 5B\n",
      "(513000, 63)\n",
      "(324000, 63)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████████▏                             | 14/22 [01:23<00:49,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 6A\n",
      "(513000, 60)\n",
      "(324000, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|███████████████████████████████████████████████████████▉                          | 15/22 [01:29<00:41,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 6B\n",
      "(513000, 63)\n",
      "(324000, 63)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████████████████████████████████▋                      | 16/22 [01:36<00:37,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 7A\n",
      "(513000, 57)\n",
      "(324000, 57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████████████████████████████████████████████████████████████▎                  | 17/22 [01:42<00:31,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 7B\n",
      "(513000, 61)\n",
      "(324000, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████████████████████████████████████               | 18/22 [01:48<00:25,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 8A\n",
      "(513000, 56)\n",
      "(324000, 56)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|██████████████████████████████████████████████████████████████████████▊           | 19/22 [01:54<00:18,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 8B\n",
      "(513000, 62)\n",
      "(324000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|██████████████████████████████████████████████████████████████████████████▌       | 20/22 [02:01<00:12,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 9A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\camil\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513000, 55)\n",
      "(324000, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|██████████████████████████████████████████████████████████████████████████████▎   | 21/22 [02:07<00:06,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------- 9B\n",
      "(513000, 55)\n",
      "(324000, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [02:13<00:00,  6.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7128000, 126)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>day</th>\n",
       "      <th>hen_1</th>\n",
       "      <th>hen_10</th>\n",
       "      <th>hen_100</th>\n",
       "      <th>hen_101</th>\n",
       "      <th>hen_102</th>\n",
       "      <th>hen_103</th>\n",
       "      <th>hen_104</th>\n",
       "      <th>hen_105</th>\n",
       "      <th>...</th>\n",
       "      <th>hen_94</th>\n",
       "      <th>hen_95</th>\n",
       "      <th>hen_96</th>\n",
       "      <th>hen_97</th>\n",
       "      <th>hen_98</th>\n",
       "      <th>hen_99</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_day</th>\n",
       "      <th>nbr_nan</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>2017-07-06 02:00:00</td>\n",
       "      <td>2017-07-06</td>\n",
       "      <td>zone_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>2017-07-06 02:00:01</td>\n",
       "      <td>2017-07-06</td>\n",
       "      <td>zone_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>10A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Timestamp        day   hen_1 hen_10 hen_100 hen_101 hen_102  \\\n",
       "1800 2017-07-06 02:00:00 2017-07-06  zone_5    NaN     NaN     NaN     NaN   \n",
       "1801 2017-07-06 02:00:01 2017-07-06  zone_5    NaN     NaN     NaN     NaN   \n",
       "\n",
       "     hen_103 hen_104 hen_105   ...   hen_94 hen_95 hen_96 hen_97 hen_98  \\\n",
       "1800     NaN     NaN     NaN   ...      NaN    NaN    NaN    NaN    NaN   \n",
       "1801     NaN     NaN     NaN   ...      NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "     hen_99 hour is_day nbr_nan session  \n",
       "1800    NaN    2   True       0     10A  \n",
       "1801    NaN    2   True       0     10A  \n",
       "\n",
       "[2 rows x 126 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>day</th>\n",
       "      <th>hen_1</th>\n",
       "      <th>hen_10</th>\n",
       "      <th>hen_100</th>\n",
       "      <th>hen_101</th>\n",
       "      <th>hen_102</th>\n",
       "      <th>hen_103</th>\n",
       "      <th>hen_104</th>\n",
       "      <th>hen_105</th>\n",
       "      <th>...</th>\n",
       "      <th>hen_94</th>\n",
       "      <th>hen_95</th>\n",
       "      <th>hen_96</th>\n",
       "      <th>hen_97</th>\n",
       "      <th>hen_98</th>\n",
       "      <th>hen_99</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_day</th>\n",
       "      <th>nbr_nan</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>487798</th>\n",
       "      <td>2017-06-14 16:59:58</td>\n",
       "      <td>2017-06-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zone_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zone_2</td>\n",
       "      <td>zone_2</td>\n",
       "      <td>zone_3</td>\n",
       "      <td>...</td>\n",
       "      <td>zone_3</td>\n",
       "      <td>zone_4</td>\n",
       "      <td>zone_5</td>\n",
       "      <td>zone_5</td>\n",
       "      <td>zone_4</td>\n",
       "      <td>zone_2</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>9B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487799</th>\n",
       "      <td>2017-06-14 16:59:59</td>\n",
       "      <td>2017-06-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zone_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zone_2</td>\n",
       "      <td>zone_2</td>\n",
       "      <td>zone_3</td>\n",
       "      <td>...</td>\n",
       "      <td>zone_3</td>\n",
       "      <td>zone_4</td>\n",
       "      <td>zone_5</td>\n",
       "      <td>zone_5</td>\n",
       "      <td>zone_4</td>\n",
       "      <td>zone_2</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>9B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Timestamp        day hen_1 hen_10 hen_100 hen_101 hen_102  \\\n",
       "487798 2017-06-14 16:59:58 2017-06-14   NaN    NaN     NaN  zone_5     NaN   \n",
       "487799 2017-06-14 16:59:59 2017-06-14   NaN    NaN     NaN  zone_5     NaN   \n",
       "\n",
       "       hen_103 hen_104 hen_105   ...    hen_94  hen_95  hen_96  hen_97  \\\n",
       "487798  zone_2  zone_2  zone_3   ...    zone_3  zone_4  zone_5  zone_5   \n",
       "487799  zone_2  zone_2  zone_3   ...    zone_3  zone_4  zone_5  zone_5   \n",
       "\n",
       "        hen_98  hen_99 hour is_day nbr_nan session  \n",
       "487798  zone_4  zone_2   16   True       0      9B  \n",
       "487799  zone_4  zone_2   16   True       0      9B  \n",
       "\n",
       "[2 rows x 126 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time: 2.94 mn\n"
     ]
    }
   ],
   "source": [
    "#open time series per session and compute the variables for each session (car time series make sence at session level), et en \n",
    "#plus des variables tel que running entropy over the whole session ateach last timestamp of each level make sence only at \n",
    "#session level\n",
    "START_TIME = time.clock()\n",
    "starting_hour = 2\n",
    "li_path = glob.glob(os.path.join(path_extracted_data, id_run+'_TimeSeries_*.csv'))\n",
    "li_path = [i for i in li_path if '_hen_' not in i]\n",
    "li_df = []\n",
    "for path_ in tqdm.tqdm(li_path):\n",
    "    \n",
    "    #download time series associated to this session\n",
    "    name_ = path_.split('_')[-1].split('.')[0]\n",
    "    print('-------------------------------------------------------------------------', name_)\n",
    "    df_ts = pd.read_csv(path_, sep=';', parse_dates=['Timestamp', 'day']) \n",
    "    li_hours_to_consider=list(range(starting_hour,17))\n",
    "    df_ts['is_day'] = df_ts['hour'].map(lambda x: x in li_hours_to_consider)\n",
    "    print(df_ts.shape)\n",
    "    df_ts = df_ts[df_ts['is_day']]\n",
    "    print(df_ts.shape)\n",
    "    df_ts['session'] = name_\n",
    "    li_df.append(df_ts)\n",
    "df_ts = pd.concat(li_df)\n",
    "print(df_ts.shape)\n",
    "display(df_ts.head(2))\n",
    "display(df_ts.tail(2))    \n",
    "END_TIME = time.clock()\n",
    "print (\"Total running time: %.2f mn\" %((END_TIME-START_TIME)/60))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7128000, 126)\n",
      "(7128000, 66)\n",
      "(7128000, 67)\n"
     ]
    }
   ],
   "source": [
    "#adapt to the need (keep columns of one species only if wanted and rows with session that we want)\n",
    "print(df_ts.shape)\n",
    "if 'LB' in title_:\n",
    "    df_ts = df_ts.filter([x for x in df_ts.columns if (not x.startswith('hen_')) | (x in li_LB)],axis=1)\n",
    "if 'LSL' in title_:\n",
    "    df_ts = df_ts.filter([x for x in df_ts.columns if (not x.startswith('hen_')) | (x in li_LSL)],axis=1)\n",
    "print(df_ts.shape)\n",
    "df_ts['sessionID'] = df_ts['session'].map(lambda x: int(x[:-1]))\n",
    "#df_ts['sessionID'].value_counts()\n",
    "print(df_ts.shape)\n",
    "df_ts = df_ts[df_ts['sessionID'].isin(Sess2keep)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics modelling (lda - latent dirichlet allocation)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "info on model parameters \n",
    "https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "https://stackoverflow.com/questions/50805556/understanding-parameters-in-gensim-lda-model\n",
    "*passes (int, optional) – Number of passes through the corpus during training\n",
    "*random_state - this serves as a seed (in case you wanted to repeat exactly the training process)\n",
    "*chunksize - number of documents to consider at once (affects the memory consumption)\n",
    "*update_every - update the model every update_every chunksize chunks (essentially, this is for memory consumption optimization)\n",
    "*passes - how many times the algorithm is supposed to pass over the whole corpus\n",
    "*alpha - can be set to an explicit array = prior of your choice. It also support special values of `‘asymmetric’ and ‘auto’: the former uses a fixed normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric prior directly from your data.\n",
    "*per_word_topics - setting this to True allows for extraction of the most likely topics given a word. The training process is set in such a way that every word will be assigned to a topic. Otherwise, words that are not indicative are going to be omitted. phi_value is another parameter that steers this process - it is a threshold for a word treated as indicative or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the fixed number of bouts included in each word\n",
    "#define 3 size of duration length (small, medium, long) that will be associated to each movement additionally to the daily hour \n",
    "#dico_size = {'transition':range(1,60),\n",
    "#            'zone':range(60,60*60*24)}\n",
    "dico_size = {'small':range(1,60),\n",
    "             'intermediate':range(60,15*60),\n",
    "             'large':range(15*60,60*60*24)}\n",
    "li_hen = [i for i in df_ts.columns if i.startswith('hen_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 36/36 [00:19<00:00,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "all_ = False\n",
    "        \n",
    "### documents:  set of words of daily hen time series\n",
    "documents = {}\n",
    "for day, df_ in tqdm.tqdm(df_ts.groupby(['day'])):\n",
    "    df_ = df_.fillna(' ')\n",
    "    for h in li_hen:\n",
    "        li = df_[h].tolist()\n",
    "        #remove if nan in ts or if only one zone (we separate for efficiency)\n",
    "        if li[0]!=' ':\n",
    "            if len(set(li))>1:\n",
    "                if type_=='varyinglength':\n",
    "                    documents[h+'/-/'+str(day).split(' ')[0]] = word_from_MLP_end_and_begin_atlongDuration(li, 60*5, \n",
    "                                                                                                           starting_hour,\n",
    "                                                                                                           dico_size, config,\n",
    "                                                                                                           morethan4_in1=morethan4_in1)\n",
    "                if type_ =='fixedLength':\n",
    "                    documents[h+'/-/'+str(day).split(' ')[0]] = word_from_MLP(li, length_words, starting_hour, dico_size)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hen_45/-/2016-10-28'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b23be86c4950>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'hen_45'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mday\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'2016-10-28'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/-/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'day'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%Y-%m-%d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'   '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'hen_45/-/2016-10-28'"
     ]
    }
   ],
   "source": [
    "h = 'hen_45'\n",
    "day = '2016-10-28'\n",
    "display(documents[h+'/-/'+day])\n",
    "plt.plot(df_ts[df_ts['day']==dt.datetime.strptime(day, '%Y-%m-%d')][h].tolist());\n",
    "plt.title(h+'   '+day);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_words = []\n",
    "for h_day, li_li_tupleWordTime in documents.items():\n",
    "    li_words.extend([x for x in li_li_tupleWordTime])\n",
    "print(len(li_words), len(set(li_words)))\n",
    "\n",
    "#compute frequencies\n",
    "c_words = Counter(li_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len([x for x,c in c_words.items() if c==1]), [x for x,c in c_words.items() if c==1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lemmatizig and Stemming\n",
    "As words have different number of zones, it have different complexity as well. \n",
    "In current language a verb have different usage depending on the tense and the subject, while a noun have fewer variation\n",
    "lemmatizing and stemming are here to deal with this.\n",
    "Rule we will have to normalize our words\n",
    "* dont take into account the duration (from >6000 to 1779 words that are unique)\n",
    "* remove all the zones in between two zones: keep only the zones that change the direction of the bird (from 1779 to 1582)\n",
    "* for sub-MLPS with strictly more than 4 zones, transform them into a list of zone ordered by their first occurences in the list only and add the correct end zone (i.e. car large) at the end if not already correct (OR into an ordered list of unique zones) and add a third variable 'nbr of zone modulo 3'(from 1582 to 330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### frequence analysis\n",
    "#create a list of all words\n",
    "li_words = []\n",
    "for h_day, li_li_tupleWordTime in documents.items():\n",
    "    li_words.extend([x for x in li_li_tupleWordTime])\n",
    "print(len(li_words), len(set(li_words)))\n",
    "\n",
    "#compute frequencies\n",
    "c_words = Counter(li_words)\n",
    "df_word_frequence = pd.DataFrame.from_dict({'word':list(c_words.keys()),'frequence':list(c_words.values())})\n",
    "df_word_frequence = df_word_frequence.sort_values('frequence',ascending=False)\n",
    "df_word_frequence.to_csv(os.path.join(path_save,'word_Frequence.csv'),index=False,sep=';')\n",
    "display(df_word_frequence.head(3))\n",
    "display(df_word_frequence.tail(3))\n",
    "\n",
    "#simple barplot (sorted with x values)\n",
    "d = {k:v for k,v in c_words.items() if v>5}\n",
    "d = sorted(d.items(), key=operator.itemgetter(1))\n",
    "x = [i[0] for i in d]\n",
    "y = [i[1] for i in d]\n",
    "fig = plt.figure(figsize=(30,7))\n",
    "ax = plt.subplot(111)\n",
    "width = 0.8\n",
    "ax.bar(range(len(x)), y, width=width)\n",
    "plt.title('most frequent words')\n",
    "ax.set_xticks(np.arange(len(x)) + width/2)\n",
    "ax.set_xticklabels(x, rotation=90,size=6);\n",
    "plt.savefig(os.path.join(path_save,'most_frequent_word.png'),dpi=300,format='png',bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "#histogram\n",
    "plt.hist(df_word_frequence[df_word_frequence['frequence']<=100]['frequence'],bins='auto') #into 15 equal parts \n",
    "#return: [0]: vector of length bins with #elements in each bins\n",
    "#and [1]: when the vectors starts (for plot)\n",
    "plt.xlabel('frequencies')\n",
    "plt.ylabel('number of words')    \n",
    "#--> choose the nbr_times parameter: number of times a words need to appear at least this amount of time, in the overall set of \n",
    "#documents to be taken into account\n",
    "plt.savefig(os.path.join(path_save,'histogram_of_word_frequencies.png'),dpi=300,format='png',bbox_inches='tight')\n",
    "plt.show()\n",
    "nbr_times = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### clean documents\n",
    "def MLPsWord4lda(documents, c_words, nbr_times):\n",
    "    #remove all words appearing less or equal to nbr_times times in the overall time series\n",
    "    tokens_removed = set(word for word in c_words.keys() if c_words[word]<nbr_times)\n",
    "    cleaned_documents = {h_day:[word for word in words if word not in tokens_removed] for h_day,words in documents.items()}\n",
    "    #perhaps later: remove with to much transition words? keep it for now, I dont know what this means\n",
    "    return(cleaned_documents, tokens_removed)\n",
    "\n",
    "cleaned_documents, tokens_removed = MLPsWord4lda(documents, c_words, nbr_times)\n",
    "len(tokens_removed)\n",
    "\n",
    "### dictionary & bag of word corpus\n",
    "#dictionary:  mapping from word IDs to words\n",
    "#keep track of the order of the li_documents regarding the ts ID\n",
    "dico_tsID_listID = dict(zip(list(cleaned_documents.keys()),range(len(cleaned_documents.keys()))))\n",
    "dico_listID_tsID = {v:k for k,v in dico_tsID_listID.items()}\n",
    "li_documents = [cleaned_documents[dico_listID_tsID[listID]] for listID in range(len(dico_listID_tsID))]\n",
    "print('We have %d documents (i.e. daily hens time series). The first one has %d words (once cleaned)'%(len(li_documents), \n",
    "                                                                                                       len(li_documents[0])))\n",
    "dictionary = corpora.Dictionary(li_documents)\n",
    "print('There is %d words in your dictionary'%len(dictionary))\n",
    "\n",
    "#corpus: list with a bag of words (sparse document vectors: list of tuples(word_id, appearance)) for each documents\n",
    "corpus = [dictionary.doc2bow(text) for text in li_documents]\n",
    "print(len(corpus),corpus[10])\n",
    "\n",
    "#save\n",
    "pickle.dump(corpus, open(os.path.join(path_save,'corpus.pkl'), 'wb'))\n",
    "dictionary.save(os.path.join(path_save,'dictionary.gensim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for nbr_topics in tqdm.tqdm(range(2,15)): \n",
    "\n",
    "    #create a director if not existing\n",
    "    path_save = os.path.join(path_extracted_data,'visual','LDA', title_, str(nbr_topics))\n",
    "    if not os.path.exists(path_save):\n",
    "        os.makedirs(path_save)\n",
    "        \n",
    "    #train lda\n",
    "    start = time.time()\n",
    "    lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, \n",
    "                                   num_topics=nbr_topics, \n",
    "                                   passes=30, \n",
    "                                   chunksize=50, \n",
    "                                   random_state=100,\n",
    "                                   update_every=5, \n",
    "                                   alpha='auto', \n",
    "                                   per_word_topics=False)\n",
    "    lda.save(os.path.join(path_save,'model'+str(nbr_topics)+'.gensim'))\n",
    "    end = time.time()\n",
    "    print (\"Total running time: %dmin\"%((end-start)/60))\n",
    "\n",
    "    #show the latent topics\n",
    "    for topicsID_topicsWordDistribution in lda.print_topics():\n",
    "        print('\\n----',topicsID_topicsWordDistribution[0])\n",
    "        print(topicsID_topicsWordDistribution[1])\n",
    "        \n",
    "    #summarize the results\n",
    "    df_topics = pd.DataFrame(list(documents.items()),columns=['documentID','li_words'])\n",
    "    df_topics['li_words_cleaned'] = df_topics['documentID'].map(lambda x: cleaned_documents[x])\n",
    "    df_topics['corpus'] = df_topics['li_words_cleaned'].map(lambda x: dictionary.doc2bow(x))\n",
    "    df_topics['lda_corpus'] = df_topics['corpus'].map(lambda x: lda[x])\n",
    "\n",
    "    #add info on hen\n",
    "    df_topics['HenID'] = df_topics['documentID'].map(lambda x: x.split('/-/')[0])\n",
    "    df_topics['day'] = df_topics['documentID'].map(lambda x: dt.datetime.strptime(x.split('/-/')[1], '%Y-%m-%d'))\n",
    "\n",
    "    #add info of topic \n",
    "    df_topics['topic_info'] = df_topics['lda_corpus'].map(lambda x: sorted(x,key=itemgetter(1))[-1])\n",
    "    df_topics['topic'] = df_topics['topic_info'].map(lambda x: x[0])\n",
    "    df_topics['topic_proba'] = df_topics['topic_info'].map(lambda x: x[1])\n",
    "    for t in range(nbr_topics):\n",
    "        df_topics['topic_'+str(t)+'_proba'] = df_topics['lda_corpus'].map(lambda x: max([i[1] for i in x if i[0]==t]+[0]))\n",
    "\n",
    "    #save\n",
    "    df_topics.to_csv(os.path.join(path_save,'df_topics'+str(nbr_topics)+'.csv'), index=False,sep=';')\n",
    "    print(df_topics.shape)\n",
    "    df_topics.head(3)\n",
    "    print(df_topics['topic'].value_counts())\n",
    "    if len(df_topics['topic'].unique())!=nbr_topics:\n",
    "        print('WARNING: to many topics, there is only %d instead of the %d wanted, we will stop now'%(len(df_topics['topic'].unique()), nbr_topics))\n",
    "        sys.exit()\n",
    "    ### topics repartition across hens\n",
    "    df_plot = df_topics.groupby(['HenID','topic']).size().reset_index().pivot(columns='topic', index='HenID', values=0)\n",
    "    df_plot = df_plot.fillna(0)\n",
    "    display(df_plot.head(3))\n",
    "    df_plot_normalized = df_plot.div(df_plot.sum(axis=1)*0.01, axis=0).sort_values([0]) #sort accoridng to subject 0\n",
    "    display(df_plot_normalized.head(3))\n",
    "    li_color = sns.color_palette(\"RdBu\", nbr_topics)\n",
    "    df_plot_normalized.plot(x=df_plot_normalized.index, kind='bar', stacked=True, figsize=(30,7), \n",
    "                            legend=True, color=li_color).legend(bbox_to_anchor=(1.2, 0.5));\n",
    "    plt.savefig(os.path.join(path_save,'topic_repartition_across_hen_'+str(nbr_topics)+'.png'), \n",
    "                dpi=300, format='png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    ### topic mixture across documents    \n",
    "    if all_:\n",
    "        for t in range(nbr_topics):\n",
    "            df_plot_normalized = df_topics[['topic_'+str(i)+'_proba' for i in range(nbr_topics)]].sort_values(['topic_'+str(t)+'_proba'])\n",
    "            df_plot_normalized.plot(x=df_plot_normalized.index, kind='bar', stacked=True, figsize=(40,10), \n",
    "                                    legend=True).legend(bbox_to_anchor=(1.2, 0.5));\n",
    "            plt.savefig(os.path.join(path_save, 'topic_mixture_across_document_'+str(t)+'.png'),dpi=300,format='png',bbox_inches='tight')\n",
    "            plt.show()\n",
    "        \n",
    "    ### topics appearance across days\n",
    "    df_plot = df_topics.sort_values('day', ascending=True).copy()\n",
    "    #dico_topicID_color = {0:(0.979891, 0.90894778, 0.84827858), 1:'blue', 2:'yellow', 3:'green', 4:'grey', 5:'lime', 6:'orange'}\n",
    "    li_color = sns.color_palette(\"rocket_r\", nbr_topics)\n",
    "    c = 2\n",
    "    if nbr_topics%3==0:\n",
    "        c = 3\n",
    "    l = math.ceil(nbr_topics/c)\n",
    "    fig = plt.figure(figsize=(c*5, l*5))\n",
    "    x = df_plot['day'].unique()\n",
    "    for i, (t, df_) in enumerate(df_plot.groupby(['topic'])):\n",
    "        ax = plt.subplot(l,c,i+1)\n",
    "        df_ = df_.groupby(['day'])['documentID'].count().reset_index()\n",
    "        x = df_['day'].tolist()\n",
    "        y = df_['documentID'].tolist()\n",
    "        print(len(x))\n",
    "        plt.plot(x,y,color=li_color[i])\n",
    "        plt.ylim(0,max(y)+max(int(max(y)*0.1),1))\n",
    "        for label in ax.get_xticklabels():\n",
    "            label.set_rotation(25) \n",
    "        plt.title('topic   '+str(t))\n",
    "    plt.savefig(os.path.join(path_save,'topic_appearance_across_days_.png'),dpi=300,format='png',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    display(df_.head(3))    \n",
    "    \n",
    "    ### hens main topics across days\n",
    "    if all_:\n",
    "        df_plot = df_topics.sort_values('day', ascending=True).copy()\n",
    "        #x = list(set(df_plot['day'].tolist()))\n",
    "        for t in range(nbr_topics):\n",
    "            fig = plt.figure(figsize=(20,6))\n",
    "            for i, (h, df_) in enumerate(df_plot.groupby(['HenID'])):\n",
    "                x = df_['day'].tolist()\n",
    "                y = df_['topic_'+str(t)+'_proba'].tolist()\n",
    "                plt.plot(x,y)\n",
    "                plt.show()\n",
    "                fig = plt.figure(figsize=(20,6))\n",
    "                if i==5:\n",
    "                    sys.exit()\n",
    "            plt.title('topic   '+str(t))\n",
    "            plt.show()\n",
    "        \n",
    "    ### main topics proba\n",
    "    plt.hist(df_topics['topic_proba'].dropna(), bins=200)\n",
    "    plt.xlabel('main topic predominance')\n",
    "    plt.ylabel('number of documents (daily hen ts)');\n",
    "    plt.savefig(os.path.join(path_save,'main_topic_proba.png'),dpi=300,format='png',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    ### plot visual time series into cluster-folders\n",
    "    for i in range(0,nbr_topics):\n",
    "        path_ts = os.path.join(path_save, 'timeseries_plot_cluster', str(i))\n",
    "        if not os.path.exists(path_ts):\n",
    "            os.makedirs(path_ts)\n",
    "\n",
    "    path_ = r'D:\\vm_exchange\\AVIFORUM\\data\\extracted_info_mobility_VF\\visual\\TimeSeriesPlot\\time_series_plot'\n",
    "\n",
    "    for i in tqdm.tqdm(range(df_topics.shape[0])):\n",
    "        HenID = df_topics.iloc[i]['HenID']\n",
    "        day = df_topics.iloc[i]['day']\n",
    "        topic = df_topics.iloc[i]['topic']\n",
    "        image_name = 'VF_'+str(day).split(' ')[0]+'_'+HenID+'.png'\n",
    "        shutil.copy(os.path.join(path_, image_name), \n",
    "                    os.path.join(path_save, 'timeseries_plot_cluster', str(topic), image_name))\n",
    "        \n",
    "    #LDA nice visual\n",
    "    lda_model = gensim.models.ldamodel.LdaModel.load(os.path.join(path_save, 'model'+str(nbr_topics)+'.gensim'))\n",
    "    lda_display = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary, sort_topics=False)\n",
    "    pyLDAvis.display(lda_display)\n",
    "    pyLDAvis.save_html(lda_display, os.path.join(path_save, str(nbr_topics)+'_lda.html'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Small interpretation of all with 6 topics\n",
    "0: 3 <-> 4 matin\n",
    "1: dehors\n",
    "2: grande transition (trois pas)\n",
    "3: 4 <-> 5\n",
    "4: 3 <-> 2\n",
    "5: 3 <-> 4 après-midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_summary = df_topics.groupby(['topic'])['HenID','day'].agg(lambda x: Counter(x)).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "verify output with one hen and visual ts\n",
    "optimize lda parameter (nbr topics etc)\n",
    "make interpretation and see if its stable with other word definition and lda parameters\n",
    "\n",
    "#TODO: verify one hen\n",
    "id1 = 'hen_1_2017-07-06'\n",
    "print(corpus[10])\n",
    "print(lda[corpus[10]])\n",
    "print(li_documents[10])\n",
    "print(documents[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
