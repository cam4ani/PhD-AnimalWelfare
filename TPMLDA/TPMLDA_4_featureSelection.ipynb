{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\camil\\anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "#basic package\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import math\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "#feature selections\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "#modelling\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "#plot\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf, acf, pacf\n",
    "\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we restrict the time series to one value per 60 seconds \n",
      "we compute the complexity variables each 30 minutes \n",
      "each variables includes the values of at least the last 120.00 minutes (i.e. are using 120.00 values)\n"
     ]
    }
   ],
   "source": [
    "from UTILS import perc_element_dico, corr_from_feature2feature, corr_from_dep2feature\n",
    "import config_mobility as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_extracted_data = config.path_extracted_data\n",
    "path_initial_data = config.path_initial_data\n",
    "id_run = config.id_run\n",
    "title_ = '_LB_newword_def_onesession_morethan4_in11_sess_1_2_3' #'_LB', '_all','_LSL', '_LB_newword_def', '_all_newword_def'\n",
    "path_save_ = os.path.join(path_extracted_data, 'visual', 'predict_KBF', title_)\n",
    "path_save_LDA = os.path.join(path_extracted_data, 'visual', 'LDA', title_)\n",
    "path_save_SNA = os.path.join(path_extracted_data, 'visual', 'SNA', 'correlation_graph','LDA',title_)\n",
    "if not os.path.exists(path_save_SNA):\n",
    "    os.makedirs(path_save_SNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 per bird - KBF prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 540)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HenID</th>\n",
       "      <th>li_severity</th>\n",
       "      <th>nbr_nan</th>\n",
       "      <th>max_severity</th>\n",
       "      <th>positive_rate_in_severity</th>\n",
       "      <th>max_positive_rate_in_severity</th>\n",
       "      <th>value_of_first_positive_rate_in_severity</th>\n",
       "      <th>li_severity_nonan</th>\n",
       "      <th>var_severity</th>\n",
       "      <th>ratio_HealingAndNonhealing</th>\n",
       "      <th>...</th>\n",
       "      <th>('Strength', 'mean')</th>\n",
       "      <th>('Mass', 'tuple')</th>\n",
       "      <th>('Mass', 'nbr')</th>\n",
       "      <th>('Mass', 'variance')</th>\n",
       "      <th>('Mass', 'mean')</th>\n",
       "      <th>('Width', 'tuple')</th>\n",
       "      <th>('Width', 'nbr')</th>\n",
       "      <th>('Width', 'variance')</th>\n",
       "      <th>('Width', 'mean')</th>\n",
       "      <th>hybrid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hen_100</td>\n",
       "      <td>[0.0, 0.0, 0.0, 4.5, 4.3, 3.0, 8.5, 8.6, 8.6, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>[4.5, 5.5, 0.09999999999999964, 0.099999999999...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>[0.0, 0.0, 0.0, 4.5, 4.3, 3.0, 8.5, 8.6, 8.6, ...</td>\n",
       "      <td>13.076529</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>42.750000</td>\n",
       "      <td>(59.6, 58.6, 67.7, 66.9)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.015000</td>\n",
       "      <td>63.200000</td>\n",
       "      <td>(0.3, 0.36, 0.3, 0.32)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hen_101</td>\n",
       "      <td>[3.0, 2.9, 2.6, 5.1, 6.1, 6.2, 9.8, 9.4, 9.8, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>[2.4999999999999996, 1.0, 0.10000000000000053,...</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>[3.0, 2.9, 2.6, 5.1, 6.1, 6.2, 9.8, 9.4, 9.8, ...</td>\n",
       "      <td>8.417025</td>\n",
       "      <td>0.118421</td>\n",
       "      <td>...</td>\n",
       "      <td>48.600000</td>\n",
       "      <td>(57.4, 58.2, 58.3, 56.0, 55.6)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.240000</td>\n",
       "      <td>57.100000</td>\n",
       "      <td>(0.31, 0.3, 0.26, 0.31, 0.27)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hen_102</td>\n",
       "      <td>[0.0, 0.0, 0.0, 2.0, 4.3, 3.2, 2.8, 2.9, 3.2, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>[2.0, 2.3, 0.10000000000000009, 0.300000000000...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 2.0, 4.3, 3.2, 2.8, 2.9, 3.2, ...</td>\n",
       "      <td>2.660165</td>\n",
       "      <td>0.393443</td>\n",
       "      <td>...</td>\n",
       "      <td>49.666667</td>\n",
       "      <td>(64.0, 63.6, 60.4)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.595556</td>\n",
       "      <td>62.666667</td>\n",
       "      <td>(0.34, 0.33, 0.36)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.343333</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 540 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HenID                                        li_severity  nbr_nan  \\\n",
       "0  hen_100  [0.0, 0.0, 0.0, 4.5, 4.3, 3.0, 8.5, 8.6, 8.6, ...        0   \n",
       "1  hen_101  [3.0, 2.9, 2.6, 5.1, 6.1, 6.2, 9.8, 9.4, 9.8, ...        0   \n",
       "2  hen_102  [0.0, 0.0, 0.0, 2.0, 4.3, 3.2, 2.8, 2.9, 3.2, ...        0   \n",
       "\n",
       "   max_severity                          positive_rate_in_severity  \\\n",
       "0           8.7  [4.5, 5.5, 0.09999999999999964, 0.099999999999...   \n",
       "1           9.8  [2.4999999999999996, 1.0, 0.10000000000000053,...   \n",
       "2           4.6  [2.0, 2.3, 0.10000000000000009, 0.300000000000...   \n",
       "\n",
       "   max_positive_rate_in_severity  value_of_first_positive_rate_in_severity  \\\n",
       "0                            5.5                                       4.5   \n",
       "1                            3.6                                       2.5   \n",
       "2                            2.3                                       2.0   \n",
       "\n",
       "                                   li_severity_nonan  var_severity  \\\n",
       "0  [0.0, 0.0, 0.0, 4.5, 4.3, 3.0, 8.5, 8.6, 8.6, ...     13.076529   \n",
       "1  [3.0, 2.9, 2.6, 5.1, 6.1, 6.2, 9.8, 9.4, 9.8, ...      8.417025   \n",
       "2  [0.0, 0.0, 0.0, 2.0, 4.3, 3.2, 2.8, 2.9, 3.2, ...      2.660165   \n",
       "\n",
       "   ratio_HealingAndNonhealing   ...    ('Strength', 'mean')  \\\n",
       "0                    0.166667   ...               42.750000   \n",
       "1                    0.118421   ...               48.600000   \n",
       "2                    0.393443   ...               49.666667   \n",
       "\n",
       "                ('Mass', 'tuple')  ('Mass', 'nbr')  ('Mass', 'variance')  \\\n",
       "0        (59.6, 58.6, 67.7, 66.9)              4.0             17.015000   \n",
       "1  (57.4, 58.2, 58.3, 56.0, 55.6)              5.0              1.240000   \n",
       "2              (64.0, 63.6, 60.4)              3.0              2.595556   \n",
       "\n",
       "   ('Mass', 'mean')             ('Width', 'tuple')  ('Width', 'nbr')  \\\n",
       "0         63.200000         (0.3, 0.36, 0.3, 0.32)               3.0   \n",
       "1         57.100000  (0.31, 0.3, 0.26, 0.31, 0.27)               4.0   \n",
       "2         62.666667             (0.34, 0.33, 0.36)               3.0   \n",
       "\n",
       "   ('Width', 'variance')  ('Width', 'mean')  hybrid  \n",
       "0               0.000600           0.320000      LB  \n",
       "1               0.000440           0.290000      LB  \n",
       "2               0.000156           0.343333      LB  \n",
       "\n",
       "[3 rows x 540 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = read.csv(\"data-marketing-budget-12mo.csv\", header=T, colClasses = c(\"numeric\", \"numeric\", \"numeric\"))\n",
    "df_modelling = pd.read_csv(os.path.join(path_save_,'df_modelling.csv'), sep=\";\")\n",
    "print(df_modelling.shape)\n",
    "df_modelling.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lets compute the \"correlations\" between some output variables and some info from LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714 378 54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['sess_1_day0_k2_topic_proba', 'sess_1_day0_k2_topic_0_proba'],\n",
       " ['sess_1_day0_k2_topic', 'sess_1_day0_k3_topic'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_var = [x for x in df_modelling.columns if ('sess_1' in x) & \\\n",
    "          (('day0' in x) | ('day1' in x) | ('day2' in x)) & \\\n",
    "          ('topic_proba' not in x)]\n",
    "li_var = [x for x in df_modelling.columns if 'sess_' in x]\n",
    "li_cat = [x for x in li_var if x.endswith('_topic')]\n",
    "li_cont = [x for x in li_var if (x not in li_cat) & ('proba' in x)]\n",
    "print(len(li_var), len(li_cont), len(li_cat))\n",
    "li_cont[0:2], li_cat[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['max_severity',\n",
       " 'max_positive_rate_in_severity',\n",
       " 'value_of_first_positive_rate_in_severity',\n",
       " 'var_severity',\n",
       " 'ratio_HealingAndNonhealing',\n",
       " \"('Strength', 'variance')\",\n",
       " \"('Strength', 'mean')\",\n",
       " \"('Mass', 'variance')\",\n",
       " \"('Mass', 'mean')\",\n",
       " \"('Width', 'variance')\",\n",
       " \"('Width', 'mean')\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_init = ['HenID', 'li_severity', 'nbr_nan','li_severity_nonan','positive_rate_in_severity',\n",
    "           \"('Strength', 'tuple')\", \"('Strength', 'nbr')\", \"('Mass', 'tuple')\", \"('Mass', 'nbr')\", \n",
    "           \"('Width', 'tuple')\", \"('Width', 'nbr')\", 'hybrid','ratio_HealingAndNonhealing_round']\n",
    "print(len(li_init))\n",
    "li_output = [x for x in df_modelling.columns if (x not in li_init) & (x not in li_var)]\n",
    "print(len(li_output))\n",
    "#li_output = ['max_severity']\n",
    "li_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:11<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "There is 0 potential issues, and 282 significant correlation\n",
      "(282, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>val_spear</th>\n",
       "      <th>pval_spear</th>\n",
       "      <th>val_pers</th>\n",
       "      <th>pval_pers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>max_severity</td>\n",
       "      <td>sess_1_day0_k6_topic_3_proba</td>\n",
       "      <td>-0.089281</td>\n",
       "      <td>0.516847</td>\n",
       "      <td>-0.285150</td>\n",
       "      <td>0.034843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max_severity</td>\n",
       "      <td>sess_1_day0_k7_topic_4_proba</td>\n",
       "      <td>-0.275005</td>\n",
       "      <td>0.042152</td>\n",
       "      <td>-0.333231</td>\n",
       "      <td>0.012917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max_severity</td>\n",
       "      <td>sess_1_day0_k8_topic_2_proba</td>\n",
       "      <td>-0.191341</td>\n",
       "      <td>0.161694</td>\n",
       "      <td>-0.295892</td>\n",
       "      <td>0.028281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           var1                          var2  val_spear  pval_spear  \\\n",
       "0  max_severity  sess_1_day0_k6_topic_3_proba  -0.089281    0.516847   \n",
       "1  max_severity  sess_1_day0_k7_topic_4_proba  -0.275005    0.042152   \n",
       "2  max_severity  sess_1_day0_k8_topic_2_proba  -0.191341    0.161694   \n",
       "\n",
       "   val_pers  pval_pers  \n",
       "0 -0.285150   0.034843  \n",
       "1 -0.333231   0.012917  \n",
       "2 -0.295892   0.028281  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_significance = corr_from_dep2feature(li_output, li_cont, df_modelling, p_val=0.05)\n",
    "df_significance.to_csv(os.path.join(path_save_LDA, 'correlation_cont_cont.csv'),sep=';')\n",
    "print(df_significance.shape)\n",
    "df_significance.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "There is 0 potential issues, and 213 significant correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████▌                                                                           | 1/11 [00:00<00:09,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "There is 0 potential issues, and 155 significant correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████████████                                                                    | 2/11 [00:01<00:08,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "There is 0 potential issues, and 166 significant correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██████████████████████▋                                                            | 3/11 [00:02<00:07,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "There is 0 potential issues, and 542 significant correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|██████████████████████████████▏                                                    | 4/11 [00:07<00:13,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "There is 0 potential issues, and 174 significant correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████████▋                                             | 5/11 [00:08<00:10,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "There is 0 potential issues, and 127 significant correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████████████████████████▎                                     | 6/11 [00:09<00:07,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "There is 0 potential issues, and 98 significant correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████████▊                              | 7/11 [00:09<00:04,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "There is 0 potential issues, and 65 significant correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|████████████████████████████████████████████████████████████▎                      | 8/11 [00:10<00:02,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "There is 0 potential issues, and 46 significant correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████████████████████████████████████▉               | 9/11 [00:10<00:01,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "There is 0 potential issues, and 321 significant correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|██████████████████████████████████████████████████████████████████████████▌       | 10/11 [00:12<00:01,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "There is 0 potential issues, and 158 significant correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:13<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>val_spear</th>\n",
       "      <th>pval_spear</th>\n",
       "      <th>val_pers</th>\n",
       "      <th>pval_pers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sess_1_day0_k8_topic_4_proba</td>\n",
       "      <td>sess_1_day0_k9_topic_5_proba</td>\n",
       "      <td>0.345751</td>\n",
       "      <td>9.722721e-03</td>\n",
       "      <td>0.412650</td>\n",
       "      <td>1.743317e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sess_1_day0_k9_topic_5_proba</td>\n",
       "      <td>sess_1_day0_k9_topic_8_proba</td>\n",
       "      <td>-0.633562</td>\n",
       "      <td>2.084403e-07</td>\n",
       "      <td>-0.609814</td>\n",
       "      <td>7.732939e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sess_1_day0_k9_topic_8_proba</td>\n",
       "      <td>sess_1_day4_k5_topic_proba</td>\n",
       "      <td>0.246655</td>\n",
       "      <td>7.217384e-02</td>\n",
       "      <td>0.365586</td>\n",
       "      <td>6.559134e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           var1                          var2  val_spear  \\\n",
       "0  sess_1_day0_k8_topic_4_proba  sess_1_day0_k9_topic_5_proba   0.345751   \n",
       "1  sess_1_day0_k9_topic_5_proba  sess_1_day0_k9_topic_8_proba  -0.633562   \n",
       "2  sess_1_day0_k9_topic_8_proba    sess_1_day4_k5_topic_proba   0.246655   \n",
       "\n",
       "     pval_spear  val_pers     pval_pers  \n",
       "0  9.722721e-03  0.412650  1.743317e-03  \n",
       "1  2.084403e-07 -0.609814  7.732939e-07  \n",
       "2  7.217384e-02  0.365586  6.559134e-03  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_df = []\n",
    "for x,df_ in tqdm.tqdm(df_significance.groupby(['var1'])):\n",
    "    li = df_['var2'].tolist()\n",
    "    df_feature_feature = corr_from_feature2feature(li, df_modelling, p_val=0.05)\n",
    "    li_df.append(df_feature_feature)\n",
    "df_feature_feature = pd.concat(li_df)\n",
    "df_feature_feature.to_csv(os.path.join(path_save_LDA, 'correlation_feature_feature.csv'),sep=';')\n",
    "print(df_feature_feature.shape)\n",
    "df_feature_feature.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>val_spear</th>\n",
       "      <th>pval_spear</th>\n",
       "      <th>val_pers</th>\n",
       "      <th>pval_pers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sess_1_day0_k8_topic_4_proba</td>\n",
       "      <td>sess_1_day0_k9_topic_5_proba</td>\n",
       "      <td>0.345751</td>\n",
       "      <td>9.722721e-03</td>\n",
       "      <td>0.412650</td>\n",
       "      <td>1.743317e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sess_1_day0_k9_topic_5_proba</td>\n",
       "      <td>sess_1_day0_k9_topic_8_proba</td>\n",
       "      <td>-0.633562</td>\n",
       "      <td>2.084403e-07</td>\n",
       "      <td>-0.609814</td>\n",
       "      <td>7.732939e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sess_1_day0_k9_topic_8_proba</td>\n",
       "      <td>sess_1_day4_k5_topic_proba</td>\n",
       "      <td>0.246655</td>\n",
       "      <td>7.217384e-02</td>\n",
       "      <td>0.365586</td>\n",
       "      <td>6.559134e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           var1                          var2  val_spear  \\\n",
       "0  sess_1_day0_k8_topic_4_proba  sess_1_day0_k9_topic_5_proba   0.345751   \n",
       "1  sess_1_day0_k9_topic_5_proba  sess_1_day0_k9_topic_8_proba  -0.633562   \n",
       "2  sess_1_day0_k9_topic_8_proba    sess_1_day4_k5_topic_proba   0.246655   \n",
       "\n",
       "     pval_spear  val_pers     pval_pers  \n",
       "0  9.722721e-03  0.412650  1.743317e-03  \n",
       "1  2.084403e-07 -0.609814  7.732939e-07  \n",
       "2  7.217384e-02  0.365586  6.559134e-03  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(282, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>val_spear</th>\n",
       "      <th>pval_spear</th>\n",
       "      <th>val_pers</th>\n",
       "      <th>pval_pers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>max_severity</td>\n",
       "      <td>sess_1_day0_k6_topic_3_proba</td>\n",
       "      <td>-0.089281</td>\n",
       "      <td>0.516847</td>\n",
       "      <td>-0.285150</td>\n",
       "      <td>0.034843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max_severity</td>\n",
       "      <td>sess_1_day0_k7_topic_4_proba</td>\n",
       "      <td>-0.275005</td>\n",
       "      <td>0.042152</td>\n",
       "      <td>-0.333231</td>\n",
       "      <td>0.012917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max_severity</td>\n",
       "      <td>sess_1_day0_k8_topic_2_proba</td>\n",
       "      <td>-0.191341</td>\n",
       "      <td>0.161694</td>\n",
       "      <td>-0.295892</td>\n",
       "      <td>0.028281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           var1                          var2  val_spear  pval_spear  \\\n",
       "0  max_severity  sess_1_day0_k6_topic_3_proba  -0.089281    0.516847   \n",
       "1  max_severity  sess_1_day0_k7_topic_4_proba  -0.275005    0.042152   \n",
       "2  max_severity  sess_1_day0_k8_topic_2_proba  -0.191341    0.161694   \n",
       "\n",
       "   val_pers  pval_pers  \n",
       "0 -0.285150   0.034843  \n",
       "1 -0.333231   0.012917  \n",
       "2 -0.295892   0.028281  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(544, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>val_spear</th>\n",
       "      <th>pval_spear</th>\n",
       "      <th>val_pers</th>\n",
       "      <th>pval_pers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>max_severity</td>\n",
       "      <td>sess_1_day0_k6_topic_3_proba</td>\n",
       "      <td>-0.089281</td>\n",
       "      <td>0.516847</td>\n",
       "      <td>-0.285150</td>\n",
       "      <td>0.034843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max_severity</td>\n",
       "      <td>sess_1_day0_k7_topic_4_proba</td>\n",
       "      <td>-0.275005</td>\n",
       "      <td>0.042152</td>\n",
       "      <td>-0.333231</td>\n",
       "      <td>0.012917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max_severity</td>\n",
       "      <td>sess_1_day0_k8_topic_2_proba</td>\n",
       "      <td>-0.191341</td>\n",
       "      <td>0.161694</td>\n",
       "      <td>-0.295892</td>\n",
       "      <td>0.028281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           var1                          var2  val_spear  pval_spear  \\\n",
       "0  max_severity  sess_1_day0_k6_topic_3_proba  -0.089281    0.516847   \n",
       "1  max_severity  sess_1_day0_k7_topic_4_proba  -0.275005    0.042152   \n",
       "2  max_severity  sess_1_day0_k8_topic_2_proba  -0.191341    0.161694   \n",
       "\n",
       "   val_pers  pval_pers  \n",
       "0 -0.285150   0.034843  \n",
       "1 -0.333231   0.012917  \n",
       "2 -0.295892   0.028281  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_feature = pd.read_csv(os.path.join(path_save_LDA, 'correlation_feature_feature.csv'), sep=';', index_col=0)\n",
    "print(df_feature_feature.shape)\n",
    "display(df_feature_feature.head(3))\n",
    "\n",
    "df_significance = pd.read_csv(os.path.join(path_save_LDA, 'correlation_cont_cont.csv'), sep=';', index_col=0)\n",
    "print(df_significance.shape)\n",
    "display(df_significance.head(3))\n",
    "\n",
    "df_corr = pd.concat([df_significance, df_feature_feature])\n",
    "print(df_corr.shape)\n",
    "df_corr.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "dico_nodename_attribute = {x:{'type_':'feature'} for x in set(list(df_corr['var1'])+list(df_corr['var2']))}\n",
    "for x in li_output:\n",
    "    dico_nodename_attribute[x] = {'type_':'dependant'}\n",
    "for x in dico_nodename_attribute.keys():\n",
    "    if 'day' in x:\n",
    "        dico_nodename_attribute[x]['day'] = int(x.split('day')[1].split('_')[0])\n",
    "        dico_nodename_attribute[x]['sess'] = int(x.split('_')[1])\n",
    "        dico_nodename_attribute[x]['k'] = int(x.split('_k')[1].split('_')[0])\n",
    "print(len(dico_nodename_attribute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute a graph of correlations to make it easier\n",
    "def correlationGraph(df_corr, dico_nodename_attribute, path_save_SNA, name_='', p_val_spear=0.01, p_val_pers=0.01, \n",
    "                     condition_type='or'):\n",
    "    '''From a dataframe of correlation, compute the associated graph, where\n",
    "    input: dataframe with the following columns: var1, var2, val_spear, pval_spear, val_pers, pval_pers\n",
    "    link: if two nodes have a significant p-values (p_val in parameters will be used)\n",
    "    node: each var1, var2*\n",
    "    edge attribute: weight_coeff: correlation coefficient, weight_pval: correlation p-value\n",
    "    node attribute: type: feature/dependant\n",
    "    condition_type: can only be \"and\", or \"or\", it will raise an error otherwise '''\n",
    "\n",
    "    #small test\n",
    "    if condition_type not in ['and','or']:\n",
    "        print('ERROR: condition_type: can only be \"and\", or \"or\"')\n",
    "        sys.exit()\n",
    "        \n",
    "    #initialise graph\n",
    "    G = nx.Graph() #only one edge between two nodes (otherwise use MultiGraph)\n",
    "\n",
    "    #add nodes with its attribute\n",
    "    dico_name_id = {}\n",
    "    dico_n_a = {}\n",
    "    for i, (n, dico_attribute) in enumerate(dico_nodename_attribute.items()):\n",
    "        dico_name_id[n] = i\n",
    "        dico_n_a[i] = dico_attribute.copy()\n",
    "        dico_n_a[i]['name'] = n\n",
    "        G.add_node(i) \n",
    "    nx.set_node_attributes(G, dico_n_a)\n",
    "    print(len(dico_n_a))\n",
    "    print(dico_n_a[3])\n",
    "  \n",
    "    #add edges\n",
    "    li_edges = [] \n",
    "    for i in range(df_corr.shape[0]):\n",
    "        x = df_corr.iloc[i]\n",
    "        n1 = x['var1'] ; n2 = x['var2'] ; pval_spear = x['pval_spear'] ; pval_pers = x['pval_pers']\n",
    "        #if significant, link it\n",
    "        if ((pval_spear<=p_val_spear) & (pval_pers<=p_val_pers) & (condition_type=='and')) |\\\n",
    "           (((pval_spear<=p_val_spear) | (pval_pers<=p_val_pers)) & (condition_type=='or')):\n",
    "            li_edges.append((dico_name_id[n1], dico_name_id[n2], {'pval_spear':float(round(pval_spear,3)),\n",
    "                                      'pval_pers':float(round(pval_pers,3)), \n",
    "                                      'val_spear':float(round(x['val_spear'],3)), \n",
    "                                      'val_pers':float(round(x['val_pers'],3))}))\n",
    "    #add edges with attributes\n",
    "    G.add_edges_from(li_edges)\n",
    "    print(len(li_edges))\n",
    "    #write G (networkX graph) in GEXF format for gephi\n",
    "    nx.write_gexf(G, os.path.join(path_save_SNA, name_+'_'+condition_type+'_'+'.gexf'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "{'type_': 'feature', 'day': 2, 'sess': 1, 'k': 10, 'name': 'sess_1_day2_k10_topic_7_proba'}\n",
      "275\n"
     ]
    }
   ],
   "source": [
    "name_ = ''\n",
    "if len(li_output)==1:\n",
    "    name_ = li_output[0]\n",
    "correlationGraph(df_corr, dico_nodename_attribute, path_save_SNA=path_save_SNA, \n",
    "                 name_=name_, p_val_spear=0.01, p_val_pers=0.01, \n",
    "                 condition_type='or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TODO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2494f9b4f745>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTODO\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'TODO' is not defined"
     ]
    }
   ],
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 1 per bird per day \n",
    "age is dependant on the MLPS so think about this first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "\n",
    "#linear regression\n",
    "df = df.dropna(subset=['f_13','total_kb_emails','ping_pong_perc','chi2_distance_max','team_location_diversity',\n",
    "        'Leistungsbeurteilung 2015_number','synchronical_duration_perc','num_sent_t','betweenness_full'])\n",
    "\n",
    "\n",
    "X = df[['f_13']]\n",
    "y = df[['total_kb_emails','ping_pong_perc','chi2_distance_max','team_location_diversity',\n",
    "        'Leistungsbeurteilung 2015_number','synchronical_duration_perc','num_sent_t','betweenness_full']]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression(normalize=True)\n",
    "\n",
    "# Train the model\n",
    "regr.fit(X, y)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % regr.score(X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelling.isna().sum()\n",
    "#duration_zone_1!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelling.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'max_severity' \n",
    "\n",
    "li_mlp = ['duration_zone_1', 'duration_zone_2','duration_zone_3', 'duration_zone_4', 'duration_zone_5',\n",
    "           'FirstTimestamp_zone_1_h', 'FirstTimestamp_zone_2_h',\n",
    "           'FirstTimestamp_zone_3_h', 'FirstTimestamp_zone_4_h',\n",
    "           'FirstTimestamp_zone_5_h', 'Total_number_zone','Max_duration_zones',\n",
    "           'Max_duration', 'Min_duration', 'Median_duration', 'Average_duration',\n",
    "           'Variance_duration', 'dico_zone_duration', 'Total_number_transition',\n",
    "           'nbr_bouts_zone_5', 'nbr_bouts_zone_4',\n",
    "           'nbr_bouts_zone_3', 'nbr_bouts_zone_2', 'nbr_bouts_zone_1',\n",
    "           'Max_duration_zone_4']\n",
    "li_cpx = ['SampEnt_order2', 'SampEnt_zone_1','SampEnt_zone_2', 'SampEnt_zone_3', 'SampEnt_zone_4', 'SampEnt_zone_5',\n",
    "          'RunSampEnt_onLastTsOfEachLevel', 'RunDistEnt_onLastTsOfEachLevel',\n",
    "          'RunSampEnt_onLastTsOfEachLevel_1', 'RunSampEnt_onLastTsOfEachLevel_2',\n",
    "          'RunSampEnt_onLastTsOfEachLevel_3', 'RunSampEnt_onLastTsOfEachLevel_4',\n",
    "          'RunSampEnt_onLastTsOfEachLevel_5',\n",
    "          'distribution_entropy']    \n",
    "\n",
    "li_choosen_mlp = ['duration_zone_1', 'FirstTimestamp_zone_4_h', 'Median_duration', 'Variance_duration','SampEnt_order2']\n",
    "\n",
    "li_cluster_lda_day1 = [x for x in df_modelling.columns if ('sess_1_day0' in x) & ('k6' in x) & ('proba' in x)\\\n",
    "                       & ('topic_proba' not in x)]\n",
    "li_mlp_day1 = [x for x in df_modelling.columns if ('sess_1_day0' in x) & ('topic' not in x) \\\n",
    "               & any(i in x for i in li_choosen_mlp)]\n",
    "li_var = list(set(['is_LSL'] + li_cluster_lda_day1 + li_mlp_day1))\n",
    "li_var"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Mixed effects models are useful when we have data with more than one source of random variability. For example, an outcome may be measured more than once on the same person (repeated measures taken over time). When we do that we have to account for both within-person and across-person variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_modelling['is_LSL'] = df_modelling['hybrid'].map(lambda x: x=='LSL')\n",
    "g = sns.PairGrid(df_modelling[[x]+li_var], hue='is_LSL')\n",
    "g.map(plt.scatter);\n",
    "plt.savefig(os.path.join(path_save_,id_run+'_scatterplot_var.png'), format='png', dpi=300)\n",
    "#Total_number_transition, duration_zone_2 , SampEnt_zone_4up --> perc_after24 up and perc_before24 down\n",
    "#Max_duration_zone_4, duration_zone_4 up --> erc_after24 down and perc_before24 up\n",
    "#??Variance_duration?? fucked up  --'SampEnt_order2','SampEnt_zone_4' down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_modelling['is_LSL'] = df_modelling['hybrid'].map(lambda x: x=='LSL')\n",
    "for day in range(0,6):\n",
    "    print(day)\n",
    "    for t in range(min_topic,max_topic+1):\n",
    "        df_ = df_modelling[['max_severity','is_LSL']+['sess_1_day'+str(day)+'_k'+str(t)+'_topic_'+str(n)+'_proba' for n in range(t)]]\n",
    "        df_ = pd.melt(df_,id_vars=['max_severity','is_LSL']) \n",
    "        sns.lmplot(x=\"max_severity\", y='value', hue='is_LSL', col='variable', data=df_, \n",
    "                   ci=95); #categorical x: x_estimator=np.mean, x_bins=5, x_ci=95, x_estimator=np.mean)\n",
    "        #order: order=2 or lowess=True\n",
    "        #logistic=True: if y is binary\n",
    "        #truncate=True\n",
    "        #robust=True\n",
    "        #x_partial=is_LSL\n",
    "        plt.savefig(os.path.join(path_save_,id_run+'sess_1_day'+str(day)+'k_'+str(t)+'_topic_proba.png'), format='png', dpi=300)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_var = ['sess_1_day0_k6_topic_5_proba',\n",
    " 'sess_1_day0_Variance_duration',\n",
    " 'sess_1_day0_Median_duration',\n",
    " 'sess_1_day0_duration_zone_1',\n",
    " 'sess_1_day0_k6_topic_0_proba']\n",
    "li_var = ['sess_1_day0_k6_topic_5_proba', 'sess_1_day0_Variance_duration', 'sess_1_day0_duration_zone_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLM\n",
    "formula = x+' ~ '+'+'.join(li_var)\n",
    "min_sample_Size = 50 + 8*len(li_var)\n",
    "df_mod = df_modelling[(~df_modelling[x].isnull())&(df_modelling['is_LSL']==False)][li_var+[x,'HenID']].copy()\n",
    "display(df_mod.head(3))\n",
    "print(min_sample_Size, df_mod.shape)\n",
    "if min_sample_Size>=df_mod.shape[0]:\n",
    "    print('WARNING: reduce the number of dependant variables or augment your number of observation')\n",
    "#    sys.exit()\n",
    "    \n",
    "mod = smf.glm(formula=formula, data=df_modelling).fit() #, family=sm.families.Binomial()\n",
    "mod.summary()\n",
    "#--> mass seems to increase with higher percentgae of shift 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sst_val = sum(map(lambda x: np.power(x,2),y-np.mean(y))) \n",
    "sse_val = sum(map(lambda x: np.power(x,2),m1.resid_response)) \n",
    "r2 = 1.0 - sse_val/sst_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RandomForest\n",
    "x = 'max_severity' \n",
    "\n",
    "li_mlp = ['duration_zone_1', 'duration_zone_2','duration_zone_3', 'duration_zone_4', 'duration_zone_5',\n",
    "           'FirstTimestamp_zone_1_h', 'FirstTimestamp_zone_2_h',\n",
    "           'FirstTimestamp_zone_3_h', 'FirstTimestamp_zone_4_h',\n",
    "           'FirstTimestamp_zone_5_h', 'Total_number_zone','Max_duration_zones',\n",
    "           'Max_duration', 'Min_duration', 'Median_duration', 'Average_duration',\n",
    "           'Variance_duration', 'dico_zone_duration', 'Total_number_transition',\n",
    "           'nbr_bouts_zone_5', 'nbr_bouts_zone_4',\n",
    "           'nbr_bouts_zone_3', 'nbr_bouts_zone_2', 'nbr_bouts_zone_1',\n",
    "           'Max_duration_zone_4']\n",
    "li_cpx = ['SampEnt_order2', 'SampEnt_zone_1','SampEnt_zone_2', 'SampEnt_zone_3', 'SampEnt_zone_4', 'SampEnt_zone_5',\n",
    "          'RunSampEnt_onLastTsOfEachLevel', 'RunDistEnt_onLastTsOfEachLevel',\n",
    "          'RunSampEnt_onLastTsOfEachLevel_1', 'RunSampEnt_onLastTsOfEachLevel_2',\n",
    "          'RunSampEnt_onLastTsOfEachLevel_3', 'RunSampEnt_onLastTsOfEachLevel_4',\n",
    "          'RunSampEnt_onLastTsOfEachLevel_5',\n",
    "          'distribution_entropy']    \n",
    "\n",
    "li_choosen_mlp = ['duration_zone_1', 'FirstTimestamp_zone_4_h', 'Median_duration', 'Variance_duration','SampEnt_order2']\n",
    "\n",
    "li_cluster_lda_day1 = [x for x in df_modelling.columns if ('sess_1_day0' in x) & ('k2' in x) & ('proba' in x)\\\n",
    "                       & ('topic_proba' not in x)]\n",
    "li_mlp_day1 = [x for x in df_modelling.columns if ('sess_1_day0' in x) & ('topic' not in x) \\\n",
    "               & any(i in x for i in li_choosen_mlp)]\n",
    "li_var = list(set(li_cluster_lda_day1 + li_mlp_day1)) #['is_LSL'] + \n",
    "li_var = li_cluster_lda_day1\n",
    "li_var\n",
    "\n",
    "df_ = df_modelling[(~df_modelling[x].isnull()) & (~df_modelling['sess_1_day0_k4_topic_3_proba'].isnull())][li_var+[x]]\n",
    "print(df_.shape, df_modelling.shape)\n",
    "df_['sess_1_day0_k2_topic_1_proba'] = df_['sess_1_day0_k2_topic_1_proba'].map(lambda x: round(x,2))\n",
    "df_['sess_1_day0_k2_topic_0_proba'] = df_['sess_1_day0_k2_topic_0_proba'].map(lambda x: round(x,2))\n",
    "df_['max_severity'] = df_['max_severity'].map(lambda x: round(x/10,1))\n",
    "\n",
    "display(df_)\n",
    "print(df_.isna().sum())\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_modelling[li_var], df_modelling[x], test_size=0.2, random_state=40)\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)\n",
    "sel = SelectFromModel(RandomForestClassifier(n_estimators=20))\n",
    "sel.fit(X_train, y_train)\n",
    "sel.get_support()\n",
    "selected_feat= X_train.columns[(sel.get_support())]\n",
    "len(selected_feat)\n",
    "\n",
    "df_test.fillna(df_test.mean())\n",
    "X_test = df_test.values  \n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
