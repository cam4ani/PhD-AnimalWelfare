{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Reshape, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "#from tensorflow.contrib.factorization.python.ops import clustering_ops\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys \n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import os\n",
    "import cv2 \n",
    "import glob\n",
    "import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "#from scipy.misc import imresize\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "from UTILS import cleaning_processing, HenDailyVariable_Origins, plot_scikit_lda, explained_var, sampen, kmeans_clustering,\\\n",
    "time_series_henColumn_tsRow, FB_daily\n",
    "import config_origins as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#initialise parameters\n",
    "path_extracted_data = config.path_extracted_data\n",
    "path_initial_data = config.path_initial_data\n",
    "id_run = config.id_run\n",
    "pal_ = config.pal_\n",
    "dico_matching = config.dico_matching\n",
    "dico_zone_order = config.dico_zone_order\n",
    "dico_night_hour = config.dico_night_hour\n",
    "dico_garden_opening_hour = config.dico_garden_opening_hour\n",
    "path_extracted_data_visual_cons = os.path.join(path_extracted_data,'visual','IndividualConsistency')\n",
    "#create a director if not existing\n",
    "if not os.path.exists(path_extracted_data_visual_cons):\n",
    "    os.makedirs(path_extracted_data_visual_cons)\n",
    "\n",
    "path_extracted_data_visual_cons_img = os.path.join(path_extracted_data_visual_cons,'visual','MLPimages')\n",
    "#create a director if not existing\n",
    "if not os.path.exists(path_extracted_data_visual_cons_img):\n",
    "    os.makedirs(path_extracted_data_visual_cons_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1186950, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HenID</th>\n",
       "      <th>PenID</th>\n",
       "      <th>system</th>\n",
       "      <th>Zone</th>\n",
       "      <th>model_prediction</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>next_record_date</th>\n",
       "      <th>duration</th>\n",
       "      <th>previous_record_date</th>\n",
       "      <th>previous_duration</th>\n",
       "      <th>next_zone</th>\n",
       "      <th>previous_zone</th>\n",
       "      <th>previous_previous_zone</th>\n",
       "      <th>correction_is_consecutive_equal_initial_zone</th>\n",
       "      <th>is_WG_open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hen_111</td>\n",
       "      <td>pen11</td>\n",
       "      <td>10 - 12</td>\n",
       "      <td>3_Zone</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-09-30 00:01:25</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>2020-09-30 00:01:38.000</td>\n",
       "      <td>0 days 00:00:13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2_Zone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hen_111</td>\n",
       "      <td>pen11</td>\n",
       "      <td>10 - 12</td>\n",
       "      <td>2_Zone</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-09-30 00:01:38</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>2020-09-30 00:14:38.000</td>\n",
       "      <td>0 days 00:13:00</td>\n",
       "      <td>2020-09-30 00:01:25.000</td>\n",
       "      <td>0 days 00:00:13</td>\n",
       "      <td>2_Zone</td>\n",
       "      <td>3_Zone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hen_147</td>\n",
       "      <td>pen8</td>\n",
       "      <td>8 - 9</td>\n",
       "      <td>5_Zone</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-09-30 00:30:49</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>2020-09-30 00:33:26.000</td>\n",
       "      <td>0 days 00:02:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5_Zone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     HenID  PenID   system    Zone  model_prediction           Timestamp  \\\n",
       "0  hen_111  pen11  10 - 12  3_Zone               1.0 2020-09-30 00:01:25   \n",
       "1  hen_111  pen11  10 - 12  2_Zone               1.0 2020-09-30 00:01:38   \n",
       "2  hen_147   pen8    8 - 9  5_Zone               1.0 2020-09-30 00:30:49   \n",
       "\n",
       "        date         next_record_date         duration  \\\n",
       "0 2020-09-30  2020-09-30 00:01:38.000  0 days 00:00:13   \n",
       "1 2020-09-30  2020-09-30 00:14:38.000  0 days 00:13:00   \n",
       "2 2020-09-30  2020-09-30 00:33:26.000  0 days 00:02:37   \n",
       "\n",
       "      previous_record_date previous_duration next_zone previous_zone  \\\n",
       "0                      NaN               NaN    2_Zone           NaN   \n",
       "1  2020-09-30 00:01:25.000   0 days 00:00:13    2_Zone        3_Zone   \n",
       "2                      NaN               NaN    5_Zone           NaN   \n",
       "\n",
       "  previous_previous_zone  correction_is_consecutive_equal_initial_zone  \\\n",
       "0                    NaN                                         False   \n",
       "1                    NaN                                         False   \n",
       "2                    NaN                                         False   \n",
       "\n",
       "   is_WG_open  \n",
       "0       False  \n",
       "1       False  \n",
       "2       False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLP\n",
    "df = pd.read_csv(os.path.join(path_extracted_data, id_run+'_CLEANEDDATA.csv'), sep=';', parse_dates=['Timestamp', 'date']) \n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the focalBirdinfo, you have 159 ative tags\n",
      "(22905, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HenID</th>\n",
       "      <th>R-Pen</th>\n",
       "      <th>InitialStartDate</th>\n",
       "      <th>ShouldBeExcluded</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>FocalLegringID</th>\n",
       "      <th>Expected % of wrong class</th>\n",
       "      <th>Expected chance of wrong treatment</th>\n",
       "      <th>comments</th>\n",
       "      <th>...</th>\n",
       "      <th>TagID</th>\n",
       "      <th>EPI GENETIC TIMESTAMP</th>\n",
       "      <th>HA 23-11-2020</th>\n",
       "      <th>comportement</th>\n",
       "      <th>weight 23-11-2020</th>\n",
       "      <th>HA 04-01-2021</th>\n",
       "      <th>weight 04-01-2021</th>\n",
       "      <th>HA 01-02-21</th>\n",
       "      <th>weight 01-02-21</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.06.2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-29</td>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>tag_105</td>\n",
       "      <td>9-date:2-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1696,5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1787,8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1800,9</td>\n",
       "      <td>2020-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.06.2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-29</td>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>tag_105</td>\n",
       "      <td>9-date:2-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1696,5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1787,8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1800,9</td>\n",
       "      <td>2020-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.06.2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-29</td>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>tag_105</td>\n",
       "      <td>9-date:2-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1696,5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1787,8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1800,9</td>\n",
       "      <td>2020-10-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HenID  R-Pen InitialStartDate  ShouldBeExcluded  StartDate    EndDate  \\\n",
       "0  hen_1    1.0       10.06.2020               NaN 2020-09-29 2021-02-01   \n",
       "1  hen_1    1.0       10.06.2020               NaN 2020-09-29 2021-02-01   \n",
       "2  hen_1    1.0       10.06.2020               NaN 2020-09-29 2021-02-01   \n",
       "\n",
       "   FocalLegringID Expected % of wrong class  \\\n",
       "0               3                       NaN   \n",
       "1               3                       NaN   \n",
       "2               3                       NaN   \n",
       "\n",
       "  Expected chance of wrong treatment comments  ...    TagID  \\\n",
       "0                                  0      NaN  ...  tag_105   \n",
       "1                                  0      NaN  ...  tag_105   \n",
       "2                                  0      NaN  ...  tag_105   \n",
       "\n",
       "  EPI GENETIC TIMESTAMP HA 23-11-2020 comportement weight 23-11-2020  \\\n",
       "0         9-date:2-2020           NaN          NaN            1696,5   \n",
       "1         9-date:2-2020           NaN          NaN            1696,5   \n",
       "2         9-date:2-2020           NaN          NaN            1696,5   \n",
       "\n",
       "  HA 04-01-2021  weight 04-01-2021 HA 01-02-21 weight 01-02-21       date  \n",
       "0           NaN             1787,8         NaN          1800,9 2020-09-30  \n",
       "1           NaN             1787,8         NaN          1800,9 2020-10-01  \n",
       "2           NaN             1787,8         NaN          1800,9 2020-10-02  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bird info\n",
    "df_FB_daily = FB_daily(config)\n",
    "df_FB_daily['TagID'] = df_FB_daily['TagID'].map(lambda x: 'tag_'+str(x))\n",
    "df_FB_daily['HenID'] = df_FB_daily['HenID'].map(lambda x: 'hen_'+str(x))\n",
    "print(df_FB_daily.shape)\n",
    "df_FB_daily.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>HenID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>[hen_1, hen_2, hen_3, hen_4, hen_5, hen_6, hen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>[hen_1, hen_2, hen_3, hen_4, hen_5, hen_6, hen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>[hen_1, hen_2, hen_3, hen_4, hen_5, hen_6, hen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              HenID\n",
       "0 2020-09-30  [hen_1, hen_2, hen_3, hen_4, hen_5, hen_6, hen...\n",
       "1 2020-10-01  [hen_1, hen_2, hen_3, hen_4, hen_5, hen_6, hen...\n",
       "2 2020-10-02  [hen_1, hen_2, hen_3, hen_4, hen_5, hen_6, hen..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_available_date = df_FB_daily.groupby(['date'])['HenID'].agg(lambda x: list(x)).reset_index()\n",
    "print(df_available_date.shape)\n",
    "df_available_date.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MLP images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pen3\n",
      "in this time series there is 26 hens\n",
      "The initial starting date in over all is: 2020-09-30 02:41:43, and the ending date will be: 2021-02-16 17:25:15\n",
      "But note that birds may have different ending and starting date which should be taken into account when computing variables\n",
      "and after ending the last day at midnight : 2020-09-30 02:41:43, and the ending date will be: 2021-02-16 23:59:59\n",
      "Total running time: 3.17 mn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 140/140 [2:45:21<00:00, 70.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pen9\n",
      "in this time series there is 26 hens\n",
      "The initial starting date in over all is: 2020-09-30 02:58:38, and the ending date will be: 2021-02-16 17:27:50\n",
      "But note that birds may have different ending and starting date which should be taken into account when computing variables\n",
      "and after ending the last day at midnight : 2020-09-30 02:58:38, and the ending date will be: 2021-02-16 23:59:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time: 2.74 mn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 140/140 [45:40<00:00, 19.57s/it]\n"
     ]
    }
   ],
   "source": [
    "#sort by timestamp\n",
    "df.sort_values(['Timestamp'], inplace=True)\n",
    "#use up to the second level only\n",
    "df['Timestamp'] = df['Timestamp'].map(lambda x: dt.datetime(x.year, x.month, x.day, x.hour, x.minute, x.second))\n",
    "#remove the first record\n",
    "df = df.drop_duplicates(subset=['HenID','Timestamp'], keep='last')\n",
    "li_pen_done = ['pen12', 'pen11', 'pen8', 'pen4', 'pen5','pen10','pen3','pen9'] # so that we avoid coputing the time serie\n",
    "li_pen = [v for v in list(df['PenID'].unique()) if v not in li_pen_done]\n",
    "#li_pen.reverse()\n",
    "#existing images\n",
    "li_p = glob.glob(os.path.join(path_extracted_data_visual_cons_img, '*.png'))\n",
    "li_p = [v.split('\\\\')[-1] for v in li_p]\n",
    "for p in li_pen:\n",
    "    print(p)\n",
    "    try:\n",
    "        df_ts = time_series_henColumn_tsRow(df[df['PenID']==p], config, col_ts='Zone', ts_with_all_hen_value=False, save=False, hen_time_series=False)\n",
    "        li_hen = [v for v in df_ts.columns if v.startswith('hen')]\n",
    "        li_date = df_ts['date'].unique()\n",
    "        for d in tqdm.tqdm(li_date):\n",
    "            li_available_hens = df_available_date[df_available_date['date']==d]['HenID'].values[0]\n",
    "            for henID in li_hen:\n",
    "                title = henID+'_'+str(d).split('T')[0]+'.png'\n",
    "                if (title not in li_p) & (henID in li_available_hens):\n",
    "                    li_zones_sec = df_ts[df_ts['date']==d][henID].tolist() \n",
    "                    #should be over the 24h, so the night hour should not change here\n",
    "                    if len(li_zones_sec)==24*60*60:\n",
    "                        M = np.zeros(shape=(max(dico_zone_order.values())+1, len(li_zones_sec))) #+1 car starts from 0\n",
    "                        for zone_, order in dico_zone_order.items():\n",
    "                            #avoid warning:\n",
    "                            if zone_ in set(li_zones_sec):\n",
    "                                M[order][list(np.where(np.array(li_zones_sec)==zone_)[0])] = 1\n",
    "                        #plot and save\n",
    "                        plt.clf() # clears the entire current figure instead of plt.figure() which will create a new one, and hence keeping all figures\n",
    "                        ax = sns.heatmap(M, cbar=False, cmap=sns.light_palette(\"black\", as_cmap=True))  \n",
    "                        plt.xticks([])\n",
    "                        plt.yticks([])\n",
    "                        ax.invert_yaxis() # we want the zone 5 to be up just for our eyes :)\n",
    "                        plt.savefig(os.path.join(path_extracted_data_visual_cons_img, title), format='png')\n",
    "                        plt.close()\n",
    "    except Exception as e:\n",
    "        print('ERROR')\n",
    "        print(e)\n",
    "        print(p,d,henID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TODO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2494f9b4f745>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTODO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'TODO' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 505\n"
     ]
    }
   ],
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "p = 0.1\n",
    "li_image_train = random.sample(li_image, int(len(li_image)*(1-p)))\n",
    "for path_ in tqdm.tqdm(li_image):\n",
    "    image = cv2.imread(path_)\n",
    "    b,g,r = cv2.split(image)           \n",
    "    image = cv2.merge([r,g,b])\n",
    "    if title=='FF':\n",
    "    #resize to have less pixels\n",
    "        image = imresize(image, size=(224,224,3))\n",
    "        #remove surroundings\n",
    "        image = image[7:-17,20:-36]\n",
    "        #normalized\n",
    "        image = image/255.\n",
    "    if title=='representation_color_and_duration':\n",
    "        #remove surroundings\n",
    "        image = image[33:-20,:]         #remove simple the title\n",
    "        image = imresize(image, size=(200,168,3))\n",
    "        #normalized\n",
    "        image = image/255.\n",
    "    if path_ in li_image_train:\n",
    "        x_train.append(image)\n",
    "    else:\n",
    "        x_test.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "pickle.dump(li_image_train, \n",
    "            open(os.path.join(path_save_model_info,'_li_image_train.pkl'), 'wb'))\n",
    "pickle.dump([x for x in li_image if x not in li_image_train], \n",
    "            open(os.path.join(path_save_model_info,'_li_image_test.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(x_test).reshape(len(x_test), len(x_test[0]), len(x_test[0][0]), len(x_test[0][0][0]))\n",
    "x_test = x_test.astype('float32')\n",
    "print(x_test.shape)\n",
    "\n",
    "x_train = np.array(x_train).reshape(len(x_train), len(x_train[0]), len(x_train[0][0]), len(x_train[0][0][0]))\n",
    "x_train = x_train.astype('float32')\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) #transform 2D 28x28 matrix to 3D (28x28x1) matrix\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255 #inputs have to be between [0, 1]\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img_size = x_train.shape[1]\n",
    "total_pixels = img_size * img_size\n",
    "translator_factor = 2\n",
    "translator_layer_size = int(total_pixels/translator_factor)\n",
    "middle_factor = 2\n",
    "middle_layer_size = int(translator_layer_size/middle_factor)\n",
    "\n",
    "inputs = keras.Input(shape=(img_size,img_size,1), name='cat_image')\n",
    "x = layers.Flatten(name = 'flattened_cat')(inputs) #turn image to vector.\n",
    "\n",
    "x = layers.Dense(translator_layer_size, activation='relu', name='encoder')(x)\n",
    "x = layers.Dense(middle_layer_size, activation='relu', name='middle_layer')(x)\n",
    "x = layers.Dense(translator_layer_size, activation='relu', name='decoder')(x)\n",
    "\n",
    "\n",
    "outputs = layers.Dense(total_pixels, activation='relu', name='reconstructed_cat')(x)\n",
    "outputs = layers.Reshape((img_size,img_size,1))(outputs)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    " \n",
    "#1st convolution layer\n",
    "model.add(Conv2D(16, (3, 3) #16 is number of filters and (3, 3) is the size of the filter.\n",
    ", padding='same', input_shape=(x_train.shape[1],x_train.shape[2],x_train.shape[3])))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    " \n",
    "#2nd convolution layer\n",
    "model.add(Conv2D(2,(5, 5), padding='same')) # apply 2 filters sized of (3x3)\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(4,4), padding='same'))\n",
    "\n",
    "#3nd convolution layer\n",
    "#model.add(Conv2D(2,(3, 3), padding='same')) # apply 2 filters sized of (3x3)\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "\n",
    "#here compressed version\n",
    " \n",
    "#4rd convolution layer\n",
    "model.add(Conv2D(2,(5, 5), padding='same')) # apply 2 filters sized of (3x3)\n",
    "model.add(Activation('relu'))\n",
    "model.add(UpSampling2D((4, 4)))\n",
    " \n",
    "#5rd convolution layer\n",
    "model.add(Conv2D(16,(3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "\n",
    "#6rd convolution layer\n",
    "#model.add(Conv2D(16,(3, 3), padding='same'))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(UpSampling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(x_train.shape[3],(3, 3), padding='same'))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = 'test1_'+title\n",
    "path_model = os.path.join(path_save_model_info,id_)\n",
    "if not os.path.exists(path_model):\n",
    "    os.makedirs(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not enough improvment on the val loss, then we will reduce the learning rate, and after that if still no change then \n",
    "#early stopping \n",
    "\n",
    "#The checkpoint only includes the model weights\n",
    "#save the network weights only when there is an improvement in classification accuracy on the validation dataset \n",
    "filepath = os.path.join(path_save_model_info,id_,'weights-improvement-{epoch:02d}-{val_loss:.2f}.h5')\n",
    "#if save_best_only=True, the latest best model according to the quantity monitored will not be overwritten\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_loss', \n",
    "                                             verbose=1, \n",
    "                                             save_best_only=False, \n",
    "                                             mode='max')\n",
    "\n",
    "#to reduce learning rate (new_lr = lr*factor) when a metric has stopped improving\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                              patience=1, #patience before changing learning rate\n",
    "                                              min_delta=0.0001,\n",
    "                                              verbose=1,\n",
    "                                              factor=0.1)\n",
    "\n",
    "#Stop training when a monitored quantity has stopped improving. to avoid overfiting\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               min_delta=0.001, \n",
    "                                               patience=3,\n",
    "                                               verbose=1, \n",
    "                                               mode='auto',\n",
    "                                               baseline=None)\n",
    "\n",
    "#create logs data for tensorboard usage\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=os.path.join(path_log,id_), \n",
    "                                          histogram_freq=0, #no histo\n",
    "                                          write_graph=True, \n",
    "                                          write_grads=False,\n",
    "                                          write_images=True)\n",
    "\n",
    "callbacks_list = [reduce_lr, early_stopping, checkpoint, tensorboard] \n",
    "#then as usuall to load weights to an instantiate model: model.load_weights(\"weights.best.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy') #'adadelta' Adam(lr=0.001)\n",
    "model.fit(x_train, x_train, epochs=100, validation_data=(x_test, x_test), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all the architecture, weights etc\n",
    "model.save(os.path.join(path_save_model_info,id_,'model_all.h5')) \n",
    "#model = load_model(os.path.join(path_save_model_info,id_,'model_all.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once TensorBoard is running, navigate your web browser to localhost:6006 to view the TensorBoard.\n",
    "#tensorboard --logdir=D:\\vm_exchange\\AVIFORUM\\data\\extracted_info_mobility_VF\\visual\\FF_model_info\\FF_model_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restored images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "restored_imgs = model.predict(x_test)\n",
    "for i in range(15):\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.show()\n",
    "    plt.imshow(restored_imgs[i])\n",
    "    plt.show()\n",
    "    print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_model = tf.contrib.learn.KMeansClustering(\n",
    "    7 #num of clusters\n",
    "    , distance_metric = clustering_ops.SQUARED_EUCLIDEAN_DISTANCE\n",
    "    , initial_clusters=tf.contrib.learn.KMeansClustering.RANDOM_INIT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(model.layers[7].output.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)\n",
    "x_test_pp = x_test[0:600].copy()\n",
    "print(x_test_pp.shape)\n",
    "print(model.layers[0].input)\n",
    "print(model.layers[7].input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#layers = len(model.layers)\n",
    "#for i in range(layers):\n",
    "#    print(i, \". \", model.layers[i].output.get_shape())\n",
    "#layer[7] is a compressed representation. It is size of (None, 7, 7, 2). this means 2 different 7x7 sized matrixes. We will \n",
    "#flatten these matrixes.\n",
    "vo = model.layers[7].output.shape\n",
    "get_3rd_layer_output = K.function([model.layers[0].input], [model.layers[7].output])\n",
    "compressed = get_3rd_layer_output([x_test_pp])[0]\n",
    "compressed = compressed.reshape(x_test_pp.shape[0],int(vo[1])*int(vo[2])*int(vo[3]))\n",
    "\n",
    "def train_input_fn():\n",
    "    data = tf.constant(compressed, tf.float32)\n",
    "    return (data, None)\n",
    "\n",
    "unsupervised_model.fit(input_fn=train_input_fn, steps=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all the architecture, weights etc\n",
    "#unsupervised_model.save(os.path.join(path_save_model_info,id_,'unsupervised_model_all.h5')) \n",
    "#model = load_model(os.path.join(path_save_model_info,id_,'model_all.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = unsupervised_model.predict(input_fn=train_input_fn)\n",
    "import scipy.misc\n",
    "id_img = 0\n",
    "for k,i in tqdm.tqdm(enumerate(clusters)):\n",
    "    cluster_id = i['cluster_idx']\n",
    "    id_img = id_img+1\n",
    "    features = x_test_pp[k]\n",
    "    path_class_ = os.path.join(path_class, str(cluster_id))\n",
    "    if not os.path.exists(path_class_):\n",
    "        os.makedirs(path_class_)\n",
    "    scipy.misc.imsave(os.path.join(path_class_,str(id_img)+'.png'), features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for j in range(0, x_train.shape[0], 500):\n",
    "    X = x_train[j-500,j]\n",
    "    compressed = get_3rd_layer_output([X])[0]\n",
    "    compressed = compressed.reshape(X.shape[0],int(vo[1])*int(vo[2])*int(vo[3]))\n",
    "\n",
    "    def train_input_fn():\n",
    "        data = tf.constant(compressed, tf.float32)\n",
    "        return (data, None)\n",
    "\n",
    "    clusters = unsupervised_model.predict(input_fn=train_input_fn)\n",
    "    id_img = 0\n",
    "    for k,i in tqdm.tqdm(enumerate(clusters)):\n",
    "        cluster_id = i['cluster_idx']\n",
    "        id_img = id_img+1\n",
    "        features = X[k]\n",
    "        path_class_ = os.path.join(path_class, title, str(cluster_id))\n",
    "        if not os.path.exists(path_class_):\n",
    "            os.makedirs(path_class_)\n",
    "        scipy.misc.imsave(os.path.join(path_class_,str(id_img)+'.png'), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
