{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic package\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import math\n",
    "import functools\n",
    "import collections\n",
    "\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "from UTILS import preprocessing_experiment2, general_cleaning, simple_cleaning_experiment2,verification_based_on_initial_record,\\\n",
    "HenVariable, print_color, boxplot_distribution_entropy, heatmap_duration_perzone_perhen, \\\n",
    "time_series_henColumn_tsRow, cleaning_mouvement_records\n",
    "import config_experiment2 as config"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This notebook has 4 steps\n",
    "1. Preprocessing\n",
    "based on initial files and is specific to each experiment, and should create dataframe of same format to be used in the functions. Has the purpose to put the csv into a general format for each experiment with movement-location data\n",
    "2. Verification\n",
    "based on the initial-preprocessed csv, verify things on record level (e.g. first record saved per day etc). This could induce some new rules, or highlight issue in the system/data. It wont produce some csv file on which we will compute variable or create time series, but it will just output info on the initial data\n",
    "3. Cleaning\n",
    "Based on the initial-preprocessed csv, it should clean the data. It is splitted into the general_cleaning, that will be used for each experiment (adding microseconds to account for equal timestamp for same hen and different zone), and into the specific_cleaning (bining or rules.... to be defined, for now it just ouput some csv with infromation to base the rules one (flickering1, flickering2, enveloppe,binning))\n",
    "4. Time series\n",
    "based on the cleaning csv, create time series (one row per timestamp, one column per hen (i.e. per time series), it will also create one csv per hen. From these files then we can extract variables and do analysis youhouuuu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change the configuration file if not done yet!\n",
      "VFVFVF_\n"
     ]
    }
   ],
   "source": [
    "print('change the configuration file if not done yet!')\n",
    "path_extracted_data = config.path_extracted_data\n",
    "id_run = config.id_run\n",
    "dico_matching = config.dico_matching\n",
    "print(id_run)\n",
    "#print('The date that will be removed are: \\n %s'%' \\n '.join([str(d) for d in sorted(config.li_date2remove)]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#date to remove:\n",
    "28.11.2019, 29.11.2019, \n",
    "2.12.2019, 4.12.2019, 6.12.2019, 09.12.2019, 11.12.2019, 21.12.2019, 22.12.2019, 23.12.2019,\n",
    "06.01.2020, 12.01.2020, \n",
    "10.02.2020,  13.02.2020,\n",
    "16.03.2020, 17.03.2020\n",
    "4.5.2020, \n",
    "2.6.2020, 29.06.2020, 30.06.2020\n",
    "list of dates:\n",
    "evtl. 26.12. â€“ 07.01 (chicken locator stopped working), \n",
    "21.-02.02. (other software of chickenlocator);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bning with miliseconds?\n",
    "verficatio/debug code?\n",
    "bining with missing days?\n",
    "variables with missing days?\n",
    "variable with bining data (i.e. not 60 seconds interval anymore)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Preprocessing step which is specific to each experiment, and should create dataframe of same format to be used in the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "path_initial_data = r'R:\\VPHI\\Welfare\\2- Research Projects\\Laura C-Klara G\\tracking_Data'\n",
    "#Note that as there is no standardize name (i.e. start with log_), we will simply open all csv files. If there is other csv file\n",
    "#it wont work\n",
    "p = glob.glob(os.path.join(path_initial_data,'TagUpdates*_*','*.csv'))\n",
    "#print('There is %d log files:\\n  %s'%(len(p),'  \\n  '.join(p)))\n",
    "path_FocalBird = os.path.join(path_initial_data,'full_tagID_long.xlsx')\n",
    "df = preprocessing_experiment2(p, path_FocalBird, config)\n",
    "print(df.shape)\n",
    "display(df.head(3))\n",
    "display(df.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we will start computing measure and stop looking at single records, lets first clean the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### general cleaning (adding ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for being more reproductible, we open the file that was saved from cleaning\n",
    "df = pd.read_csv(os.path.join(path_extracted_data, id_run+'_PreprocessRecords.csv'), sep=';', parse_dates=['Timestamp'],\n",
    "                 index_col=0) \n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#same for each experiment\n",
    "df = general_cleaning(df, config)\n",
    "print(df.shape)\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning rules info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only if needed. Exemple check if system is working better after recalibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for being more reproductible, we open the file that was saved from cleaning\n",
    "df = pd.read_csv(os.path.join(path_extracted_data, id_run+'_records_GeneralCleaning.csv'), sep=';', \n",
    "                 parse_dates=['Timestamp', 'Timestamp_initial', 'day']) \n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = cleaning_mouvement_records(df, config, nbr_block_repetition=3, flickering_type1=True)\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create one second interval time serie"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First we transform into time series of one second interval not to loose any info with each column one hen and each row one second interval timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12453168, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>HenID</th>\n",
       "      <th>Zone</th>\n",
       "      <th>PenID</th>\n",
       "      <th>log_file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>ts_order_logname</th>\n",
       "      <th>ts_order_list</th>\n",
       "      <th>ms</th>\n",
       "      <th>Timestamp_initial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12453165</th>\n",
       "      <td>2020-07-18 23:59:56</td>\n",
       "      <td>hen_51</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-07-15_224155_2020-07-19_055005csv</td>\n",
       "      <td>2020-07-18</td>\n",
       "      <td>2020-07-15_224155_2020-07-19_055005csv_98992</td>\n",
       "      <td>2020-07-15_224155_2020-07-19_055005csv_98992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-07-18 23:59:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12453166</th>\n",
       "      <td>2020-07-18 23:59:59</td>\n",
       "      <td>hen_8</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>12</td>\n",
       "      <td>2020-07-15_054035_2020-07-19_041254csv</td>\n",
       "      <td>2020-07-18</td>\n",
       "      <td>2020-07-15_054035_2020-07-19_041254csv_84479</td>\n",
       "      <td>2020-07-15_054035_2020-07-19_041254csv_84479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-07-18 23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12453167</th>\n",
       "      <td>2020-07-18 23:59:59</td>\n",
       "      <td>hen_82</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-07-15_054035_2020-07-19_041254csv</td>\n",
       "      <td>2020-07-18</td>\n",
       "      <td>2020-07-15_054035_2020-07-19_041254csv_84480</td>\n",
       "      <td>2020-07-15_054035_2020-07-19_041254csv_84480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-07-18 23:59:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Timestamp   HenID    Zone  PenID  \\\n",
       "12453165 2020-07-18 23:59:56  hen_51  2 Zone      3   \n",
       "12453166 2020-07-18 23:59:59   hen_8  5 Zone     12   \n",
       "12453167 2020-07-18 23:59:59  hen_82  5 Zone     11   \n",
       "\n",
       "                                   log_file_name       date  \\\n",
       "12453165  2020-07-15_224155_2020-07-19_055005csv 2020-07-18   \n",
       "12453166  2020-07-15_054035_2020-07-19_041254csv 2020-07-18   \n",
       "12453167  2020-07-15_054035_2020-07-19_041254csv 2020-07-18   \n",
       "\n",
       "                                      ts_order_logname  \\\n",
       "12453165  2020-07-15_224155_2020-07-19_055005csv_98992   \n",
       "12453166  2020-07-15_054035_2020-07-19_041254csv_84479   \n",
       "12453167  2020-07-15_054035_2020-07-19_041254csv_84480   \n",
       "\n",
       "                                         ts_order_list   ms  \\\n",
       "12453165  2020-07-15_224155_2020-07-19_055005csv_98992  0.0   \n",
       "12453166  2020-07-15_054035_2020-07-19_041254csv_84479  0.0   \n",
       "12453167  2020-07-15_054035_2020-07-19_041254csv_84480  0.0   \n",
       "\n",
       "           Timestamp_initial  \n",
       "12453165 2020-07-18 23:59:56  \n",
       "12453166 2020-07-18 23:59:59  \n",
       "12453167 2020-07-18 23:59:59  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for being more reproductible, we open the file that was saved from cleaning\n",
    "df = pd.read_csv(os.path.join(path_extracted_data, id_run+'_records_GeneralCleaning.csv'), sep=';', \n",
    "                 parse_dates=['Timestamp', 'Timestamp_initial', 'date']) \n",
    "print(df.shape)\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START_TIME = time.clock()\n",
    "df_ts = time_series_henColumn_tsRow(df, config, col_ts='Zone', save=True, hen_time_series=False)\n",
    "END_TIME = time.clock()\n",
    "print (\"Total running time to create time series: %.2f mn\" %((END_TIME-START_TIME)/60)) \n",
    "print(df_ts.shape)\n",
    "df_ts.head(3)\n",
    "#more efficient: by month-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### binning based on the one second time series"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that for this cleaning we should use time series as input by definition of this cleaning.\n",
    "We might loose some of the last records information depending on the nbr_sec_mean value (if one range is not complete of records then avering would be wrong as it would not include all needed info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>HenID</th>\n",
       "      <th>Zone</th>\n",
       "      <th>PenID</th>\n",
       "      <th>log_file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>ts_order_logname</th>\n",
       "      <th>ts_order_list</th>\n",
       "      <th>ms</th>\n",
       "      <th>Timestamp_initial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-22 07:37:52.000000</td>\n",
       "      <td>hen_71</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_292</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_292 201...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2019-10-22 07:37:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-22 07:37:52.000000</td>\n",
       "      <td>hen_39</td>\n",
       "      <td>4 Zone</td>\n",
       "      <td>12</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_293</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_293 201...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2019-10-22 07:37:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-22 07:37:52.166666</td>\n",
       "      <td>hen_71</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_294</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_292 201...</td>\n",
       "      <td>166666.666667</td>\n",
       "      <td>2019-10-22 07:37:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Timestamp   HenID    Zone  PenID  \\\n",
       "0 2019-10-22 07:37:52.000000  hen_71  3 Zone     10   \n",
       "1 2019-10-22 07:37:52.000000  hen_39  4 Zone     12   \n",
       "2 2019-10-22 07:37:52.166666  hen_71  3 Zone     10   \n",
       "\n",
       "                            log_file_name       date  \\\n",
       "0  2019-10-16_113343_2019-11-02_060257csv 2019-10-22   \n",
       "1  2019-10-16_113343_2019-11-02_060257csv 2019-10-22   \n",
       "2  2019-10-16_113343_2019-11-02_060257csv 2019-10-22   \n",
       "\n",
       "                             ts_order_logname  \\\n",
       "0  2019-10-16_113343_2019-11-02_060257csv_292   \n",
       "1  2019-10-16_113343_2019-11-02_060257csv_293   \n",
       "2  2019-10-16_113343_2019-11-02_060257csv_294   \n",
       "\n",
       "                                       ts_order_list             ms  \\\n",
       "0  2019-10-16_113343_2019-11-02_060257csv_292 201...       0.000000   \n",
       "1  2019-10-16_113343_2019-11-02_060257csv_293 201...       0.000000   \n",
       "2  2019-10-16_113343_2019-11-02_060257csv_292 201...  166666.666667   \n",
       "\n",
       "    Timestamp_initial  \n",
       "0 2019-10-22 07:37:52  \n",
       "1 2019-10-22 07:37:52  \n",
       "2 2019-10-22 07:37:52  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create time series\n",
      "in this time series there is 18 hens\n",
      "The initial starting date in over all is: 2019-10-22 07:37:52, and the ending date will be: 2020-07-18 23:53:19\n",
      "But note that birds may have different ending and starting date\n",
      "and after ending the last day at midnight : 2019-10-22 07:37:52, and the ending date will be: 2020-07-18 23:59:59\n",
      "[17 16 15 14 12 11  9  8  7  6  5  4  3  2  1  0]\n",
      "-------------- Lets remove timestamp without all hen\n",
      "as we want the time series to start at the same time, we remove the dates without info on each hen, making us start on  2019-10-22 07:38:43\n"
     ]
    }
   ],
   "source": [
    "for p in df['PenID'].unique():\n",
    "    df_final = simple_cleaning_experiment2(df[df['PenID']==p], config, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by batch as we want at minute level, then we are sure each day will not need the day before (it needs jsut to know where it is,\n",
    "#i.e. to create seconds time series but thats it)\n",
    "def simple_cleaning_experiment2(df, config, nbr_sec_mean, mi=None, ma=None, save=True):\n",
    "    \n",
    "    ''' \n",
    "    *input: nbr_sec_mean: period, df_ts: time serie dataframe, typically created by the function \"time_series_henColumn_tsRow()\"\n",
    "    *output: a csv where timestamp ts results in the bining the all record from ts-period to ts]\n",
    "    *main idea: create time series for each hen by taking the most frequent zone for each \"nbr_sec_mean\" seconds period\n",
    "    *programming main idea: First we create a list of timestamp including only the one we want (i.e. one per nbr_sec_mean seconds). Then we match the old timestamp with the smallest of the list taht is beger of equal to the actual timestamp\n",
    "    '''\n",
    "    \n",
    "    #start recording the time it last\n",
    "    START_TIME = time.clock()\n",
    "    \n",
    "    print('create time series')\n",
    "    df_ts = time_series_henColumn_tsRow(df, config, col_ts='Zone', save=True, hen_time_series=False)\n",
    "    print('finish creating time series')\n",
    "      \n",
    "    #initialize parameters\n",
    "    id_run = config.id_run\n",
    "    path_extracted_data = config.path_extracted_data\n",
    "    nbr_sec = 1 #should stay one for now\n",
    "    \n",
    "    #create a directory if not existing\n",
    "    path_ = os.path.join(path_extracted_data, 'HensTimeSeries')\n",
    "    if not os.path.exists(os.path.join(path_)):\n",
    "        os.makedirs(os.path.join(path_))\n",
    "        \n",
    "    #######################################################################################################################\n",
    "    ##### create a list of dates that we want starting from our initial and end dates with the wanted binning period ######\n",
    "    if mi==None:\n",
    "        mi = min(df_ts['Timestamp'].tolist())\n",
    "    if ma==None:\n",
    "        ma = max(df_ts['Timestamp'].tolist())\n",
    "    #keeping dataframe that is linked to these dates\n",
    "    df_ts = df_ts[(df_ts['Timestamp']>=mi) & (df_ts['Timestamp']<=ma)]\n",
    "    \n",
    "    #on arondi a la minute du bas/haut pour ne pas rater des records \n",
    "    Daterange = pd.date_range(start = mi-dt.timedelta(seconds=mi.second), \n",
    "                              end = ma+dt.timedelta(seconds=60-ma.second), \n",
    "                              freq = 'S')    \n",
    "    print('The starting date of the datetime list is: %s, and the ending date is: %s'%(str(Daterange[0]), str(Daterange[-1])))\n",
    "    #take only the wanted timestamps (nbr_sec_mean)\n",
    "    Daterange = [Daterange[i] for i in range(len(Daterange)) if i%nbr_sec_mean==0]\n",
    "    print('The starting date of the selected datetime list is: %s, and the ending date is: %s'%(str(Daterange[0]), str(Daterange[-1])))\n",
    "    \n",
    "    #######################################################################################################################\n",
    "    #add new timestamp to the initial file\n",
    "    df_date = pd.DataFrame({'New_Timestamp':Daterange})\n",
    "    df_date['New_Timestamp'] = df_date['New_Timestamp'].map(lambda x: pd.to_datetime(x))\n",
    "    df_ts = pd.merge_asof(df_ts, df_date, left_on=['Timestamp'], right_on=['New_Timestamp'], direction='forward')\n",
    "    #df_ts['New_Timestamp_old'] = df_ts['Timestamp'].map(lambda x: min([d for d in Daterange if d >= x], default=np.nan))\n",
    "    \n",
    "    #aggregate (by using groupby: for each hen take its time serie and find the most frequent zone per new_timestamp)\n",
    "    for h in tqdm.tqdm([x for x in df_ts.columns if x.startswith('hen_')]):\n",
    "        df_ = df_ts[[h,'New_Timestamp']].copy()\n",
    "        df_['nbr_sec'] = nbr_sec\n",
    "        #df_verification = df_ts.groupby(['New_Timestamp']).agg({'Timestamp':['max', 'min']}).reset_index()\n",
    "        #df_verification.to_csv(os.path.join(path_ ,id_run+'_ts_MostFrequentZone_period_VERIFICATION'+str(nbr_sec_mean)+'_'+str(mi).split(' ')[0]+'_'+str(ma).split(' ')[0]+'_'+h+'.csv'), sep=';', index=False)\n",
    "        df__ = df_.groupby(['New_Timestamp',h])['nbr_sec'].sum().reset_index() #sum to count as we have seconds\n",
    "        df_final = df__.groupby(['New_Timestamp'])[h,'nbr_sec'].agg(lambda x: tuple(x)).reset_index()\n",
    "        df_final['most_frequent_zone'] = df_final.apply(lambda x: x[h][x['nbr_sec'].index(max(x['nbr_sec']))], axis=1)\n",
    "        df_final['nbr_duration_per_zone'] = df_final.apply(lambda x: str({x[h][k]:x['nbr_sec'][k] for k in range(len(x[h]))}), axis=1)\n",
    "        df_final['nbr_lost_duration_per_zone'] = df_final['nbr_duration_per_zone'].map(lambda x: str({z:v for z,v in eval(x).items() if \\\n",
    "                                                                           v!=max(eval(x).values())}))\n",
    "        df_final['nbr_lost_duration'] = df_final['nbr_lost_duration_per_zone'].map(lambda x: sum(eval(x).values()))\n",
    "        df_final['perc_lost_duration'] = df_final['nbr_lost_duration'].map(lambda x: x/nbr_sec_mean*100)\n",
    "        df_final['day'] = df_final['New_Timestamp'].map(lambda x: dt.datetime(x.year,x.month,x.day))\n",
    "        if save:\n",
    "            df_final.to_csv(os.path.join(path_ ,id_run+'_ts_MostFrequentZone_period'+str(nbr_sec_mean)+'_'+str(mi).split(' ')[0]+\\\n",
    "                                         '_'+str(ma).split(' ')[0]+'_'+h+'.csv'), sep=';', index=False)\n",
    "    \n",
    "    #running time info and return final cleaned df\n",
    "    END_TIME = time.clock()\n",
    "    print (\"Total running time: %.2f mn\" %((END_TIME-START_TIME)/60))\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12453168, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>HenID</th>\n",
       "      <th>Zone</th>\n",
       "      <th>PenID</th>\n",
       "      <th>log_file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>ts_order_logname</th>\n",
       "      <th>ts_order_list</th>\n",
       "      <th>ms</th>\n",
       "      <th>Timestamp_initial</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-22 07:37:52.000000</td>\n",
       "      <td>hen_71</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_292</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_292 201...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2019-10-22 07:37:52</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-22 07:37:52.000000</td>\n",
       "      <td>hen_39</td>\n",
       "      <td>4 Zone</td>\n",
       "      <td>12</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_293</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_293 201...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2019-10-22 07:37:52</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-22 07:37:52.166666</td>\n",
       "      <td>hen_71</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_294</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_292 201...</td>\n",
       "      <td>166666.666667</td>\n",
       "      <td>2019-10-22 07:37:52</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Timestamp   HenID    Zone  PenID  \\\n",
       "0 2019-10-22 07:37:52.000000  hen_71  3 Zone     10   \n",
       "1 2019-10-22 07:37:52.000000  hen_39  4 Zone     12   \n",
       "2 2019-10-22 07:37:52.166666  hen_71  3 Zone     10   \n",
       "\n",
       "                            log_file_name       date  \\\n",
       "0  2019-10-16_113343_2019-11-02_060257csv 2019-10-22   \n",
       "1  2019-10-16_113343_2019-11-02_060257csv 2019-10-22   \n",
       "2  2019-10-16_113343_2019-11-02_060257csv 2019-10-22   \n",
       "\n",
       "                             ts_order_logname  \\\n",
       "0  2019-10-16_113343_2019-11-02_060257csv_292   \n",
       "1  2019-10-16_113343_2019-11-02_060257csv_293   \n",
       "2  2019-10-16_113343_2019-11-02_060257csv_294   \n",
       "\n",
       "                                       ts_order_list             ms  \\\n",
       "0  2019-10-16_113343_2019-11-02_060257csv_292 201...       0.000000   \n",
       "1  2019-10-16_113343_2019-11-02_060257csv_293 201...       0.000000   \n",
       "2  2019-10-16_113343_2019-11-02_060257csv_292 201...  166666.666667   \n",
       "\n",
       "    Timestamp_initial  month  \n",
       "0 2019-10-22 07:37:52     10  \n",
       "1 2019-10-22 07:37:52     10  \n",
       "2 2019-10-22 07:37:52     10  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['month'] = df['Timestamp'].map(lambda x: x.month) \n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial starting date in over all is: 2019-10-22 07:37:52, and the ending date will be: 2020-07-18 23:59:59\n",
      "But note that birds may have different ending and starting date\n",
      "and after ending the last day at midnight : 2019-10-22 07:37:52, and the ending date will be: 2020-07-18 23:59:59\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(['Timestamp'], ascending=True)\n",
    "x0 = df.shape[0]\n",
    "#No need now to have no duplicate timestamp per hen:\n",
    "#df = df.groupby(['Timestamp','HenID'])[col_ts].agg(lambda x: list(x)[0]).reset_index()  \n",
    "df_hens = df.pivot(index='Timestamp', columns='HenID', values='Zone')\n",
    "\n",
    "#fill \"None\" values with the last non-empty value (by propagating last valHenID observation forward to next valHenID)\n",
    "#In order to fill in between timestamp, ie. timestamp that another hen had, then the other should also have their latest zone \n",
    "#entered instead of nan Note that the first ones will stay None\n",
    "df_hens = df_hens.fillna(method='ffill')\n",
    "\n",
    "#Warning: not all hens have same initial/enddate!\n",
    "#add missing dates \n",
    "mi = min(df['Timestamp'].tolist())\n",
    "ma = max(df['Timestamp'].tolist())\n",
    "print('The initial starting date in over all is: %s, and the ending date will be: %s'%(str(mi), str(ma)))\n",
    "print('But note that birds may have different ending and starting date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and after ending the last day at midnight : 2019-10-22 00:00:00, and the ending date will be: 2020-07-21 00:00:00\n",
      "393121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2019-10-22 00:00:00', '2019-10-22 00:01:00',\n",
       "               '2019-10-22 00:02:00', '2019-10-22 00:03:00',\n",
       "               '2019-10-22 00:04:00', '2019-10-22 00:05:00',\n",
       "               '2019-10-22 00:06:00', '2019-10-22 00:07:00',\n",
       "               '2019-10-22 00:08:00', '2019-10-22 00:09:00'],\n",
       "              dtype='datetime64[ns]', freq='T')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add dates until minuit of the last day\n",
    "ma = dt.datetime(ma.year,ma.month,ma.day,23,59,59)+dt.timedelta(seconds=1)\n",
    "mi = dt.datetime(mi.year,mi.month,mi.day,0,0,0)\n",
    "print('and after ending the last day at midnight : %s, and the ending date will be: %s'%(str(mi), str(ma)))\n",
    "Daterange = pd.date_range(start=mi, end=ma, freq='min') \n",
    "print(len(Daterange))\n",
    "Daterange[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add missing seconds (i.e. all seconds that never had a record) and fillnan with last non-nan values by propagating last \n",
    "#valHenID observation (even if its an observation that will be removed) forward to next valHenID\n",
    "df_hens_ = df_hens.reindex(Daterange, method='ffill').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>HenID</th>\n",
       "      <th>hen_1</th>\n",
       "      <th>hen_10</th>\n",
       "      <th>hen_100</th>\n",
       "      <th>hen_101</th>\n",
       "      <th>hen_102</th>\n",
       "      <th>hen_103</th>\n",
       "      <th>hen_104</th>\n",
       "      <th>hen_105</th>\n",
       "      <th>hen_106</th>\n",
       "      <th>hen_107</th>\n",
       "      <th>...</th>\n",
       "      <th>hen_90</th>\n",
       "      <th>hen_91</th>\n",
       "      <th>hen_92</th>\n",
       "      <th>hen_93</th>\n",
       "      <th>hen_94</th>\n",
       "      <th>hen_95</th>\n",
       "      <th>hen_96</th>\n",
       "      <th>hen_97</th>\n",
       "      <th>hen_98</th>\n",
       "      <th>hen_99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-07-18 23:59:47</th>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>...</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-18 23:59:49</th>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>...</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-18 23:59:50</th>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>...</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-18 23:59:56</th>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>...</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-18 23:59:59</th>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>...</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "HenID                 hen_1  hen_10 hen_100 hen_101 hen_102 hen_103 hen_104  \\\n",
       "Timestamp                                                                     \n",
       "2020-07-18 23:59:47  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone   \n",
       "2020-07-18 23:59:49  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone   \n",
       "2020-07-18 23:59:50  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone   \n",
       "2020-07-18 23:59:56  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone   \n",
       "2020-07-18 23:59:59  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone   \n",
       "\n",
       "HenID               hen_105 hen_106 hen_107  ...  hen_90  hen_91  hen_92  \\\n",
       "Timestamp                                    ...                           \n",
       "2020-07-18 23:59:47  3 Zone  2 Zone  5 Zone  ...  5 Zone  5 Zone  3 Zone   \n",
       "2020-07-18 23:59:49  3 Zone  2 Zone  5 Zone  ...  5 Zone  5 Zone  3 Zone   \n",
       "2020-07-18 23:59:50  3 Zone  2 Zone  5 Zone  ...  5 Zone  5 Zone  3 Zone   \n",
       "2020-07-18 23:59:56  3 Zone  2 Zone  5 Zone  ...  5 Zone  5 Zone  3 Zone   \n",
       "2020-07-18 23:59:59  3 Zone  2 Zone  5 Zone  ...  5 Zone  5 Zone  3 Zone   \n",
       "\n",
       "HenID                hen_93  hen_94  hen_95  hen_96  hen_97  hen_98  hen_99  \n",
       "Timestamp                                                                    \n",
       "2020-07-18 23:59:47  5 Zone  5 Zone  2 Zone  5 Zone  3 Zone  2 Zone  5 Zone  \n",
       "2020-07-18 23:59:49  5 Zone  5 Zone  2 Zone  5 Zone  3 Zone  2 Zone  5 Zone  \n",
       "2020-07-18 23:59:50  5 Zone  5 Zone  2 Zone  5 Zone  3 Zone  2 Zone  5 Zone  \n",
       "2020-07-18 23:59:56  5 Zone  5 Zone  2 Zone  5 Zone  3 Zone  2 Zone  5 Zone  \n",
       "2020-07-18 23:59:59  5 Zone  5 Zone  2 Zone  5 Zone  3 Zone  2 Zone  5 Zone  \n",
       "\n",
       "[5 rows x 112 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hens.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>HenID</th>\n",
       "      <th>index</th>\n",
       "      <th>hen_1</th>\n",
       "      <th>hen_10</th>\n",
       "      <th>hen_100</th>\n",
       "      <th>hen_101</th>\n",
       "      <th>hen_102</th>\n",
       "      <th>hen_103</th>\n",
       "      <th>hen_104</th>\n",
       "      <th>hen_105</th>\n",
       "      <th>hen_106</th>\n",
       "      <th>...</th>\n",
       "      <th>hen_90</th>\n",
       "      <th>hen_91</th>\n",
       "      <th>hen_92</th>\n",
       "      <th>hen_93</th>\n",
       "      <th>hen_94</th>\n",
       "      <th>hen_95</th>\n",
       "      <th>hen_96</th>\n",
       "      <th>hen_97</th>\n",
       "      <th>hen_98</th>\n",
       "      <th>hen_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393116</th>\n",
       "      <td>2020-07-20 23:56:00</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>...</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393117</th>\n",
       "      <td>2020-07-20 23:57:00</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>...</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393118</th>\n",
       "      <td>2020-07-20 23:58:00</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>...</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393119</th>\n",
       "      <td>2020-07-20 23:59:00</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>...</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393120</th>\n",
       "      <td>2020-07-21 00:00:00</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>...</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>2 Zone</td>\n",
       "      <td>5 Zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "HenID                index   hen_1  hen_10 hen_100 hen_101 hen_102 hen_103  \\\n",
       "393116 2020-07-20 23:56:00  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone   \n",
       "393117 2020-07-20 23:57:00  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone   \n",
       "393118 2020-07-20 23:58:00  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone   \n",
       "393119 2020-07-20 23:59:00  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone   \n",
       "393120 2020-07-21 00:00:00  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone  5 Zone   \n",
       "\n",
       "HenID  hen_104 hen_105 hen_106  ...  hen_90  hen_91  hen_92  hen_93  hen_94  \\\n",
       "393116  5 Zone  3 Zone  2 Zone  ...  5 Zone  5 Zone  3 Zone  5 Zone  5 Zone   \n",
       "393117  5 Zone  3 Zone  2 Zone  ...  5 Zone  5 Zone  3 Zone  5 Zone  5 Zone   \n",
       "393118  5 Zone  3 Zone  2 Zone  ...  5 Zone  5 Zone  3 Zone  5 Zone  5 Zone   \n",
       "393119  5 Zone  3 Zone  2 Zone  ...  5 Zone  5 Zone  3 Zone  5 Zone  5 Zone   \n",
       "393120  5 Zone  3 Zone  2 Zone  ...  5 Zone  5 Zone  3 Zone  5 Zone  5 Zone   \n",
       "\n",
       "HenID   hen_95  hen_96  hen_97  hen_98  hen_99  \n",
       "393116  2 Zone  5 Zone  3 Zone  2 Zone  5 Zone  \n",
       "393117  2 Zone  5 Zone  3 Zone  2 Zone  5 Zone  \n",
       "393118  2 Zone  5 Zone  3 Zone  2 Zone  5 Zone  \n",
       "393119  2 Zone  5 Zone  3 Zone  2 Zone  5 Zone  \n",
       "393120  2 Zone  5 Zone  3 Zone  2 Zone  5 Zone  \n",
       "\n",
       "[5 rows x 113 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hens_.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8740161, 112) (393121, 113)\n"
     ]
    }
   ],
   "source": [
    "print(df_hens.shape, df_hens_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12453168, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>HenID</th>\n",
       "      <th>Zone</th>\n",
       "      <th>PenID</th>\n",
       "      <th>log_file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>ts_order_logname</th>\n",
       "      <th>ts_order_list</th>\n",
       "      <th>ms</th>\n",
       "      <th>Timestamp_initial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-22 07:37:52.000000</td>\n",
       "      <td>hen_71</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_292</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_292 201...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2019-10-22 07:37:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-22 07:37:52.000000</td>\n",
       "      <td>hen_39</td>\n",
       "      <td>4 Zone</td>\n",
       "      <td>12</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_293</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_293 201...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2019-10-22 07:37:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-22 07:37:52.166666</td>\n",
       "      <td>hen_71</td>\n",
       "      <td>3 Zone</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_294</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_292 201...</td>\n",
       "      <td>166666.666667</td>\n",
       "      <td>2019-10-22 07:37:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Timestamp   HenID    Zone  PenID  \\\n",
       "0 2019-10-22 07:37:52.000000  hen_71  3 Zone     10   \n",
       "1 2019-10-22 07:37:52.000000  hen_39  4 Zone     12   \n",
       "2 2019-10-22 07:37:52.166666  hen_71  3 Zone     10   \n",
       "\n",
       "                            log_file_name       date  \\\n",
       "0  2019-10-16_113343_2019-11-02_060257csv 2019-10-22   \n",
       "1  2019-10-16_113343_2019-11-02_060257csv 2019-10-22   \n",
       "2  2019-10-16_113343_2019-11-02_060257csv 2019-10-22   \n",
       "\n",
       "                             ts_order_logname  \\\n",
       "0  2019-10-16_113343_2019-11-02_060257csv_292   \n",
       "1  2019-10-16_113343_2019-11-02_060257csv_293   \n",
       "2  2019-10-16_113343_2019-11-02_060257csv_294   \n",
       "\n",
       "                                       ts_order_list             ms  \\\n",
       "0  2019-10-16_113343_2019-11-02_060257csv_292 201...       0.000000   \n",
       "1  2019-10-16_113343_2019-11-02_060257csv_293 201...       0.000000   \n",
       "2  2019-10-16_113343_2019-11-02_060257csv_292 201...  166666.666667   \n",
       "\n",
       "    Timestamp_initial  \n",
       "0 2019-10-22 07:37:52  \n",
       "1 2019-10-22 07:37:52  \n",
       "2 2019-10-22 07:37:52  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#IDEA:\n",
    "create a column: next record, duration maximised by the next minute\n",
    "then bining on this column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:blue>-----------------------------------------------------------------------------------------------</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:blue>Compute duration for bining</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:blue>-----------------------------------------------------------------------------------------------</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [13:49<00:00,  7.40s/it]\n"
     ]
    }
   ],
   "source": [
    "def duration4bining(y,x):\n",
    "    #y = next_record_date, x = actual_record_date\n",
    "    if (pd.isnull(y)) | (pd.isnull(x)):\n",
    "        return np.datetime64('NaT')\n",
    "    return min(60-(x-dt.datetime(x.year, x.month, x.day, x.hour, x.minute, 0,0)).total_seconds(),(y-x).total_seconds())\n",
    "#example 1\n",
    "#x = dt.datetime(2020,12,2,12,32,2,0)\n",
    "#y = dt.datetime(2020,12,2,12,33,34,0)\n",
    "#duration4bining(y,x) #58.0\n",
    "#example 2\n",
    "#x = dt.datetime(2020,12,2,12,33,12,0)\n",
    "#y = dt.datetime(2020,12,2,12,33,34,0)\n",
    "#duration4bining(y,x) #22.0\n",
    "#example 3\n",
    "#x = np.nan\n",
    "#y = dt.datetime(2020,12,2,12,33,34,0)\n",
    "#duration4bining(y,x) #numpy.datetime64('NaT')\n",
    "#example 4\n",
    "#x = dt.datetime(2020,10,22, 7, 38, 1, 66666)\n",
    "#y = dt.datetime(2020,10,22, 10, 4, 50, 0)\n",
    "#duration4bining(y,x) #58.933334\n",
    "\n",
    "#######################################################################################################################\n",
    "############################################# Compute duration for bining #############################################\n",
    "#######################################################################################################################\n",
    "\n",
    "print_color((('-----------------------------------------------------------------------------------------------','blue'),))\n",
    "print_color((('Compute duration for bining','blue'),))\n",
    "print_color((('-----------------------------------------------------------------------------------------------','blue'),))\n",
    "\n",
    "li_df = []\n",
    "#more efficient to do it per hen, as it wont need to search in the whole dataframe, and we can simply shift the timestamp column\n",
    "for i, df_hen in tqdm.tqdm(df.groupby(['HenID'])):\n",
    "    #as the next record date (sort by date, then simply shift by one row and add nan at then end)\n",
    "    df_hen = df_hen.sort_values(['Timestamp'], ascending=True) #ts_order\n",
    "    #same date, one must take the last recorded one & sorting by date might change it. Also it already should be sorted by date\n",
    "    df_hen['next_record_date'] = df_hen['Timestamp'].tolist()[1:]+[np.nan]\n",
    "    #compute duration\n",
    "    df_hen['duration_maximised4bining'] = df_hen.apply(lambda x: duration4bining(x['next_record_date'],x['Timestamp']), axis=1)\n",
    "    li_df.append(df_hen)\n",
    "#put again in one dataframe\n",
    "df_ = pd.concat(li_df)\n",
    "#dont care about the false positive warning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HenID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>duration_maximised4bining</th>\n",
       "      <th>next_record_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>2019-10-22 07:38:01.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2019-10-22 07:38:01.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>2019-10-22 07:38:01.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2019-10-22 07:38:01.666666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>2019-10-22 07:38:01.666666</td>\n",
       "      <td>58.3333</td>\n",
       "      <td>2019-10-22 10:04:50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5739</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>2019-10-22 10:04:50.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2019-10-22 10:04:50.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>2019-10-22 10:04:50.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2019-10-22 10:04:50.666666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      HenID                  Timestamp duration_maximised4bining  \\\n",
       "61    hen_1 2019-10-22 07:38:01.000000                  0.333333   \n",
       "62    hen_1 2019-10-22 07:38:01.333333                  0.333333   \n",
       "64    hen_1 2019-10-22 07:38:01.666666                   58.3333   \n",
       "5739  hen_1 2019-10-22 10:04:50.000000                  0.333333   \n",
       "5741  hen_1 2019-10-22 10:04:50.333333                  0.333333   \n",
       "\n",
       "               next_record_date  \n",
       "61   2019-10-22 07:38:01.333333  \n",
       "62   2019-10-22 07:38:01.666666  \n",
       "64   2019-10-22 10:04:50.000000  \n",
       "5739 2019-10-22 10:04:50.333333  \n",
       "5741 2019-10-22 10:04:50.666666  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_[['HenID','Timestamp','duration_maximised4bining','next_record_date']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The starting date of the datetime list is: 2019-10-22 07:37:00, and the ending date is: 2020-07-19 00:00:00\n",
      "389784\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################################################\n",
    "##### create a list of dates that we want starting from our initial and end dates with the wanted binning period ######\n",
    "#######################################################################################################################\n",
    "\n",
    "#on arondi a la minute du bas/haut pour ne pas rater des records \n",
    "mi = min(df_['Timestamp'].tolist())\n",
    "ma = max(df_['Timestamp'].tolist())\n",
    "Daterange = pd.date_range(start = mi-dt.timedelta(seconds=mi.second), \n",
    "                          end = ma+dt.timedelta(seconds=60-ma.second), \n",
    "                          freq = 'min')    \n",
    "print('The starting date of the datetime list is: %s, and the ending date is: %s'%(str(Daterange[0]), str(Daterange[-1])))\n",
    "print(len(Daterange))\n",
    "Daterange[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2019-10-22 07:37:00', '2019-10-22 07:38:00',\n",
       "               '2019-10-22 07:39:00', '2019-10-22 07:40:00',\n",
       "               '2019-10-22 07:41:00', '2019-10-22 07:42:00',\n",
       "               '2019-10-22 07:43:00', '2019-10-22 07:44:00',\n",
       "               '2019-10-22 07:45:00', '2019-10-22 07:46:00'],\n",
       "              dtype='datetime64[ns]', freq='T')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Daterange[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>HenID</th>\n",
       "      <th>Zone</th>\n",
       "      <th>PenID</th>\n",
       "      <th>log_file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>ts_order_logname</th>\n",
       "      <th>ts_order_list</th>\n",
       "      <th>ms</th>\n",
       "      <th>Timestamp_initial</th>\n",
       "      <th>next_record_date</th>\n",
       "      <th>duration_maximised4bining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2019-10-22 07:38:01.000000</td>\n",
       "      <td>hen_1</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_313</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_313 201...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2019-10-22 07:38:01</td>\n",
       "      <td>2019-10-22 07:38:01.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2019-10-22 07:38:01.333333</td>\n",
       "      <td>hen_1</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-10-16_113343_2019-11-20_150252csv</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>2019-10-16_113343_2019-11-20_150252csv_313</td>\n",
       "      <td>2019-10-16_113343_2019-11-02_060257csv_313 201...</td>\n",
       "      <td>333333.333333</td>\n",
       "      <td>2019-10-22 07:38:01</td>\n",
       "      <td>2019-10-22 07:38:01.666666</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Timestamp  HenID    Zone  PenID  \\\n",
       "61 2019-10-22 07:38:01.000000  hen_1  5 Zone     10   \n",
       "62 2019-10-22 07:38:01.333333  hen_1  5 Zone     10   \n",
       "\n",
       "                             log_file_name       date  \\\n",
       "61  2019-10-16_113343_2019-11-02_060257csv 2019-10-22   \n",
       "62  2019-10-16_113343_2019-11-20_150252csv 2019-10-22   \n",
       "\n",
       "                              ts_order_logname  \\\n",
       "61  2019-10-16_113343_2019-11-02_060257csv_313   \n",
       "62  2019-10-16_113343_2019-11-20_150252csv_313   \n",
       "\n",
       "                                        ts_order_list             ms  \\\n",
       "61  2019-10-16_113343_2019-11-02_060257csv_313 201...       0.000000   \n",
       "62  2019-10-16_113343_2019-11-02_060257csv_313 201...  333333.333333   \n",
       "\n",
       "     Timestamp_initial           next_record_date duration_maximised4bining  \n",
       "61 2019-10-22 07:38:01 2019-10-22 07:38:01.333333                  0.333333  \n",
       "62 2019-10-22 07:38:01 2019-10-22 07:38:01.666666                  0.333333  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109510, 13)\n",
      "(468069, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HenID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Zone</th>\n",
       "      <th>duration_maximised4bining</th>\n",
       "      <th>next_record_date</th>\n",
       "      <th>New_Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109510</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-10-22 07:37:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109511</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-10-22 07:38:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>2019-10-22 07:38:01.000000</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2019-10-22 07:38:01.333333</td>\n",
       "      <td>2019-10-22 07:39:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>2019-10-22 07:38:01.666666</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>58.3333</td>\n",
       "      <td>2019-10-22 10:04:50.000000</td>\n",
       "      <td>2019-10-22 07:39:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hen_1</td>\n",
       "      <td>2019-10-22 07:38:01.333333</td>\n",
       "      <td>5 Zone</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2019-10-22 07:38:01.666666</td>\n",
       "      <td>2019-10-22 07:39:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109512</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-10-22 07:40:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109513</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-10-22 07:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109514</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-10-22 07:42:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109515</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-10-22 07:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109516</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-10-22 07:44:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        HenID                  Timestamp    Zone duration_maximised4bining  \\\n",
       "109510    NaN                        NaT     NaN                       NaN   \n",
       "109511    NaN                        NaT     NaN                       NaN   \n",
       "0       hen_1 2019-10-22 07:38:01.000000  5 Zone                  0.333333   \n",
       "2       hen_1 2019-10-22 07:38:01.666666  5 Zone                   58.3333   \n",
       "1       hen_1 2019-10-22 07:38:01.333333  5 Zone                  0.333333   \n",
       "109512    NaN                        NaT     NaN                       NaN   \n",
       "109513    NaN                        NaT     NaN                       NaN   \n",
       "109514    NaN                        NaT     NaN                       NaN   \n",
       "109515    NaN                        NaT     NaN                       NaN   \n",
       "109516    NaN                        NaT     NaN                       NaN   \n",
       "\n",
       "                 next_record_date       New_Timestamp  \n",
       "109510                        NaT 2019-10-22 07:37:00  \n",
       "109511                        NaT 2019-10-22 07:38:00  \n",
       "0      2019-10-22 07:38:01.333333 2019-10-22 07:39:00  \n",
       "2      2019-10-22 10:04:50.000000 2019-10-22 07:39:00  \n",
       "1      2019-10-22 07:38:01.666666 2019-10-22 07:39:00  \n",
       "109512                        NaT 2019-10-22 07:40:00  \n",
       "109513                        NaT 2019-10-22 07:41:00  \n",
       "109514                        NaT 2019-10-22 07:42:00  \n",
       "109515                        NaT 2019-10-22 07:43:00  \n",
       "109516                        NaT 2019-10-22 07:44:00  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add new timestamp to the initial file\n",
    "df_date = pd.DataFrame({'New_Timestamp':Daterange})\n",
    "df_date['New_Timestamp'] = df_date['New_Timestamp'].map(lambda x: pd.to_datetime(x))\n",
    "df_ = df_.sort_values(['Timestamp'], ascending=True)\n",
    "df_ts = pd.merge_asof(df_[df_['HenID']=='hen_1'], df_date, \n",
    "                      left_on=['Timestamp'], right_on=['New_Timestamp'], direction='forward')\n",
    "print(df_ts.shape)\n",
    "df_ts = pd.merge(df_ts, df_date, on=['New_Timestamp'], how='outer')\n",
    "print(df_ts.shape)\n",
    "#df_ts['New_Timestamp_old'] = df_ts['Timestamp'].map(lambda x: min([d for d in Daterange if d >= x], default=np.nan))\n",
    "df_ts = df_ts.sort_values(['New_Timestamp'], ascending=True)\n",
    "df_ts[['Timestamp','Zone','duration_maximised4bining','next_record_date',\n",
    "                                'New_Timestamp']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new timestamp to the initial file\n",
    "df_date = pd.DataFrame({'New_Timestamp':Daterange})\n",
    "df_date['New_Timestamp'] = df_date['New_Timestamp'].map(lambda x: pd.to_datetime(x))\n",
    "df_ = df_.sort_values(['Timestamp'], ascending=True)\n",
    "df_ts = pd.merge_asof(df_, df_date, left_on=['Timestamp'], right_on=['New_Timestamp'], direction='forward')\n",
    "#df_ts['New_Timestamp_old'] = df_ts['Timestamp'].map(lambda x: min([d for d in Daterange if d >= x], default=np.nan))\n",
    "df_ts[df_ts['HenID']=='hen_1'][['HenID','Timestamp','Zone','duration_maximised4bining','next_record_date',\n",
    "                                'New_Timestamp']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109510, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>HenID</th>\n",
       "      <th>hen_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-22 07:38:01.000000</th>\n",
       "      <td>5 Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-22 07:38:01.333333</th>\n",
       "      <td>5 Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-22 07:38:01.666666</th>\n",
       "      <td>5 Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-22 10:04:50.000000</th>\n",
       "      <td>2 Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-22 10:04:50.333333</th>\n",
       "      <td>2 Zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "HenID                        hen_1\n",
       "Timestamp                         \n",
       "2019-10-22 07:38:01.000000  5 Zone\n",
       "2019-10-22 07:38:01.333333  5 Zone\n",
       "2019-10-22 07:38:01.666666  5 Zone\n",
       "2019-10-22 10:04:50.000000  2 Zone\n",
       "2019-10-22 10:04:50.333333  2 Zone"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hens = df_ts[df_ts['HenID']=='hen_1'].pivot(index='Timestamp', columns='HenID', values='Zone')\n",
    "print(df_hens.shape)\n",
    "df_hens.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Timestamp' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-a1796bb4f2b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_ts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_ts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDaterange\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ffill'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0mkind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOSITIONAL_OR_KEYWORD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4024\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"axis\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4025\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4026\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4028\u001b[0m     def drop(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4451\u001b[0m         \u001b[1;31m# perform the reindex on the axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4452\u001b[0m         return self._reindex_axes(\n\u001b[1;32m-> 4453\u001b[1;33m             \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4454\u001b[0m         ).__finalize__(self, method=\"reindex\")\n\u001b[0;32m   4455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   3871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3872\u001b[0m             frame = frame._reindex_index(\n\u001b[1;32m-> 3873\u001b[1;33m                 \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3874\u001b[0m             )\n\u001b[0;32m   3875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_reindex_index\u001b[1;34m(self, new_index, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   3887\u001b[0m     ):\n\u001b[0;32m   3888\u001b[0m         new_index, indexer = self.index.reindex(\n\u001b[1;32m-> 3889\u001b[1;33m             \u001b[0mnew_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3890\u001b[0m         )\n\u001b[0;32m   3891\u001b[0m         return self._reindex_with_indexers(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[0;32m   3327\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"is_overlapping\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3328\u001b[0m                     indexer = self.get_indexer(\n\u001b[1;32m-> 3329\u001b[1;33m                         \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3330\u001b[0m                     )\n\u001b[0;32m   3331\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   2974\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2975\u001b[0m             return this.get_indexer(\n\u001b[1;32m-> 2976\u001b[1;33m                 \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2977\u001b[0m             )\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   2983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"pad\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"backfill\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2985\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_fill_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2986\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nearest\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2987\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_nearest_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_get_fill_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3021\u001b[0m                 \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_backfill_indexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3022\u001b[0m             )\n\u001b[1;32m-> 3023\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mengine_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3024\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3025\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_fill_indexer_searchsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_pad_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\algos.pyx\u001b[0m in \u001b[0;36mpandas._libs.algos.pad\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'Timestamp' and 'int'"
     ]
    }
   ],
   "source": [
    "df_hens = df.pivot(index='Timestamp', columns='HenID', values=col_ts)\n",
    "\n",
    "#fill \"None\" values with the last non-empty value (by propagating last valHenID observation forward to next valHenID)\n",
    "#In order to fill in between timestamp, ie. timestamp that another hen had, then the other should also have their latest zone \n",
    "#entered instead of nan Note that the first ones will stay None\n",
    "df_hens = df_hens.fillna(method='ffill')\n",
    "df_ts = df_ts.reindex(Daterange, method='ffill').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts[df_ts['HenID']=='hen_1'][['HenID','Timestamp','duration_maximised4bining','next_record_date','New_Timestamp']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### turn all bining hens time series into the usual ts csv file (one column per hen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for path_ in glob.glob(os.path.join(path_extracted_data, 'HensTimeSeries', id_run+'_ts_MostFrequentZone_period60_?_?_hen_*.csv')):\n",
    "li_df = []\n",
    "for path_ in tqdm.tqdm(glob.glob(os.path.join(path_extracted_data, 'HensTimeSeries', \n",
    "                                    id_run+'_ts_MostFrequentZone_period60_2019-11-01_2020-03-26_hen_*.csv'))):\n",
    "    name_ = path_.split('\\\\')[-1].split('_MostFrequentZone_')[-1].split('.csv')[0]\n",
    "    HenID = 'hen_'+name_.split('hen_')[-1]\n",
    "    df_h = pd.read_csv(path_, sep=';', parse_dates=['New_Timestamp']) \n",
    "    df_h = df_h.filter(['New_Timestamp','most_frequent_zone'],axis=1).reset_index(drop=True)\n",
    "    df_h.rename(columns={'most_frequent_zone':HenID}, inplace=True)\n",
    "    li_df.append(df_h)\n",
    "\n",
    "df = pd.DataFrame(columns=['New_Timestamp'])\n",
    "for df_h in li_df:\n",
    "    df = pd.merge(df, df_h, on='New_Timestamp', how='outer')\n",
    "df.to_csv(os.path.join(path_extracted_data,id_run+'ts_MostFrequentZone_period60_2019-11-01_2020-03-26_allhens.csv'), sep=';', index=False)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#VACCINATION 2\n",
    "for minute in [0.5,1,2,3,4,5,10,30]:\n",
    "    print('Lets compute with time interval ', minute)\n",
    "    simple_cleaning_experiment2(df_ts, config, \n",
    "                                nbr_sec_mean=int(minute*60),\n",
    "                                mi=dt.datetime(2019,12,20,0,0,0), \n",
    "                                ma=dt.datetime(2019,12,25,23,59,59))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at lost info depending on zones when bining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#this script is temporaire\n",
    "li_path = glob.glob(os.path.join(path_extracted_data, 'HensTimeSeries', id_run+'_ts_MostFrequentZone_period*.csv'))\n",
    "\n",
    "dico_mi_li_df = {}\n",
    "for path_ in tqdm.tqdm(li_path):\n",
    "    h = 'hen_'+path_.split('hen_')[1][:-4]\n",
    "    if 'VERIFICATION' not in path_:\n",
    "        period = int(path_.split('period')[1].split('_')[0])\n",
    "        df_ = pd.read_csv(path_, sep=';', parse_dates=['New_Timestamp'])\n",
    "        d1 = path_.split('period')[1].split('_')[1]\n",
    "        d2 = path_.split('period')[1].split('_')[2]\n",
    "        mi=dt.datetime(int(d1[0:4]),int(d1[5:7]),int(d1[8:]),0,0,0)\n",
    "        ma=dt.datetime(int(d2[0:4]),int(d2[5:7]),int(d2[8:]),23,59,59)\n",
    "        n = str(mi)+'   '+str(ma)\n",
    "        df_['HenID'] = h\n",
    "        #if 'nbr_lost_duration_per_zone' in df_.columns:\n",
    "        df_['nbr_lost_duration_per_zone'] = df_['nbr_lost_duration_per_zone'].map(lambda x: eval(x))\n",
    "        dico_ = dict(functools.reduce(operator.add, map(collections.Counter, df_['nbr_lost_duration_per_zone'].tolist()))) \n",
    "        df_['nbr_duration_per_zone'] = df_['nbr_duration_per_zone'].map(lambda x: eval(x))\n",
    "        dico_all = dict(functools.reduce(operator.add, map(collections.Counter, df_['nbr_duration_per_zone'].tolist()))) \n",
    "        if n not in dico_mi_li_df:\n",
    "            dico_mi_li_df[n] = []\n",
    "        for z in dico_all.keys():\n",
    "            dico_mi_li_df[n].append({'period':period, 'HenID':h, \n",
    "                                     'percentage_lost_duration':dico_.get(z,0)/dico_all[z]*100, 'zone': z})\n",
    "\n",
    "#plot\n",
    "for n, li_df in dico_mi_li_df.items():\n",
    "    df_plot = pd.DataFrame(li_df)\n",
    "    #plot for each datarange a plot showing the evolution of duration lost per zone and period\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    ax = sns.boxplot(x=\"period\", y=\"percentage_lost_duration\", hue=\"zone\", data=df_plot) #, palette=\"Set3\")\n",
    "    plt.title(n)\n",
    "    plt.savefig(os.path.join(path_extracted_data,'visual','for_verification', \n",
    "                             id_run+'_percentage_lost_duration_during_bining'+n.split(':')[0][:-3]+'_'+n.split(':')[2][5:-3]+'.png'), dpi=300,\n",
    "                             format='png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "#-->zone 1: si flickering pas possible peut-Ãªtre que nous devrons le garder en plus deu bining, puis choisir un bining \n",
    "#plus petit ou Ã©gale Ã  5mn peut etre :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
